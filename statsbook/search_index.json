[
["index.html", "Statistics: The Story of Numbers Preface", " Statistics: The Story of Numbers Robert G. Franklin, Jr. 2017-09-25 Preface This book has three purposes: Purpose 1: Remove any fear of statistics When I tell people I teach psychology, I get one of two responses. One response is “that’s interesting” and we talk about something fascinating about behavior or the mind. Another response is “I have a [insert relative] who you really need to talk to”, followed to some allusion to clinical psychology. Either way, they ask what I teach and I say, proudly, “Statistics”! (Yes, with the exclamation point). It won’t surprise you that the response is usually disgust. Or fear. People are afraid of statistics and those who practice it. If it weren’t for lawyers, statisticians would be the most hated profession. There are quotes like “don’t you know 90% of statistics are made up.” (Really, it’s 79.2% of statistics that are made up, but who is counting). People associate statistics with lies and so they’re afraid of statistics. It’s no coincidence the best known book about Statistics is called How to Lie with Statistics. The reason for this is that we don’t understand where the lies come from. Statistics can be used in two ways. One way is to twist numbers to fit the facts that we want to believe. The other way is that statistics can tell us hidden truths about the world. Statistics tell stories to those who want to put the effort into finding out the answers. Once we know this, I think statistics becomes a lot more interesting. Purpose 2: Teach you how to consume statistics It’s important to note that there are a lot of bad statistics out there. One thing which might be even worse than statisticians twisting numbers to fit their own story is when people take good statistics and make bad conclusions with them. If you are pursuing a psychology degree, you’re going to read a lot of psychology papers. Each of these papers will have statistics buried within them. Most of the students who review papers will skip most of these sections and just accept the conclusions the authors gave. But you, the student who completes this class, will have the power to do more than this. You will be able to go behind the curtain and find out what these numbers really mean. And you will be able to go to these numbers and actually use them. For instance, a paper may tell you that there is a link between meditating and increasing meditating. You might want to decide, “should I start meditating”, especially if you’re the type of person who doesn’t like sitting still to meditate. The question will be: does meditating improve my happiness enough to make it worth the effort? With what we learn in this class, you can answer that. Purpose 3: Teach you how to conduct statistics This section has two parts. The first is the ability to graphically present data. Well after you have graduated and decided that you would rather make money in the real world rather than trying to make school your career, you may have to do a presentation where you show statistics using graphs. You might be tempted to make the mistake of using bad charts, such as 3d bar graphs. But then you will remember this class and how awful those charts are. And you will make simple and elegant graphs which convey all the relevant information in a way that will make your colleagues proud and more importantly, your boss happy. The second part is to actually conduct statistics. In a research methods class, or perhaps pursuing a masters or dissertation, or at some other point in your life, you may want to analyze data. This part will teach you how to do this, how to find out the true story of your data, and how to avoid the biases and mistakes which are lurking around to snatch the unsuspecting statistician. You will be able to find the truth. How this book is structured This book is more practical than math-heavy. It has many examples and will talk about what you should do, why you should do it, and how you should do it. It will try to explain which techniques you do and what situations you do them. I will also teach this using the statistical language R. Computers make stats easy, but the computers are not very easy to use. Many of the software packages are very expensive, but R is free. That’s why I use it. It’s not the easiest thing to use, but once you use it, it becomes very powerful. In fact, R is so powerful, I’m using it to write this book! R can be used to do almost anything in statistics and data science, because it is open-source and thousands of wonderful people have written free extensions, called packages, which allow R to do thousands of things. "],
["chapter-1-statistical-stories.html", "Chapter 1 Chapter 1: Statistical Stories 1.1 Story 1: To catch a thief 1.2 Story 2: The quirks of what we don’t see 1.3 Story 3: What causes cancer 1.4 Story 4: Simpson’s Paradox 1.5 The takeaway", " Chapter 1 Chapter 1: Statistical Stories My main goal for this book is to help you to love statistics. If I’m unable to do that, at least I hope that you won’t be afraid of statistics and possibly find them useful. The best way to start this goal is to give some interesting stories about how statistics can be used. 1.1 Story 1: To catch a thief The worst sin in behavioral science is to make up data. We spend a lot of time collecting our data and so we treat our numbers like they are honest bearers of truth. We turn concepts into numbers and our numbers might be biased, or have errors, or mistakes, but we trust that they aren’t completely made up. However, there can be a lot of profit in making up numbers. Just changing a few digits here and there can be the difference between profit and loss for a company. A researcher might put years of effort into a study and when they found out the study didn’t work, changing a couple 3’s to a 6 or a 7 could be the difference between a big publication and losing their job. But how do you catch someone when they make up data? There’s a secret: real data are less random-looking than fake data. Truly random data aren’t really that random-looking. Imagine a coin-flip, which has a 50% chance of being heads and a 50% chance of being tails. If you’re making up the results of 100 coin flips, you may think that you would make up 50 heads and 50 tails. However, the chance that you would get exactly 50 heads and 50 tails is only 8%. Fifty heads and 50 tails is the most likely single outcome, but it’s far more likely you would get more than 50 heads than getting 50 heads exactly. When we make up our data, we have a tendency to avoid repeats, when repeats are actually quite common in real data. In writing this, I randomly flipped a coin 100 times (actually I had R do this for me). The first 4 results were heads, followed by a tails, followed by 6 heads. This does not seem random at all, right? In fact it’s perfectly random. Made-up data has fewer repeats. Even if we take these properties in account, there might be other properites of random data we fail to analyze. For instance, imagine you wanted to make up numbers for your tax return. You have to create lots of “deductions” that were not real. You assume the IRS won’t audit you if your data looks random enough and so you create lots of expenses for things you did not buy. You make sure your data look random and are spread out. You’re careful that you don’t use the same number all the time, especially with the first digit. You might make up numbers like this: “$36.20, $55.38, $72.22” and so forth. However, even this random list of numbers wouldn’t be random at all and it’s very possible the IRS could catch up this list of made-up numbers because of a property called Binford’s Law. Binford’s Law is the observation that in naturally occuring lists of numbers, or data, the first digit fits a unique pattern. Lower digits, like 1 and 2, are much more common than higher digits, such as 8 or 9. In fact, in a large enough sample, you would expect 30% of the numbers to have 1 as a first digit and 47% to have a 1 or a 2. Conversely, little more than 15% of the numbers start with a 7, 8, or 9. This fact has been used to develop software to flag possible instances of fraud in financial data (Nigrini (1996)). If the numbers submitted do not fit this distribution, an auditing group or investigator may ask for more detailed documentation, which can either prove innocence or, more commonly, prove wrongdoing. 1.2 Story 2: The quirks of what we don’t see What do you think the chance that two people in your class share the same birthday? If your statistics class has more than 23 people, then there is a greater than 50% chance this would be true. This seems very unlikely because the probabiliy that someone shares your birthday is very low. But of all the people in your class, that probability keeps increasing until it’s very high. In World War II, aircraft designers wanted to figure out where to put armor on bombers in order to increase survival. The technicians had noticed that bombers would come back with many holes on some parts of the plane but other parts would rarely have holes in them. So they came to the conclusion that some parts of the plane were more likely to be hit, and so those parts needed more armor. Abraham Wald was a statistician who saw the problem the opposite way. When he was asked about this problem, he suggested putting extra armor on the parts of the plane that had no holes and to ignore the parts that had many holes. The designers were confused, but what Wald was seeing was that there was an important variable missing from the designers’ analysis. They only saw the planes that made it back to the runway safely. There were many more planes that never made it back. Though the designers couldn’t see where those planes had been damaged, Wald assumed that they were damaged in precisely the places which were undamaged in the planes that survived. If antiaircraft gunners are shooting at planes from far away, there’s no way they could aim for specific parts of the plane. The parts of the plane that got hit was random. When certain parts of the plane that were really critical to flying, such as the nose, were damaged, the plane was likely to crash. Therefore, none of the planes that returned had damage to those areas. It looked like those areas were unlikely to be hit, but in reality, those areas were just as likely to be hit, and when those areas were hit, it led to catastropic consequences. (Mangel and Samaniego 1984) Psychology shows us that we have biases in what we see. We don’t pay attention to what is important, but we have a subconscious bias to think what we’re paying attention to is more important than it really is. The key is to find the variables which are unseen. 1.3 Story 3: What causes cancer The headlines were alarming: “Bacon Causes Cancer”, “A Bad Day for Bacon”, and so forth. This was serious and sad news (unless you’re a pig). A scientific article in a very prominent journal said that bacon caused cancer (Bouvard et al. (2015)), and based on this research, the World Health Organization classified bacon and other processed meats as a group 1 carcinogen. Other group 1 carcinogens include tobacco and plutonium. Reading the list of group 1 carcinogens is like reading a list of substances likely to be used in a spy movie, more than one’s diet. So is bacon as bad for you as smoking? Or plutonium? No, absolutely not! And for good reason. There are two related but separate ideas in statistics when we are trying to determine an effect: how sure we are an effect is true and how powerful is that effect. Substances classified as group 1 carcinogens are those which are strongly linked to increasing one’s risk for cancer. We have strong evidence that bacon and other processed meats does increase one’s risk for cancer. But how much does it increase one’s risk? These studies measured something called relative risk or how much one’s risk increases when they do something compared to a person’s risk when they do not do the behavior. A person who eats two slices of bacon a day will increase their risk of colorectal cancer by 15-20%. This sounds like a lot. But this does not factor in something called absolute risk which is how likely something is in the general population. Colorectal cancer is not too common, and only 5% of people will suffer from this. Eating this much bacon may raise your risk from 5% to 6%. For more information, see this.. Conversely, smoking may raise your relative risk of lung cancer over 1000%. Smokers are 15 to 20 times more likely to suffer from lung cancer than nonsmokers. Less than .5% or 5 in 1000 non-smokers will suffer from lung cancer, whereas between 15-20% of smokers who smoke for many years will suffer from lung cancer. In this case the high relative risk translates into a high absolute risk. When people look at diet studies, they often look at relative risk. The examples might be: eating X increases your chance of some medical condition 33%. But these studes do not measure how likely the initial condition is. If a medical condition is very rare, then raising your risk even 100% may not be that big of a deal at all. Another example is aspirin as a medicine to reduce heart attacks. Aspirin can reduce the risk of a first heart attack more than 20%. That sounds great, right? Should we all start taking aspirin? Not exactly. Heart attacks may be relatively common, but only in people who are more disposed to heart attacks. One analysis suggested that if 1000 middle-aged women take aspirin daily for a decade, it would prevent 11 heart attacks. However 22 of those women would suffer major gastrointestinal bleeds that they wouldn’t suffer otherwise. The takeaway is that aspirin can be very effective for those who are at high risk for heart disease, but probably not for everyone. 1.4 Story 4: Simpson’s Paradox One of the most famous quirks of statistics was an analysis of graduate admission data that the University of California: Berkeley did in 1973. They found the following results: Gender Applicants Percent Admitted Men 8,442 44% Women 4,321 35% This showed a clear pattern that women were less likely to be admitted than men and so the administration feared a lawsuit. However, these data were missing one important variable: men and women applied to different departments. If you look at the six largest departments, the following pattern emerges Department Female Applicants Percent Admitted Male Applicants Percent Admitted A 108 82% 825 62% B 25 68% 560 63% C 593 34% 325 37% D 375 35% 417 33% E 393 24% 191 28% F 341 7% 373 6% Looking at these data, it is clear there is no overall pattern of discrimiation. Instead, women are more likely to apply to departments with low admission rates, such as department E and F whereas men are more likely to apply to departments with higher admission rates, such as A and B. In male-dominated departments, women are more likely to be admitted than men whereas there is almost no difference. Simpson’s paradox occurs when data show different patterns depending on how they are broken down. Many times, patterns we see when we look at data in aggregate or all combined, disappear when we break the data down. 1.5 The takeaway Data tells a story and it is our job to help the data tell the most accurate story possible. It’s impossible to be perfect, but we can try to find out what the data are really saying. The important takeaways from these examples is that often it’s what we can’t see that is important and so we need to try to look deeper and find out why things are happening. To do this, we should make sure we look at what is there and try to think about other variables or factors that may not be present. References "],
["chapter-2-how-we-use-statistics.html", "Chapter 2 Chapter 2: How We Use Statistics 2.1 What are data? 2.2 Getting data 2.3 What can we use data for? 2.4 Where do data come from? 2.5 How accurate are our data? 2.6 Claims about data 2.7 Summary", " Chapter 2 Chapter 2: How We Use Statistics In this chapter, you will learn: What are some of the basic terms we use in behavioral science statistics? What are data and where do they come from? What is reliability and validity and how do they influence data? What are the types of claims we make about data? 2.1 What are data? Before we can learn the story of numbers, there are a few terms we must define. I know definitions can seem boring and as a reader, you’re already tempted to skim this chapter. Fight the temptation! Understanding these definitions will make it a lot easier to understand how we use numbers to answer questions. Statistics is a story about data, which is a catch-all term for things we are analyzing. Data can be anything from a random list of numbers to the text of a Shakespeare play. For the purposes of this class, data are a series of individual values called data points, often grouped in variables and observations. A variable is something that we can measure with a series of data points. It is simply a list of data points where each value reflects a different data point. Examples of variables are things such as the names of students in your class, the addresses of your aunts and uncles, the time of day people were born, the height of basketball players, and so forth. Notice that variables take many different forms of data, such as numbers, text, or times. A variable in and of itself may not be that useful. For instance, a variable called age may have the values 33, 21, 48, 12, 69. And a variable called name could have the values “Fiona”, “Serena”, “Octavius”, “Kasper”, “Marshall”, and “Elena”. These two variables together can become more powerful if we connect them. A dataset, as defined here, is a set of data, grouped in variables and observations, or related data points between variables that are connected. When we present and analyze data, we will always use columns to represent variables and rows to represent related observations. Often, each row will represent an individual person, with each column being specific information about a person. For instance, variables might be name, address, sex, age, and birthday. Each row would represent a person. Or, in an experiment, a row may represent a different participant, with their scores on each variable of interest, such as a different column for each type of personality test they took. Here’s an example, with each row indicating a different person and each column indicating different variables: Name Age Favorite type of food Spatial Navigation Test Score Ravi 37 Italian 41 Mandala 24 Peruvian 56 Reese 18 Mexican 39 David 57 Chinese 24 Setting up your data correctly is very important. I would say 50% of the stats questions I get from students doing research projects are about how to set up data. If you set up your data in this format, with rows representing related observations and columns representing variables, you’ll be set. I’ll talk more about this in Chapter 4, but it’s good to start thinking of data this way. 2.2 Getting data When we measure variables, we are interested in measuring populations, which is the group we are interested in measuring. We decide what group we are interested in making conclusions about based on what we are interested in studying. The population of interest can be as big or as small as we want them to be. In psychology, we may be interested in all humans, or a subset, like all children between the ages 2-4, or all younger siblings. Or our groups may be more limited. A pollster might only be interested in registered voters in a specific county. Usually, we can’t measure all the members of a population, so we have to use samples. A sample is any subset of a population. A person conducting a survey may only ask some of the shoppers in a store about their attitudes in order to assess what shoppers overall think about the store. Pollsters only ask a subset of voters to find out which candidates are likely to be elected. The hope with samples is that we can use samples to understand the population. But since samples do not measure the entire population, they are never exact. 2.3 What can we use data for? In my experience, statistics can be used to do three things with data: Describe, Infer, and Predict. Statistics has several branches, and separate branches for each of these processes. The first purpose of statistics is to describe and summarize data, which is generally called descriptive statistics, or the branch of statistics dedicated to using techniques to effectively describe data. This includes summary statistics such as means and standard deviations, which can give us an idea about large amounts of data. This also involves graphs, which can transform many numbers into beautiful and informative figures. Inferential statistics is a branch of statistics concerned with using samples to understand and make conclusions about populations. Samples are subsets of populations and every sample may be different. I may want to find out what colors are popular. If I ask 20 people at random what their favorite color is, I would get one set of results. Then if I ask a different set of people their favorite color, I would likely get different results. Inferential statistics helps us to make conclusions about populations with samples. Models are mathematical functions which allow us to use data to predict. They can be simple lines or complex models which can use millions of variables to predict the weather. Models are never perfect but can be very useful if used correctly. 2.4 Where do data come from? I am a psychologist. I became a research psychologist because psychology is hard. I believe psychologists make some of the best statisticians because psychology is so difficult. Psychology is hard because what we study is really hard to observe. Psychology can be defined as “the study of the brain, mind, and behavior”. But how does one observe these things and then turn those observations into data? Behavior is fairly easy to observe, but psychologists are interested in all those unseen processes in the mind as well. How do you measure a thought? An emotion? A memory? No one in the right mind (pun definitely intended) would deny that there is a mind and that mental concepts like memory, attention, thoughts, emotions, and so forth are real. But how do we measure them? Science used to be about measuring things we could observe with our basic senses. For instance, physicists could observe ideas like momentum and energy. They could agree on basic definitions of these concepts, like agreeing that temperature could be measured by how much mercury expanded in a tube. This ability to objectively observe phenomena is one of the most important elements of science and is one reason many people used to consider psychology not to be a science. To address these concerns, behaviorists wanted to apply this approach to psychology and only study stimuli and behaviors which could be objectively observed and ignore mental processes which cannot be observed. Fortunately, science has moved well beyond this basic definition. Now we accept we can study phenomena that we can’t directly observe if we can operationalize them. Operationalization is where we get our data from. When we measure something, we have a conceptual idea of what we want to measure, which is called a construct. A construct is a set of concepts that is usually subjective and generally has a theoretical meaning. A construct is often something that we know exists and can often easily understand but is often hard to define. For instance, you can try defining time. You can take a minute, or if you’re a philosopher, take a life-time (another awful pun!). We all know what time is, but it’s very hard to explain the concept. Color is another example. It’s really hard to define different colors precisely, and if you’ve ever had an argument about whether something is sea green or robin’s-egg blue, this becomes obvious. But we generally agree on what certain colors are. Taking a shade of color and calling it “blue” is operationalizing. In statistics, we like to be precise about operationalizing. An operational definition is a precise set of steps that takes a concept and turns it into a specific datapoint. Operational definitions have to be clearly defined and able to be reproduced in many different circumstances. Here are some examples of operationalizing color: Take the wavelength of the light reflected by a color and say that certain colors are certain wavelengths as measured by a specific machine. Ask a famous artist to label colors based on their expertise Ask twenty people to choose what word best describes a particular shade of color Each of those definitions has problems. There is no correct way to operationalize a concept but there are many wrong ways, and some ways are clearly better than other ways. Here is another example. I want to operationalize how fast schoolchildren can run. I could do the following: Ask children to tell me whether they consider themselves “fast”, “average”, or “not fast” Have children run a 100m dash and time them with a stopwatch Have children run a 1 mile race and write down what place they finished in, such as first place, second place, etc. Each of these definitions gives us a clear datapoint I can associate with each child. The steps are reproducible, such that any person could generate similar data using the same definition. However, each of these definitions measure different concepts. How fast a person can run is a broad concept. It could reflect speed in a short distance or speed in a long distance. It could reflect how fast people run in a race versus how fast they run alone. Like what was mentioned in the last section, some operational definitions are better than others. If a person wants to make data fit a specific story, then they can change the definition to make that story look better. For instance, if a new police commissioner wants to show that they have reduced crime, one thing they can do is redefine crime itself. For instance, many crime surveys ignore crime in prison, which suggests that crime statistics themselves are biased and placing more people in prison doesn’t reduce crime rates, but only shifts the crime to prisons, where it won’t be counted. 2.5 How accurate are our data? When we use the word “accuracy”, we’re measuring how much our numbers match the thing they are supposed to measure. In science, accuracy consists of two principles: validity and reliability. Reliability is the degree to which our measurements are stable or consistent. For instance, if we measure the same thing repeatedly, will we get the same results? Validity is the degree to which our data reflect the concept we are trying to assess. There are many types of validity, which are much better discussed in a research methods textbook. But I’ll give a brief overview here. One example is my fitness tracker, which is a band I wear on my arm that estimates how many steps I take, floors I climb, and my heart rate. My fitness tracker also tells me how many miles I’ve walked today. A reliable fitness tracker will give the same estimates each time I do the same amount of walking. For instance, if I walk ten laps around a track, the amount of steps it says I took each lap should be similar, since each lap is the same distance. If it measures .2 miles one lap and then .9 miles the next lap, the reliability is probably very low. High reliability doesn’t necessarily mean high accuracy. Imagine every time I run a mile, the fitness tracker says I only ran .8 miles. The readings are reliable, because they are close to one another, but they are reliably underestimating my distance. Or likewise, my tracker might double-count each step. Each time I run a mile, it suggests I ran 2 miles. It is reliable but it is reliably overestimating my data. Validity is a much more philosophical concept than reliability, and one that is often outside of the scope of statistics. At its simplest, validity represents whether the data I collect are a good measure of the concept. In the steps example above, it captures how well my fitness tracker actually captures steps. If the fitness tracker is close to the actual number of steps, it is valid. However, the true concept of validity is beyond accuracy. This is because validity also represents how well variables are operationalized. Simple to operationalize variables like steps are easy to examine, but most variables in psychology are harder to operationalize. Many examples of how people misuse statistics come from using less valid operational definitions. Like I mentioned above, redefining crime by not counting prison crime may cause crime rates to decline as more people are incarcerated. This may make it appear like crime is less likely and that higher rates of incarceration reduces crime. If you are trying to count the number of times that a crime happens, this might not be a very valid definition. However, if you’re trying to use crime as an index of how safe it is outside of prison, prison crime is probably less relevant. The number of assaults in a prison has very little to do with one’s likelihood of being assaulted. The final thing to note is that variables can be either reliable or valid or both (or neither). A reliable but not valid variable will give the same answer each time, but overestimate or underestimate the variable. For instance, if I have a blood pressure machine that always reads my blood pressure as 10 points higher than it really is, it is reliable but not valid. A variable can be valid but not reliable. If for instance my fitness tracker sometimes overestimates and sometimes underestimates my steps, but on average is pretty close to the actual number of steps I take, it would be valid but not reliable. 2.6 Claims about data We want to figure out what kind of stories data are telling. Many times we do this by having specific questions and using data to answer those questions. In psychology, we often do this by designing experiments with the purpose of answering specific types of questions, which I will call claims. 2.6.1 Frequency claims The first type of claim are frequency claims. These are claims about how often or how frequently something happens. If I am interested in how frequently something occurs in a group of people or if something is more frequent in one group of people versus another group of people, this would be a frequency claim. A frequency claim may be a poll examining what percent of people will vote for a specific candidate. It might be an analysis about what percent of the population has a specific attitude or belief. It could be a measure of whether a mental disorder occurs more frequently in a certain sample than in the population at large. It could be a comparison between two groups, like if I set up an experiment comparing children who saw a violent TV show versus children who did not. My hypothesis is that a greater percentage of children who saw the violent TV show would engage in violent play than the percentage of children who did not see the violent TV show. Frequency claims almost always compare categorical variables, which are variables that only indicate what category a person is in and nothing else. A categorical variable can be a word, such as “female” or “student”. They indicate what group a person is in. There can be cases where categorical variables are dichotomous or where there are only two options for groups. Often these are cases where the options are yes or no, like if I indicated whether a person is a smoker or is not a smoker. Categorical variables can also be numbers, but in this case the numbers are only indicating what group a person is in. They do not have any other meaning. Zip codes are an example. For example, the zip code 33472 is not 10000 more anything than the zip code 22472. In this case, the numbers only tell us what category a person lives in. Whether a zip code is higher or lower means nothing; it could just as easily be expressed as a word. The important reason for this is because doing math on categorical numbers is meaningless. For instance, the total zip code of a group of data would be meaningless. In some cases, we dummy code categories using numbers. For instance, I may code a variable indicating smoking status as 0 for non-smoker or 1 for smoker. Even though I’m using numbers, the numbers only indicate category and the fact that one category is coded 0 and the other 1 doesn’t mean the latter category is more or better or higher in any way. 2.6.2 Association claims Association claims are claims where a person is examining whether two or more variables are related to one another. This includes ideas such as correlation or even examining whether one variable predicts another variable. In association claims, sometimes we just want to see whether two variables are related. For instance, we could examine if there’s an association between how extraverted a person is and how many friends they have. Or I could examine whether there’s a an association between a person’s relationship satisfaction and how much time they spend with their partner. Or I could see whether the amount of words in a list is associated with how many words a person remembers. In some association claims, I want to examine whether two variables are related without having any idea whether one variable is causing the other variable. However, in other cases, I may have a clear idea that one variable is causing another variable. For instance, I may have the hypothesis that the amount of time a couple spends together is related to their relationship satisfaction. In that case, I would call the amount of time a predictor variable and the relationship satisfaction the outcome variable. Sometimes people call these independent and dependent variables respectively, though independent variables usually refer to variables where the experimenter manipulates something. Association claims are measured by correlations and regressions. The key to know about association claims is this: even though we may have an idea which variable causes the other variable, we can never know for sure. This is why I go around shouting “correlation does not imply causation”. I may think that the amount of time a couple spends together is causing their relationship satisfaction, but the opposite may be true. Relationship satisfaction may be the thing which causes a couple to spend time together or not. Correlation may not imply causation but often it’s as good as we can get in science. This is because it’s often unethical or impossible to test things using real experiments, where we carefully manipulate a variable and see how it affects another variable. One example is the well-known link between smoking and cancer. This is only a correlation because we can’t do a study where we assign one group to smoke for many years and assign another group to not smoke for many years and examine which group gets cancer more. That experiment would be impossible. When comparing the biological evidence that there are many carcinogens in cigarette smoke with the strong correlation between the amount and time a person smokes and their likelihood of cancer, we can make a very strong case that smoking causes cancer. 2.6.3 Mean claims The third type of claims are what I call mean claims or claims about the mean (or average) of a sample. These claims are questions examining whether the mean of something in a sample is different from a population or whether the mean of something is higher in one group than another group. I might examine whether there is a difference in the mean GPA of high schoolers who play sports versus those who do not. Or I might want to look at whether the mean score on an anxiety measure is lower in a group of people who meditate regularly than the population at large. Another example might be looking whether there is differences in mean reaction time in a group that drank no coffee, versus a group who drank one cup, or a group who drank two cups. In all those cases, I have a measure which can be turned into a number for each person in my study. Then I look at whether the mean of that number (the average) is different in one group versus another group or whether the mean of a sample is the same or different from the population. A mean claim is different from a frequency claim because a mean claim involves a variable that is not categorical but can be a number which is averaged. Mean claims involve means which are usually not percents or proportions or frequencies. 2.7 Summary To summarize, there are a lot of things we can use statistics for. In this chapter, you should have learned about the following terms: Principles of data Data Observations Population Sample Types of statistics Descriptive Statistics Inferential Statistics Models Where do data come from Construct Operationalize Reliability Validity Types of claims Frequency claims Categorical variable Dichotomous variable Dummy code Association claims Predictor variable Outcome variable Mean claims "],
["data-distributions-and-descriptive-statistics.html", "Chapter 3 Data, Distributions and Descriptive Statistics 3.1 The idea of describing data 3.2 Distributions and summarizing data 3.3 Plotting distributions 3.4 Using Descriptive Statistics to Characterize Distributions 3.5 The importance of descriptive statistics 3.6 Summary", " Chapter 3 Data, Distributions and Descriptive Statistics In this chapter, we will cover How we think about and format data How we can summarize data using distributions Shapes of distributions and percentiles What descriptive statistics are and why we use them How we can summarize central tendency and variance When you are done with this chapter, you should be able to know or do the following: 3.1 The idea of describing data One of the most powerful things about the internet is how it has made it so easy to find out information about which products to buy, or things to do, or shows or movies to watch. The ability to seek out hundreds or thousands of other people’s reviews means that there is a ton of information at our fingertips. If I want to find out what movie I should watch this weekend, I may go look up reviews about the movie. I could read through many people’s opinions, at the risk that their opinions might spoil the movie’s ending, or I could do what I think most people do and look at the movie’s ratings. Several internet sites give different opinions about movies and how they are rated. Did this movie get 7 out of 10 stars? Or what percent of experts gave the movie a positive rating? Or if I wanted to buy a toaster online, I could look at the reviews. I like cheap things, but I don’t want cheap things that break easily. So I could read hundreds of reviews on a shopping site online, or just look and see at it’s rating. If two toasters each cost $40, and one toaster has a rating of 3 out of 5 stars and the other toaster has a rating of 4.5 out of 5 stars, it’s obvious which one I would buy. These are all concepts of organizing data. When you see that a movie got 77% positive reviews or a toaster got 4.5 out of 5 stars in a rating, you’re seeing a lot of data organized to get an important message out of the rating. Describing data is the art of taking large amounts of data and synthesizing them into a simpler message. Essentially, we are trying to figure out what the story of the data is and answer the question that we are interested in. Datasets can tell many stories, and the key is to find out what story we are interested in and how we will go about finding out the answer by summarizing data. 3.2 Distributions and summarizing data Distributions are a way of representing how frequently a specific value occurs in a data frame. This can tell us about patterns in data very quickly. Here’s an example. I asked twenty preschool children to tell me what their favorite color was. (This is a great way to get preschoolers to talk). I got a list like this: blue &nbsp; &nbsp; &nbsp; green &nbsp; &nbsp; &nbsp; blue &nbsp; &nbsp; &nbsp; yellow &nbsp; &nbsp; &nbsp; red &nbsp; &nbsp; &nbsp; blue &nbsp; &nbsp; &nbsp; blue &nbsp; &nbsp; &nbsp; red &nbsp; &nbsp; &nbsp; pink &nbsp; &nbsp; &nbsp; red &nbsp; &nbsp; &nbsp; black &nbsp; &nbsp; &nbsp; black &nbsp; &nbsp; &nbsp; blue &nbsp; &nbsp; &nbsp; green &nbsp; &nbsp; &nbsp; pink &nbsp; &nbsp; &nbsp; purple &nbsp; &nbsp; &nbsp; yellow &nbsp; &nbsp; &nbsp; red &nbsp; &nbsp; &nbsp; purple &nbsp; &nbsp; &nbsp; blue &nbsp; &nbsp; &nbsp; If you asked children their favorite color, you are probably not interested in only having a list of all the responses. You may be more interested in questions such as which color is most commonly picked or how many different colors are chosen. Each of these questions is more easily answered by a frequency table. A frequency table is a table which records each possible response of the variable and then lists how often that response happened. In this case, the table would be: Color Frequency black 2 blue 6 green 2 pink 2 purple 4 red 4 If you look at the list carefully, some patterns become very apparent. Blue is the most common color, followed by purple and red. Also, there are only 6 colors chosen, and certain colors, like white and brown, are not chosen. The frequency table allows us to see all of that at a glance. Frequency tables work very well for cases where there are only a few possible values for each data point. In this case, there are only a few colors that preschoolers can select. However if I asked art majors this question. I might get responses like “robin’s egg blue” or “cerulean” or “mahogany”. If I had a set of data where 50 people each chose a different color, a frequency table would be useless. Sometimes, it helps if we group data into categories that correspond to several different possibilities. We could do the following: with my limited art skill, I grouped colors into the categories cool: (blue, green, purple), hot: (red, yellow, pink), and other (black). Color Frequency Cool 10 Hot 8 Other 2 Frequency data can also work well for outcomes that are numerical. Imagine I have ratings for a new restaurant, and the ratings are a list like this: 3, 5, 2, 4, 4, 4, 1, 1, 3, 5, 5, 4, 5, 3, 5, 3, 4, 5, 5, 2, 5, 1, 1, 1, 4, 5, 1, 4, 5, 3, 3, 4, 1, 2, 5, 2, 4, 1, 5, 3, 5, 3, 5, 5, 5, 1, 5, 1, 5, 4, 4, 2, 5, 4, 3, 5, 3, 4, 4 and 4 Yuck! That is a long list of numbers. A frequency chart would look like this: Rating Frequency 1 10 2 5 3 10 4 15 5 20 In this case, it’s a lot clearer to see what the pattern is. The ratings seem to be higher than average, with most people giving a fairly high rating and only a smaller percentage giving two stars or less. With numerical data, we have to consider whether our numbers are discrete or continuous. Data are discrete when there are no possible fractional values. For instance, a person can only have a whole number of children. Money can only be in certain specific chunks. Except for gas stations and stock markets, fractions of a penny are impossible. Not all data are discrete. For instance, both my wife and I are five feet, eight inches tall. Does that mean we’re exactly the same height? No. One of us is taller than the other (a question that is still up for debate). We say we’re 68 inches tall, but each of us is not 68.0000000 inches tall. In reality, my height is something like 68.24238452… inches tall, if I could measure height that precisely. Continuous data are data where there are an infinite amount of possible values between each number. Anything we measure, rather than we count, is continuous, because our measurements are never exact. For instance, I can say for sure that I have exactly $34.11 in my wallet. Money is a discrete variable. Even though cents are expressed as decimals, you can’t have between 0 and 1 cent (unless you’re at a gas station). However, time is a continuous variable. I ran a mile in 9 minutes, 11 seconds. I run a mile and time myself using a stopwatch, getting a result of 9 minutes, 11 seconds. If I had a precise timing mechanism, I might be able to say I ran the mile in 9 minutes, 11.42347289 seconds. Even with more precision, I can’t know exactly how long it took me. When we analyze continuous data, it is often useful to make continuous data into discrete data by binning or combining related data into ranges called bins. Binning is natural and happens all the time. When I ask anyone how old they are, they don’t give me answers like 3.5482828398438. They just say 3 years old. We naturally assume if you are between 3.000000 and 3.99999 years old, you will say 3 years old. The bins don’t have to be whole numbers. Young children often will be very proud of their half years and say “I’m 4 and a half years old”. In that case, the range would be anywhere between 4.500000 and 4.9999 years old. Likewise, someone might say “I’m in my 40s”, which would mean a range between 40.000000 and 49.999999 (though some people might change the values in that range). When making frequency charts using continuous data, you have to decide what size of a bin to use. For instance, I could have a chart of times it took a rat to complete a maze: Time (in minutes) Frequency 0-.99 3 1-1.99 8 2-2.99 16 3-3.99 11 4 or more 5 The important thing in deciding what size bin to use is to choose a size that allows you to see the pattern. Too few options and the chart may look like this: Time (in minutes) Frequency 0-2.99 27 3 or more 16 In this case, the pattern is obscured. We can’t see that a few values that are really fast and a few that are really slow and most values are in the middle. Finding the right bin size takes a bit of trial and error. 3.3 Plotting distributions When we make a frequency table, we can present the data using graphs. Traditionally, bar graphs are the way that frequency tables are presented. A bar graph is used when frequency tables are presented with different categories. Each of the categories are on the x-axis and the frequency of each of the events is on the y-axis. An example of a bar graph is below, plotting out the color data from before: In a bar graph, each of the bars are separate, with space between them. That is because each bar represents a category. Order is statistically irrelevant in a bar graph; I could have put these bars in any order, but alphabetical order made sense to me. A bar graph is used when data are discrete: that is, data are broken up into indivisible categories. When looking at class standing, there are only four possibilities and a person fits into only one possibility. All categorical and ordinal data are discrete, as are many forms of interval and ratio data. When I graph a frequency chart using continuous data, I make something called a histogram. This is different than a bar graph. In a histogram, the bars touch each other, because they are not separate categories, but bins that represent ranges of numbers. An example histogram is below. Each bar represents the number or frequency of observations in a certain range. This histogram goes from 6.5-7.499, 7.5-8.499 and so forth. The tall bar represents observations between 8.5 and 9.499. If we use a line graph rather than a bar graph, we can create a distribution called a frequency polygon. This is made by placing a point at the center of each bar in a histogram and then connecting those points with lines. This gives us an estimate of how many values are at every value. Here’s an example: The final way we may graph histograms and frequency polygons is that instead of having an absolute count of how many data points are a certain value, we change that into a percent, and the bar or line represents what percent of the total observations are in each category. This is called a probability density plot and is very important for understanding how a distribution is affected by probability, which we’ll talk about in another chapter. Distributions are very important because they help us to interpret what a data point means. For instance, I gave an intelligence test and found that a person scored 138. Is that a good score? It’s impossible to know. Another example: a three-year old weighs 33lbs. Is she underweight, overweight, or a proper weight? To answer questions like this, we are interested in a value’s relative position in a distribution. We use this by the concept of percentiles. A percentile is defined as the percentage of the distribution that is below that value? For instance, if I score an 82 on an exam and 40% of the people score below me and 60% score above me, I am in the 40th percentile. Likewise, if a toddler weighs 33 lbs, and 70% of toddlers of the same age weigh less than that toddler, then he is in the 70th percentile. To calculate what an observation’s percentile rank is, you follow this formula, where \\(N_{below}\\) equals the number of observations below a given observation and \\(N_{total}\\) equals the total number of observations in a variable. \\[Percentile = \\frac{N_{below} }{ N_{total} - 1}\\] Here’s an example. For instance, imagine if you took a test and got an 84 on it. You are in a class of 25 people and 11 people did better than you and 12 people did worse than you. One person got the same grade as you. To calculate percentile, you would type: \\[\\frac{12}{ 25 - 1} = 0.522\\] This would be called the 52nd percentile, since percentiles, like percents, are decimals. This person scored better than 52% of the people in the class, which helps us to know more about their relative performance. 3.4 Using Descriptive Statistics to Characterize Distributions Distributions have characteristics that can tell us about the underlying data and help to answer our questions. 3.4.1 Central tendency The first characteristic of a distribution we consider is called central tendency, or a way of trying to determine which value is the most characteristic of a distribution. In essence, this property represents if we take an entire set of data and want to say which value best represents the data. Colloquially, we often call this the “average” of a variable, though the word average often means a specific way of measuring tendency. The first way we can discuss central tendency is by the mode of a variable. This is the value (or values) which are most common in a dataset. Calculating a mode is simple; figure out which value is most common. If two values are equally common, the data is called bimodal and both values are the mode. In the example above for ratings, the mode would be 5. In the color example, the mode would be blue, since blue was the most common color. The mode is useful because they can be calculated for any kind of data, nominal, ordinal, interval, or ratio. The mode can also be computed for non-numeric data as well. However, the mode only tells us about the peak of the data, and the mode can be biased if that most common value is not representative of the rest of the data. Here’s an example: I have people rate on a 5 point scale how much they like a certain restaurant, with 1 being very low and 5 being very high. I get the following results: Rating Frequency 1 30 2 28 3 2 4 5 5 31 My mode would be 5. Now I take a second restaurant and get the following rating: Rating Frequency 1 3 2 4 3 8 4 33 5 31 In this case, the second restaurant has a mode of 4, which is less than the first restaurant. But would you rather eat at the first restaurant? Probably not, because most people did not rate that restaurant highly. The mode only incorporates some of the data, so it can miss some important trends. To incorporate all the data, the most common measure of central tendency is the mean. This is technically called the arithmetic mean and is colloquially what is meant when people say average. To calculate the mean, you would do the following, assuming you have a variable called X with N elements. Simply, this is that you add all the numbers in your variable and divide by the number of values in the variable. \\[\\mu = \\frac{\\sum{X_{i}}}{{N_{i}}}\\] In stats, we refer to the mean of a population by the Greek letter mu, \\(\\mu\\) and the mean of a sample by “X bar”, or \\(\\bar{x}\\). We’ll use these symbols a lot in this class. Means are the most common way to deal with summarizing data but there can be a problem using means in some instances. Means are very susceptible to outliers. Income is a great example of this. Imagine a company with 20 workers who make $50,000 a year and a CEO who makes $1 million a year. A person recruiting for the company might say “the average income for a worker is $95,238”. But does that really reflect reality? Another example is trying to say “what’s a person’s average income”. In 2014, the mean income for a household is $72,641. But if you look at data for income, having a household income of $72,641 would put you in the 62.5th percentile, so that a person with “average” income makes more than 62% of people. The reason for this is because income data are skewed. A skewed distribution is any distribution that is asymmetric and therefore most of the data points are either above the mean or below the mean. A distribution with positive skew will have most of the data below the mean and outliers that are above the mean. However, a distribution with negative skew will have the reverse, where most of the data are above the mean and a few outliers are below the mean. The graph below has positive skew, with most of the values below the mean, which is marked with a bright red line. Compare this to a graph with negative skew, where most of the data are above the mean. Income is a great example of a distribution with positive skew. A few people make a lot of money and many people make less money. So if we want to quantify how the “average” American is doing, we don’t care if a few people are extremely rich make $1 million or $1 billion a year. We are more concerned about a person who is neither rich or poor. The final measure of central tendency we might use is the median. The median is defined as the 50th percentile, or which score is at the middle of the distribution. To calculate a median, you do the following steps: Order your scores from least to greatest If you have an odd number of scores, take the middle score. That is the median. If you have an even number of scores, add the two scores in the middle and divide by two (taking the mean of the two scores). Medians are robust to outliers. Adding one extremely high score or one extremely low score will not change the median very much. That said, for various mathematical reasons, we usually like to use the mean to describe data unless we have good reason to do otherwise. 3.4.2 Measures of Variance Central tendency tells us what the middle or the “average” member of a distribution is. Another important property of distributions is the idea of variance, or how spread out distributions are. Variance tells us if the observations in a distribution are close together or far apart. Imagine I have two books and wanted to test what people thought about them. I give a series of people the book to read and ask them to rate how good the book was on a 10 point scale. For instance, I might get the following ratings: Rater Book1 Book2 A 6 3 B 4 4 C 5 6 D 7 4 E 4 3 F 3 5 G 3 2 H 5 10 I 6 10 J 6 1 K 6 9 L 5 3 If we look at these data points, the mean for each group is the same, with the mean for both groups equal to 5. However, it’s clear that there is a different story happening here, especially if you look at the distributions. There is more disagreement in what people thought about the second book. The first book is consistently average, but people have a lot more disagreement about the second book. Some hate it, some like it, and some are in the middle. You’ve probably seen this concept in what people think about stories, whether it’s books, television, or movies. Some stories are consistent where people think the same about them, whereas other ones become “cult” hits, which have a small fanbase who passionately like the movie or book whereas many people either don’t care or may really dislike it. This disagreement is a concept called variance and it reflects the amount of spread in the data. To think about it mathematically, it is a measure of how far the “average” person is from the middle of a dataset, usually defined as the mean. If there is very little variance, then most of the data are alike. For instance, imagine a high school where students wear a uniform. Their clothes are not all alike, because there are subtle things they can do to show their individuality. But there is very little “variance” in the data. On the other hand, a high school with no dress code would have much more variance in how people dress. One measure of variance is using range, or measuring how spread a distribution is. The range is the largest score minus the smallest score. Range gives us a way to understand how spread data are, but range has a few problems. Range is only affected by two scores, and by definition it is taking the highest and the lowest score. If there are outliers in the data, they will either be the highest score, or the lowest score, and so they will affect the range. Because of this we use another measure of variance, the standard deviation. The standard deviation is a measure that gives us essentially how far on average, each score is from the mean. Standard deviation is defined as the following: \\[s = \\sqrt{ \\frac {\\sum_{i=1}^N (x_i - \\overline{x})^2} {N-1}}\\] This is a complicated formula for a simple concept. Here’s a step-by-step to explain what this means. First we calculate the deviations, which is taking each score in a distribution and subtracting the mean from each score. A small deviation is close to the mean whereas a large deviation is far from the mean. As you can see in the table below, the deviations tells us a story about the data. There are very small deviations for Book 1 but very large deviations for most of the observations of Book 2. Rater Book1 Book2 Book1.deviation Book2.deviation A 6 3 1 -2 B 4 4 -1 -1 C 5 6 0 1 D 7 4 2 -1 E 4 3 -1 -2 F 3 5 -2 0 G 3 2 -2 -3 H 5 10 0 5 I 6 10 1 5 J 6 1 1 -4 K 6 9 1 4 L 5 3 0 -2 Now we want to combine the deviations into one number. We can’t take the mean of the distributions because that would always equal zero. So what we do is we square each of the deviations and take the sum of the squared numbers to create a property called sum of squared deviations or sums of squares. The sums of squares are an interesting mathematical property of a distribution and one that will come up later. However, there is one problem with this property, it’s biased by how many observations we have. As we add more values, sums of squares will always go up. A distribution with twenty observations with the same amount of variance as a distribution with ten observations will have a much higher sums of squares, just because we are adding. To account for this, we divide sums of squares by the number of observations. For various mathematical reasons which are well beyond the scope of this book, this number is slightly biased when discussing samples, so it is better to divide by the sample size minus 1. In the example of the books, where we have 10 observations, we would divide the sums of squares by 9. This quality is called variance, which is defined as the sums of squares divided by the number of observations minus 1. Variance is written as the Greek letter sigma squared, \\(\\sigma^2\\), if it is the variance of a population and \\(s^2\\) if it is the variance of a sample. In the example above, the variance for book one would be: \\[s^2 = \\frac{4}{10-1} = .44\\] And the variance for Book 2 would be: \\[s^2 = \\frac{148}{10 - 1} = 16.4\\] Variance is a good measure of deviations, but one problem is that it is not on the same scale as the original units, because we squared the original deviations. To fix that, we often take the square root of the variance to get the quality called standard deviation. Standard deviation is the most common way to measure the amount of spread in data. It represents the average amount that each observation deviates from the mean. Since standard deviation is the square root of variance, it is written as the Greek letter sigma, \\(\\sigma\\), if it is the square root of a population and \\(s\\) if it is the standard deviation of a sample. So in the example of the books, the standard deviation for book 1 would be the square root of .44, or .66 and the standard deviation for book 2 would be the square root of 16.44 or 4.05. In this case, when rating book 1, each person is on average only .66 points from the mean. For book 2, each person is more than 4 points from the mean. Now, you might wonder why the concept of variance matters for you. Here’s one example. Imagine I want to buy a vacuum cleaner, because those are the important purchases once you settle down. I find two models online and look at their ratings. People give the vacuum between 1 star and 5 stars. Both of these vacuums have ratings of 4 stars. One vacuum has a standard deviation of .1 whereas the other has a standard deviation of .5. What this is telling me is that there’s a lot of agreement about the first vacuum cleaner and that most people give it 4 stars. However, there’s less agreement about vacuum cleaner 2. Vacuum cleaner 2 certainly has more 5 star ratings than the other vacuum cleaner. However it has more lower ratings. What this means is that people are more inconsistent with the second vacuum cleaner. Perhaps vacuum cleaner 2 is better but more customers happen to get a broken model and give it very low ratings. Perhaps vacuum cleaner 2 is harder to use and some people can’t figure it out, whereas those who figure it out really love it. The standard deviation helps to tell the story as to whether I’ll pick a vacuum cleaner that I’ll be sure to like somewhat or should I take the chance and get one I might love. To summarize, there are several steps involved in getting the standard deviation. I’ll summarize them below: Calculate deviations, or how far each score is from the mean Square the deviations and add these squared deviations to get sums of squares Divide the sums of squares by the number of observations minus 1 to get variance Take the square root of variance to get standard deviation. 3.5 The importance of descriptive statistics As I mentioned above, distributions and descriptive statistics allow us to simplify data and examine what data are telling us. I have described this like a story. All data have a story and your job is to find the story through your analysis. This is what descriptive statistics is about. There are no right answers when it comes to which statistics to use. This is why statistics is hard. But, there are a lot of wrong answers, and some really wrong answers. This is why I have this section to know what descriptive statistics to have in the behavioral sciences. In this section, I have some common uses of descriptive statistics in behavioral science and a few pointers about what to avoid. 3.5.1 Summarizing Populations We use descriptive statistics to summarize characteristics about who participates in our studies. This helps us to know at a glance who is doing a study. For instance, I may have a study where I test whether using light therapy (bright lights) helps people with seasonal affective disorder (SAD). I may have a randomized experiment where I assign some individuals to receive light therapy and other individuals not to receive light therapy. My idea is that the groups who receive the treatment would have a reduction of their symptoms compared to those who do not receive the treatment. It’s important to know the characteristics of each group of participants and descriptive statistics help with this. Central tendency can let us know who the “average” participant is. At a glance, we can use the mean to see what kind of participants are in each group. Are they older? Younger? We can also use frequencies to examine the gender and racial background of the participants. In this case, we can examine if these groups are similar. Variance is also very important in behavioral science. In some cases, we are interested in variance and we want to encourage higher levels of variance. If I design a memory test, I want it to separate between people who have higher levels of memory versus lower levels of memory. If everyone scores exactly the same, the test has no usefulness. Like with central tendency, understanding variance lets us know how diverse our sample is. If the sample is very similar, it will have little variance. In some cases, this can be a good thing because it allows us to control for other issues which may affect our studies. However, this has been one of the biggest issues with behavioral science research for many years. The “average” participant is often selected from undergraduate students. College students are not diverse. They are almost always younger and do not reflect the racial and cultural diversity of the world. This lack of diversity has affected many theories. 3.5.2 Mistakes with descriptive statistics People misuse descriptive statistics all the time. This is part of the reason that many people think that the word “statistics” is just another synonym for “lies”. It’s not a coincidence that the most popular statistics book of all time is How to Lie with Statistics. But we want to be better than that, and so here are some mistakes people make with descriptive statistics and how you can avoid them: The first mistake is only looking at the mean. This is a mistake a lot of people make when evaluating statistical data. The mean is the most common way to summarize data but when people only look at the mean, they can miss a lot of things. I was recently looking at buying soccer nets for my daughter’s birthday and since I had no idea how good the different products were, I was looking at the reviews on the online shopping website I used. Like most websites, they allow people to review products and you can sort the products for the “highest review first”. This looked like a good idea but it was actually useless. The first product had a mean of 5 stars, but it only had 3 reviewers. Another product buried several pages later had with a mean of 4.5 stars with 1000 reviews. Which one would you choose? You’d probably be better off choosing the one with more reviews. If you missed how many reviewers there were, you might miss important data. Later in this book, we’ll talk about why sample size is important and why we can be more certain of our conclusions if we have larger samples. Another mistake is ignoring the distribution. Sometimes, we have outliers which can really skew our data. In a lot of psychology studies, people like to use reaction time as a way to understand how the brain works. If a person responds more quickly in one condition than another condition, this can tell us that there’s a stronger connection between the concepts in the first condition than the second condition. In one study I did in graduate school, we were investigating whether people recognized anger more quickly if it was a male face that was angry than on a female face that was angry. Part of the reason for this is that there is a stereotype that men are more likely to be angry than women. In this experiment, it is very easy to recognize anger quickly. It takes participants a mean of 750 milliseconds (3/4 of a second) or so to recognize and label the face. However, reaction time can have serious positive skew. In one trial, a participant took 10,000 milliseconds (or 10 seconds) to label the face. This is such an outlier that it threw off the entire mean for that condition. In our experiment, it looked like people were faster to recognize anger on female faces than on male faces, which is the opposite of our prediction. However, if you removed that one trial, the pattern reversed. So what happened here? I don’t know for sure, but the person took a break during that one face. Maybe they sneezed when that face appeared and was too slow to respond. Or the participant could have fallen asleep for a moment, or yawned, or anything. In this case, it’s unthinkable that a person could take 10 seconds to figure out whether a face is angry or fearful, when in all the other trials this person only took less than a second. This is clearly an outlier. This is why in reaction time research, people usually remove trials that take a very long time, like more than 2 or 3 seconds. The final lesson is this: look at your raw data in addition to your descriptive statistics. I had found an interesting research finding, one that was so interesting, I was scheduled to give a talk when I would discuss this new data. I was excited because it was novel and all the descriptive statistics looked nice. However, I never looked at the raw data until a little while before the talk, when I noticed something odd. I had an outlier in the data. It was hard to notice this outlier from looking at the distribution, but it was a value that was miscoded and was something which is physically impossible. When I fixed this issue, one of my interesting findings was no longer true. I had to report this mistake. So I was left with a quandary: should I give up my talk, or do the talk and report I made a mistake? This was a nerve wracking decision, but then a miracle happened and I was saved. A snowstorm came the day before I had to fly to my conference, and since snow completely paralyzes airports in the Southern US, my flight was canceled and I couldn’t go to the conference. I didn’t have to make the talk and I let the mistake be my little secret. Until now, when I give you my story as a warning. The summary of this section is simple: look at your data. Look at your raw numbers, look at the distributions, look at the histograms, and look at the mean, median, and standard deviation. If there are any oddities, this can be adjusted. However, many people have gotten into trouble because they didn’t look at their data. 3.6 Summary Descriptive statistics allow us to simplify data to understand the main point about a set of data. It allows us to understand at a glance what the properties of data are and what the data are telling us. Here’s a summary of a few principles from descriptive statistics. Frequency distributions allow us to see how data are distributed, which values are most common, and what shape the data have. We can plot frequency distributions on a graph to create bar graphs for categorical data and histograms for other data. When connected with a line, we can create frequency polygons. If the bars represent the percent of values that are in a specific category, we can create probability density plots. Central tendency is a concept representing which value best represents what is the prototypical or “average” observation. We can use the mode, median, or mean to examine this. The mean is most commonly used, but is susceptible to outliers. Variance represents how spread data are and reflects how far on average each score is from the mean. Variance is measured through standard deviation. Whenever you do an analysis, look at all the descriptive statistics and look at the distributions of your variables. This can help you find out interesting stories in your data and tell you about problems which might occur. "],
["introduction-to-r.html", "Chapter 4 Introduction to R 4.1 What is R? 4.2 The basics of R 4.3 Inputting data into R 4.4 Descriptive statistics in R 4.5 Summary", " Chapter 4 Introduction to R In this chapter, you will learn about the R statistical language and how to use it to analyze data. Specifically, we’ll cover: What is R and how do we use R and RStudio to do statistical analysis What is the basic structure of R? How does R work? This will include understanding what variables, functions, and packages are. How do we enter data into R? This will involve understanding how to use packages to import data that comes from other sources? How do we do basic analyses in R? This will involve understanding how to use functions to calculate descriptive statistics. 4.1 What is R? I have a secret to share. I used R to write this book. All the (not so) pretty graphs and functions were generated in R. I even used R to make up some of the data that I “collected” in other chapters. So R is powerful and done right, it can be beautiful. In this book, I’ll discuss how to do some of the statistical tests and calculations by hand, but everyone uses computers to do this today. Once you try to calculate some of these stats by hand, you will start to understand why this is. Since computers are amazing at crunching numbers, there are a lot of different programs which can do statistical analyses. In my experience there are three types of programs you can use: Spreadsheet programs: Examples include Excel, Google Docs, etc. These are programs where you can enter lists of numbers and do basic calculations. They are good for entering data and I usually use them to enter in numbers. They only have limited statistical capabilities. Once you are beyond means and standard deviations, these programs are not easy to use and can even lead to some big mistakes. Commercial stats programs. Examples include SAS and SPSS. They are programs written for doing statistical analyses. They can do a lot of different things, and many of them are relatively easy to use. However, they are very expensive, with licenses that can be in the thousands of dollars. Unless you are independently wealthy or work somewhere which has bought one of these programs, you may not get to use them. Open source programs: Open-source programs are programs where the code is available for anyone to use and modify. Volunteers write these programs and give the code away for free. So, anyone can modify and edit it to make it what they want. This means that most open-source programs are free. Free is good, right? However, there’s a downside. Since the people writing the program are volunteers and are not getting paid to write the program directly, these programs can often be hard to use. R is an open-source statistical programming language. A programming language is a set of commands that are interpreted by an interpreter to do something. What that means is that a user will give R a command, or a list of commands, and get an output. So instead of clicking with a mouse, what a user will do is type a command and get an answer. A command could be something really simple. Here are some commands written below. In this book, the commands will be written in this blocky font and will often have the color highlighting that’s listed below. 3 + 5 mean(c(22,44,99,311)) numbers = 1:100 t.test(DV~IV, data=mydata) print(&quot;I love R&quot;) R comes with a set of commands built into it called the base package. These are the first commands made with R and they cover basic math functions, data entry functions, and statistics functions. But the power of R is that there are functions to do almost anything that any statistician would ever want. And if there is a new stats test or new function that doesn’t exist, you can write your own. The best part is that you usually don’t have to write your own because someone else has probably already done it. People have written thousands of add-on packages you can add to do all sorts of analyses. We will use a lot of these packages this in this class. So R is free, and has thousands of add-ons. It sounds wonderful, right? If it weren’t free, wouldn’t I be trying to sell it? But there’s a catch. R is a programming language and that’s difficult to use if you don’t have experience programming. Plus, as programming languages go, R is not the easiest to use. So you might struggle with R. However, whenever you struggle with R, just remember you can pay several thousand dollars and get a commercial program which is only slightly easier to use. Now, how do I get R. You can download R from the cran-r-project website: https://cran.r-project.org. This gives versions for Windows, Mac, and Linux. The website has installation instructions. Once you install R, you should see something like this. Basic R installation If you click around, you may find this hard to use. It has some menus but they are not very helpful. What I recommend doing is downloading a program called RStudio to use with R. RStudio is a program called an integrated development environment (IDE) for R. It’s a program that runs on top of R and makes it easier to use R. As you get more advanced using R, it will be more helpful. You can download and install RStudio at the website https://www.rstudio.com/products/rstudio/download/ Once you install RStudio, you can open it instead of opening R. It should look like this: Basic RStudio Installation Here is what RStudio looks like, with an image of me typing this book at this moment: My current RStudio It is important to note that both R and RStudio will be on your computer, so you can either of them. RStudio needs R but R does not need RStudio. However, I strongly recommend opening RStudio, instead of opening basic R. 4.2 The basics of R If you open RStudio, you should see something like this. Let me give you a brief tour of some of the different features of RStudio. Basic RStudio Installation RStudio has two different panes. On the left side, you have what is called the console. This is where you can enter in your R commands, which we will do in a minute. On the right side, you have two other windows which are helpful. The top-right window contains something called your workspace. This is a list of all the functions, variables, and other things you have in R at one time. And at the bottom right, you will see a window with several tabs, such as files, plots, packages, and help. This contains additional information which may be useful. 4.2.1 Basic Math Functions As mentioned in the last section, R is a programming language. You do some input and get some output. The input are called commands and you run them by typing them in the console and pressing enter. At its simplest, R is like a big calculator. For instance, I could type the following: 38 + 74 ## [1] 112 As a note, any command I show you in R will be in the shaded box like the box above. Additionally, the output that you would see in the R Console will be listed below the command. In this case, the output says “[1] 112”. Some basic math functions in R are listed here: Function Symbol Addition + Subtraction - Multiplication * Division / (forward slash) Exponent ^ If I wanted to multiply 24 and 8, I would type this: 24*8 ## [1] 192 I get 192. You might be wondering what the 1 in brackets means. It means that R is outputting one number and that its value is 192. This is helpful if R outputs a list of numbers. The number in brackets tells me what number in the list is the value immediately to its right. For instance, if it said [33], then I would know that the value immediately to the right of the [33] is the thirty-third element. With that said, let’s try to type some commands and see what the output is. Try typing the following commands: (note that I didn’t output the answers below). 45 +39 34562 * 24728 854/2 11^3 4.2.2 Objects If this was all that R could do, it would be no better than a calculator. The power in R is that we can do a lot more, and the first element of this power is the concept of objects. An object (also known as a variable in other programming languages) is something created in R that stores something. An object is like a box. The box stores something and the thing stored in the box can change, depending on the context. There are many types of objects depending on what is stored in the object. The simplest object is a variable, which is an object that stores one value. A variable is something that stands for one value, usually a number but it can also be a string of letters. The simplest variable might be the one you remember from algebra, the letter x. If we want to make x equal 1, we would type: x = 1 That’s because there is nothing to be output. R simply does the command without a word. But if you look to the top right of your RStudio screen, you might see a change to your workspace. It will say under Values, x and 1. Values When assigning variables, we always put the variable on the left, then equals, and then the value to assign on the right. If this confuses you, you can use another way to assign values, which uses an arrow, made up with a less-than sign and a dash, which looks like &lt;-. So you could type: x &lt;- 1 Many R people like this way of assigning variables, but it is a way of assigning variables that is particular to R whereas using equals is something done in many programming language. The choice is up to you, but since I used other programming languages before learning R, I use the equals to assign variables. Variables can have any name. You could use a single letter, or longer letters. You can also use numbers and some symbols, as long as the variable starts with a letter. However, I recommend sticking with letters. Here are a few examples: x = 3 y = 11 number = 24 name = &quot;Sarah&quot; Notice in the last example I used letters instead of numbers. If I put letters in quotes, I can assign letters or any other characters to a variable, called a string. I can also do math with variables. Look at the following: x = 4 z = 12 z - x ## [1] 8 Notice now I get an output when I type z - x. This is because R is calculating the value for z minus the value for x and then outputting it in the screen. I can also use this to use one variable to set the value for another variable. x = 11 y = x In this case, I set the value y to equal what x was, which is 11. Notice that if I mix up the order of x and y, I get an error: x = 14 x = k ## Error in eval(expr, envir, enclos): object &#39;k&#39; not found In this case, I get an error, because R is trying to set the variable x to be what the variable k is, and since there’s no variable k, there’s no value for x. Objects can hold more than one number as well. If you type the following, you will get an error: x = 1, 3, 5, 7, 9 That’s because R has no idea what is going on. R assumes you are assigning the value 1 to x, and then there are a bunch of other letters randomly added to the command. Whenever R gets input it doesn’t understand it give you an error. So, to enter multiple numbers, you can use something called a function. In R, all functions are a command followed by parenthesis. What is contained in the parenthesis is the input to the function. Each function takes the inputs, does something, and then sends an output, either to the screen (if nothing is given with equals), or to an object. The simplest command for R is one called concatenate, which means to put more than one item into a list. Since concatenate is a long word and I generally can’t spell it right, R just shortened it to c(). So type the following: x = c(1, 3, 5, 7, 9) This is simply a command that tells R that you’re making a vector, or a variable with more than one number. Try typing the following examples: ages = c( 19, 21, 24, 27) IQ = c(97, 132, 88, 101, 94, 107) presidents = c(&#39;George&#39;,&#39;John&#39;,&#39;Thomas&#39;) Vectors are powerful because you can use them to enter variables and do statistics on the variables. An important thing to note is that like most computer languages, R is case sensitive. So if you make a variable “Ages”, it is different from “ages”, or “AGES”. If you are like me, you will make many errors by forgetting how you capitalized variables. My solution is to never capitalize anything unless it’s a standard abbreviation which is always capitalized, like “IQ”. 4.2.3 Data frames There are many ways to enter data into R, but the one that I use most commonly is a dataframe. A dataframe is a table with rows and columns. Each column represents a variable and each row represents an observation. This is the same way as we mentioned in Chapter 2. A table may look like this: name sex test1 test2 test3 April F 85 79 80 Bret M 77 80 64 Cassie F 69 64 90 Deshauna F 98 93 81 Egor M 89 90 99 Each row corresponds a subject (in psychology, usually this is a person) and each column is a variable. By following each row, we can see all the values for a particular person. If we wanted to see Deshauna’s scores, we would follow the fourth row. This format of data is called tidy data and it makes it much easier to set up data. You can read more about this format in the book linked here. To make a data frame in R, I have to create several variables on their own using the c() command and then combine them using the data.frame() command. Here is how I would make the data frame in the table above. In this case, I am naming the data frame “grades”. name = c(&#39;April&#39;,&#39;Bret&#39;,&#39;Cassie&#39;,&#39;Deshauna&#39;,&#39;Egor&#39;) sex = c(&quot;F&quot;,&quot;M&quot;,&quot;F&quot;,&quot;F&quot;,&quot;M&quot;) test1 = c(85,77,69,98,89) test2=c(79,80,64,93,90) test3=c(80,64,90,81,99) grades = data.frame(name,sex,test1,test2,test3) If you type the name of the dataframe into R, you will see the dataframe listed, like this: grades ## name sex test1 test2 test3 ## 1 April F 85 79 80 ## 2 Bret M 77 80 64 ## 3 Cassie F 69 64 90 ## 4 Deshauna F 98 93 81 ## 5 Egor M 89 90 99 Now, it might be helpful to access various parts of the dataframe. If you want to see just a single variable or column, you can use the following notation: dataframe$variable To view the scores for test 1, we would type grades$test1 ## [1] 85 77 69 98 89 We can also create new columns by doing math to other columns. If we want to create a variable that has the total number of points in a class, we would do the following: grades$total = grades$test1 + grades$test2 + grades$test3 Now if we want to see our new variable, we can type: grades$total ## [1] 244 221 223 272 278 If you want to see a data frame, you can look at them by using the View() command. So you would type the following to view grades: View(grades) This has to be a capital V. In RStudio, this will pop up a window above your console with the data frame entered. This is how it looks in RStudio. Viewing a dataframe in RStudio Notice how RStudio has four windows. The top-right window shows what you are viewing. If you want to close it out, you can click the little x by the tab that says “grades”. With R, you can have many data frames open at once. If you see the top-right window, it should have a tab called “workspace”. The R workspace is the list of all the objects you have, including vectors and dataframes. You can also see all that is in your workspace by typing: ls() This is a weird command because there are two parentheses with nothing in them. Occasionally we have commands that have no inputs. We put the parentheses () at the end so that R knows this is a command If you want to back up your workspace, you can save your workspace by going to the Session -&gt; Save Workspace As. menu. You can also load previously saved workspaces by going to Session -&gt; Load Workspace. Loading a workspace doesn’t delete your previous workspace; it just adds to them, like a desk getting more cluttered. Finally, to remove items, you can use the rm() command, putting the name of the object in the parenthesis. So to remove the vector containing names, you could type: rm(name) This is a good way to keep your workspace clean, because the more you have in your workspace, the more memory it uses. If you’re working with big files, workspaces can get very large. 4.3 Inputting data into R Inputting data into R by making vectors and using the data.frame() command is awkward, especially for a lot of data. It’s much easier to input your data into a spreadsheet and then import the data from the spreadsheet into R. I will discuss two ways to input data: One is using .csv files using a spreadsheet program and the second is using the gsheet package which allows you to input data from a spreadsheet on Google sheets. 4.3.1 Inputting data from a .csv file Any spreadsheet program, such as Microsoft Excel or Google Docs, will allow you to export data using the csv file format. CSV stands for comma separated values, and it is a common way to store a table of data. To save data as a .csv file, you should select this as an option when saving a spreadsheet. For instance, you would select the following in Excel: Exporting a spreadsheet as a .csv file Before you save the spreadsheet, there are a few steps you can do to make sure the data are imported into R. Data frames have data listed so that each column is a different variable and each row is a unique observation. So data must be exactly rectangular. Each row must have a value in all the columns and each column must have a value in all the rows. The first row should contain column names. This isn’t necessary, but is a lot easier than doing it without. Column names should be simple names with no spaces or other non-letter characters. If you want to use multiple words, you can separate them with a period or underscore. Examples might be: “reaction_time” or date.of.birth. Each column should contain all the same type of data, other than the column names. If a column contains numeric data, do not mix any other types of characters. This includes text or other non-numeric symbols except decimal points (like numbers with commas). If a column has both numbers and text, R will assume that variable is all text strings and turn the numbers into the wrong variable. Good data entry This is an example of good data. Each column has a value in all the rows and there are all the same type of data. Bad data entry This is terrible data entry. The Peter row is not full and the word absent is written into the row for test 1. When this dataframe is read into R, it will not read correctly. R will assume that the column for “test1” are strings, rather than numbers. If you are missing data, you should use the letters NA (capital N and capital A). R uses this for data which are missing. Handling missing data is a pain and beyond the scope of this book, so I will try to avoid it as much as possible. Once you have written your table, you should save it as a .csv file. For instance, this is listed in Excel as “CSV (comma delimited)”. Once you save your data, you can input the data into RStudio by clicking Import Dataset in the environment window (or clicking File -&gt; Import Dataset -&gt; From CSV). This pops up the following window: Import Dataset Here you would select the .csv file you saved and then if you set up the data correctly, it should import into R. Once you have input your spreadsheet into R, you should make sure it looks right by typing View(x), replacing x with whatever named your dataframe. If it does not look right, you may have to adjust how you put your data into R. One final note: you might input your data and see a bunch of extra rows or extra columns added to your data. This is because Excel added empty rows or columns to the csv file that it should not have. To fix that, open your file in excel, select only the data you want, just the rectangle containing your rows and columns, and copy that to a new spreadsheet and immediately save it as a csv file. In this book, many of the examples will be saved as csv files. I like to use these because they are usable on any operating system and don’t require a non-free program like Excel or SPSS to use. You can use several free programs to make csv files, such as LibreOffice or an online program like Google Docs. 4.3.2 Importing data from Google Sheets Many of my students use Google Sheets, a part of Google Drive, to enter their data. We can take data straight from Google Sheets and import it into R by using a package called gsheet. As mentioned above, a package is an set of addon functions that someone wrote for R. To use these addons, we have to first install them. We would do that by using the install.packages() command. To install gsheet, you would type: install.packages(&#39;gsheet&#39;) If you do this for the first time, R might ask you to select a mirror, which is a server which holds all the packages for R. You should select one close to your physical location. Then you may see a bunch of characters appear, and hopefully no errors. You only have to install packages once. However, you need to load a package each time you use R. To do this, you would type: This loads the package into R. Now to import a spreadsheet into R, you will first need to enter the data into Google Sheets just like mentioned above, and then get the shareable link. This link is available by selecting the “Share” icon in Google Sheets, and then clicking the “Get Shareable link” in the dialog box that appears. Once you have the link, you would paste it in the following command. This command saves the spreadsheet at the link below to the data frame d. d = gsheet2tbl(&#39;https://docs.google.com/spreadsheets/d/1zew/edit?usp=sharing&#39;) 4.4 Descriptive statistics in R R has some functions which allow for us to get descriptive statistics for variables and dataframes. The simplest of these are the built-in functions mean(), median(), and sd() which give the mean, median, and standard deviation. These functions take one argument, which is a list of numbers. If we want to calculate the mean of a list of numbers, we would do it this way: x = c(12,47,92,38,74) mean(x) ## [1] 52.6 Instead of assigning numbers to a variable, we can nest the c() function inside of the mean() function. mean(c(12,47,92,38,74)) ## [1] 52.6 Both of these do the same thing. These functions can also be used with data frames to get the means of variables. In this case, to select the use the $ character, we can select only the column. We would type dataframe$column. mean(grades$test1) ## [1] 83.6 sd(grades$test2) ## [1] 11.38859 If we take the data frame we made in the last section, called grades, we can get some summary statistics by using the summary() function that is built into R. summary(grades) ## name sex test1 test2 test3 ## April :1 F:3 Min. :69.0 Min. :64.0 Min. :64.0 ## Bret :1 M:2 1st Qu.:77.0 1st Qu.:79.0 1st Qu.:80.0 ## Cassie :1 Median :85.0 Median :80.0 Median :81.0 ## Deshauna:1 Mean :83.6 Mean :81.2 Mean :82.8 ## Egor :1 3rd Qu.:89.0 3rd Qu.:90.0 3rd Qu.:90.0 ## Max. :98.0 Max. :93.0 Max. :99.0 ## total ## Min. :221.0 ## 1st Qu.:223.0 ## Median :244.0 ## Mean :247.6 ## 3rd Qu.:272.0 ## Max. :278.0 This gives us the mean of every variable, along with the minimum, first quartile (25th percentile), median, third quartile (75th percentile), and maximum. This is helpful but does not include the standard deviation, which is a commonly reported statistic. We can use another function, the describe() function, to look at many descriptive statistics for all the data at once. The describe function is in the psych package, so we have to install that first. (See the section on importing data for more about installing packages). We only have to install the psych package once. install.packages(&#39;psych&#39;) Then we would type the following: library(psych) describe(grades) ## vars n mean sd median trimmed mad min max range skew kurtosis ## name* 1 5 3.0 1.58 3 3.0 1.48 1 5 4 0.00 -1.91 ## sex* 2 5 1.4 0.55 1 1.4 0.00 1 2 1 0.29 -2.25 ## test1 3 5 83.6 11.13 85 83.6 11.86 69 98 29 -0.04 -1.81 ## test2 4 5 81.2 11.39 80 81.2 14.83 64 93 29 -0.38 -1.66 ## test3 5 5 82.8 13.03 81 82.8 13.34 64 99 35 -0.19 -1.63 ## total 6 5 247.6 26.67 244 247.6 34.10 221 278 57 0.09 -2.18 ## se ## name* 0.71 ## sex* 0.24 ## test1 4.98 ## test2 5.09 ## test3 5.83 ## total 11.93 4.5 Summary This chapter only gives the basics of how to use R. However, we will use R in the rest of this book in order to illustrate the concepts that we will cover. In addition, I will also describe how to do the various statistical tests and procedures in R, along with describing basics about the theory behind the test. The point of this book is to help you know how to analyze data by knowing which techniques to apply, when to apply them, why you are applying them, and what the results tell us. R helps us tremendously in doing this. After this chapter, you should be able to do the following: Be able to understand how to install R and open R and RStudio. After this, you should be able to know the main parts of the RStudio window and know how to use R like a calculator. Understand what variables and commands are and how to use them. Know how to make a variable (single number), vector (list of numbers) using the c() command, and how to make a dataframe out of a series of vectors. Be able to import data into R using a .csv file and optionally importing data from Google Sheets using the gsheets package. Be able to get basic summary statistics from R using the mean(), median() and sd() command and get summary statistics from a dataframe using the summary() command and the describe() command from the psych package. "],
["probability-and-samples.html", "Chapter 5 Probability and Samples 5.1 What is probability? 5.2 Calculating probability of events 5.3 Probability and sampling 5.4 Summary", " Chapter 5 Probability and Samples In this chapter, we will cover the following topics: What is probability and what does it represent How probability works, using basic examples of probability problems How we use probability to interface with distributions What sampling is and how probability allows us to sample 5.1 What is probability? There’s a thirty percent chance for rain. A certain horse has 5-1 odds of winning a horse race. You have a fifty/fifty shot of getting heads when you flip a coin. The odds of winning the lottery are one in a million. We hear probability estimates all the time in our life. But what does probability really mean? When there’s a thirty percent chance of rain, what is that saying? Probability is a branch of math which is involved with studying uncertainty in events, usually future events. Uncertainty is when we do not know what is going to happen, but we have some information about what the truth is going to be. I’ll start with the complicated definition of probability and then break it down. Probability is defined as a subjective estimate of likelihood that something will happen, defined as a decimal from 0 to 1. A probability of 0 indicates an event which will never happen, or is impossible. A probability of 1 indicates an event which is certain. Since few things in life are that certain, almost all estimates of probability are somewhere between zero and one. There are two ways of thinking about subjective estimates of likelihood. The first is called relative long run frequency of occurrence. Based on this, probability represents the percent of times something would happen, if I did the exact same scenario millions of times (the long run). For instance, the probability you get heads if you flip a coin is 50%, or 1 in 2. That means you are just as likely to get heads as you are to get tails. If you flipped a coin twice, you won’t always get 1 heads and 1 tails; actually this happens only 50% of the time. However, if you flip a coin ten million times, you would get heads almost exactly 50% of the time. You won’t get exactly 5 million heads, but you would be very close to 50%. In fact, you are almost certain to get heads 49.95% and 50.05% of the time. The more often you do an event, the closer the frequency of occurrences (like number of heads) gets to the probability. In many cases, we can use math to figure out what the relative frequency of occurrence would be in a certain situation. We’ll talk about this in the rest of the chapter. However, this is only if we know all the possible information about a situation. For a coin or for lotto tickets or for rolling dice, I know all the possible outcomes and I know (or assume) that the probability of each outcome is equal, so I can calculate the probability. However, in many cases, we don’t know all the information. For instance, bookmakers set the odds that a team will win a game or the odds a certain horse will win a horse race. Their view of the odds is based on what they know, and their best guess about how good a team is. However, they cannot know all the information. So what probability in this case represents is a subjective belief, or a best guess of the likelihood that something is the case, given what we know. This second view of probability, or the idea of subjective estimate of belief, is most relevant to behavioral science. In science, we don’t know the full story. We are trying to find information from data, and the more data we get, the more information we have, but we don’t have all the information, so we use the information we have to give a probability about what we don’t know. This class is designed to teach how we use the information we have to give estimates of what we don’t know. The words “subjective estimate” were not in many examples of definitions of probability that I found when I was looking up what probability meant. So I figure I ought to explain what that is. Probability is based on one’s perspective and different observers have different perspectives. Therefore, there is no “right” estimate of probability. Instead it depends on the observer. This can lead to some interesting paradoxes. Imagine a pregnant woman getting an ultrasound to find out whether her baby is a girl or boy. There is about a 50% chance that the baby is a girl or boy (actually the likelihood of having a girl or boy is 51% boy, 49% girl). Once the ultrasound technician looks at the ultrasound, they see the baby is a girl. To the technician, there is a 99% chance the baby is a girl. However, the mother does not want to know the sex, so she is not told the sex. Did her probability estimate change? No, because she doesn’t know the information. To the mother, the odds are still 50/50 for a boy or girl, even though to the ultrasound tech, the odds are 99% girl. Here’s another example. The odds makers say there’s a 60% chance that a certain team is going to win the World Series. However, you know that the team’s best pitcher is sick and will miss the series. So you know that the estimate is too high, because you have additional information. Since probability is a subjective estimate of how likely something is, more information should change our estimates. In science, we never have all the information about how likely something is, and so probability is a degree of belief about how likely something is and is always changing. One framework for examining how probabilities change given information is called Bayesian probability or Bayesian statistics. This is a way of using mathematics to determine how information changes probability. The way that Bayesian probability works is beyond the scope of this book, but the way that Bayesian perspectives think about probability can be applied to understanding how we think about information. Imagine that you are having a picnic this afternoon and you want to know whether it will rain. You figure it rains 25% of the time so that’s your first guess. This is a concept called prior probability, or your initial estimate of probability. You are about to leave and you see big storm clouds overhead. This is more information and so you should change your estimate. In this case, you would probably think that rain is more likely, so you would increase your estimate of the probability of rain, maybe to 50%. This new estimate is called posterior probability, or the probability after more information is added. Then the posterior probability can then be a new prior probability as you add more information. After you see the clouds, you look at the weather forecast and you see that there is little chance for rain. So with even more information, you may change your estimate again. Bayesian statistics gives us a way to use statistics as information to change the likelihood that something is true. When we are doing research, we generally have an idea that we think may be true and a prior probability in our mind. For instance, if I think that coffee may protect against Alzheimer’s disease, I can take all the articles I read about coffee and Alzheimer’s disease and come up with a likelihood that this is true. If there’s a lot of good evidence, I may give a high initial prior probability. If there is not much evidence, I may give a low prior probability Bayes’s Theorem is a concept that mathematically says how we can change prior probability to posterior probability when given new information. However, for this section it’s important to just know how probability relates to new information. 5.2 Calculating probability of events Objective estimates of probability can be made by following one of a series of rules. In mathematical treatments of probability, these are called axioms. In this section, we will discuss some of these rules and the concept of independence. As mentioned in the last section, probability is defined as the likelihood something will happen, estimated between 0 and 1. So, the notation for this is \\(P(A)\\), which is the probability that A will happen. Because the probabilities add up to 1, we can also know that the probability that A happens or that A does not happen (denoted as \\(P(A&#39;)\\)), is 1. Now, if you have several events, and each event has an equal chance of occurring and only one event can occur, the probability for any one event is \\(\\frac{1} {N_{\\text{total}}}\\), which is 1 divided by the total number of events. If you have a fair coin with an equal likelihood of each event, you have two possible events, heads and tails. In this case, the chance for any one event is \\(\\frac{1}{ 2}\\). The chance for heads is \\(\\frac{1}{2}\\) and the chance for tails is \\(\\frac{1}{2}\\). If you have a six-sided die, you have six possibilities for outcomes, rolling a one, two, three, four, five, or six. In this case, the probability that you roll a 1 is \\(\\frac{1}{6}\\). since there are six possible outcomes. If you are playing cards with a 52 card deck, the chance you draw any one card is \\(\\frac{1}{52}\\). because there are 52 possibilities. This may seem simple, but it is the first building block to the rest of probability. The second concept, called the addition law of probability, is that the probability of one or another event is the sum of their probabilities minus the chance that both events would happen. Since we assume that only one event can happen, the chance both events can happen is zero. This is written using the following math-speak: \\[P(A \\text{ or } B) = P(A) + P(B) – P(A \\text{ and } B).\\] This can help with calculating more complex questions. The chance you roll either a 1 or a 2 in a six-sided die is: \\[P(\\text{rolling 1}) + P(\\text{rolling 2}) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}\\]. The way I remember this is by thinking the chance of something OR something else is adding. So OR leads to adding. By putting these two concepts together, I can extend probability to calculate multiple outcomes at once. If there are many possible outcomes and I want to know the chance that a subset of those outcomes happen, the chance of that is: \\[P(\\text{What I want)} = \\frac{\\text{Number of outcomes I want}}{\\text{Total number of outcomes}}\\] For instance, if I’m playing a card game and I want to draw a heart and there are 45 cards left in the deck and 11 of them are hearts, the chance of drawing a heart is 11/45. Likewise, if I’m playing bingo, and I have three numbers that can give me a win and there are 20 possible numbers, the chance I will win is 3/20 when the next number is drawn. The final thing to know is that these concepts can be extended to situations where the likelihood of each event is not the same. For instance, imagine if I am playing a game where I roll two six-sided dice. The chance I roll a 2 is different than the chance I roll a 7, for reasons we will discuss later. The chance of rolling a 2 is 1/36 whereas the chance of rolling a 7 is 6/36. So the chance of rolling a 2 or a 6 is just adding those probabilities up, 1/36 + 6/36, or 7/36. Probability can be extended to multiple events. For instance, I talked about the probability of getting a number that is the sum of two six-sided dice in the last section. Each die represents its own possible event with its own outcome. Also, I might want to know the outcome that I have two girls as children. The chance of having a boy or girl is roughly 50%, but what about two children, which are two separate outcomes. When combining probabilities of two or more events, we have to know whether those events are independent. Statistical independence means that our knowledge about the outcome of one event or variable does not affect our beliefs about another variable. One’s height and one’s favorite color are probably statistically independent. If I know your height, this does not tell me anything about what your favorite color would be. On the other hand, statistical independence is defined if knowledge of one variable or outcome affects the probability of the other outcome or probability. For instance, knowledge about how long someone’s legs are necessarily affects knowledge of how tall someone is because leg length is part of one’s height. Leg length does not guarantee height, but it does predict height since it is part of what makes a person tall or short. In probability, if two events are independent, knowing about one event does not affect our subjective belief of the probability of another event. If I roll two dice, one die being red and the other blue, the outcome of the red die does not have any influence on the outcome of the blue die. Knowing that I rolled a 1 on the red die will not change any estimate I have about the outcome of the blue die. However, the number that is the total of both dice is dependent on the red die. Knowing that I rolled a 1 with the red die affects my view of the probability that the sum of both dice is a certain number. Another example might be playing a game where I roll two dice repeatedly. Each roll of two dice is independent of the next roll. If I roll a high number several times in a row, I might think I’m on a lucky streak and more likely to do better. This is not the case because the probability of each roll is not affected by the other probabilities. However, this is an example of the gambler’s fallacy. The gambler’s fallacy is a bias of thinking where people think that prior outcomes have influence on subsequent outcomes even though they are independent, and thus unrelated. A person may think they are on a lucky streak and that they will keep getting good rolls. Or they might think that they have had a lot of bad rolls in a row and are due for a good roll. This bias of thinking is the erroneous belief that one roll affects another roll. However, there are many cases where probability is dependent, or where knowing the outcome of one event changes the probability of the subsequent events. Imagine I have a deck of 52 playing cards and I draw one card, look at it, and then draw a second card. When I see the first card, my estimate of probability about the second card changes, because I have more information. If I draw a king of clubs for the first card, I know that the second card cannot be the king of clubs. The chance of any other card has changed. After drawing the king of clubs, there are 51 possibilities, not 52. So the chance of drawing a different card, like say the eight of hearts, is 1/51, not 1/52. Likewise, the chance for drawing a club is different. Since a card deck starts with 13 clubs, the original chance of drawing a club is 13/52, or 1/4. Now, after drawing a king of clubs, the chance is 12/51, since there are only 12 clubs remaining in the 51 card deck. The idea of statistical independence and dependence is a very important one because it helps us to determine the types of statistics we can do. Since dependence adds information, it helps us to be more accurate about our probability estimates. For instance, if you and a friend are playing this card game and you draw a king of clubs without showing your friend and your friend is supposed to draw the next card, you have a different view of the probability than your friend. Since your friend knows nothing about what you draw, from her perspective, there is a 1/52 chance to draw any card. The fact you drew a card and didn’t show her is irrelevant. The chance that two separate outcomes both happen from two separate trials is \\(P(A)*P(B)\\), or the product of the probability of each of those events. If the two events are independent, it is easy to find out the probability of each event, because you calculate it on your own. For instance, if you have a blue six-sided die and a red six-sided die and you roll them, the probability you roll two even numbers is the chance you roll an even number with the red die multiplied by the chance you roll an even number with the blue die. As we discussed above, the chance of rolling an even number is 1/2, so the probability is \\(1/2 * 1/2 = 1/4\\). Likewise, the chance a person will have two girls is \\(1/2 * 1/2 = 1/4\\), since the chance of having a girl is 1/2. I call this the “and” rule of probability. Assuming A and B are independent, the chance that A and B happen is A multiplied by B. Or: \\(P(A \\text{ and } B) = P(A) \\cdot P(B)\\) This is contrast to the “or” rule discussed earlier. The chance A or B happen, is the sum of A and B (minus the probability that A and B both happen). 5.3 Probability and sampling The final thing is how we can apply what we know about probability to answer questions about sampling. Sampling is when we take a subset of a population and the simplest form of sampling is to take one individual. Imagine I had a study where I asked people to rate their mood on a 5 point scale, with one being very negative and 5 being very positive. I might get results like this, in a frequency chart: Rating Frequency 1 5 2 10 3 20 4 20 5 15 If I select one person at random, what is the chance that they have a rating of 4? The way to answer that is to use the simple probability formula above. I take the number of outcomes I want, which is how many people had a rating of 4. Then I divide by the total number of outcomes. This gives me: \\[\\frac{20}{70} = .29 \\] There’s a 29% chance that if I select one observation at random, it would be a rating of 4. If I wanted to find the chance that I select someone who gave a rating of over 3, I would use the AND rule above. I would just add the chance I select someone with a rating of 4 with the chance I select someone with a rating of 5. \\[\\frac{20}{70} + \\frac{15}{70}= .5 \\] There’s a 50% chance I select someone with a rating of above 3. By combining the probability rules, I can make these calculations. In chapter 3, we talked about something called a probability density plot. This was a plot which plotted the percent of observations with a certain value. The reason we call this a probability density plot is because the percent of observations with a certain value is equal to the probability of selecting a certain value, if we select a value at random. This can be helpful if we have very large samples. Imagine I gave 1000 people an IQ test and I found the following histogram, plotting the number of people in ranges of 10 (45 to 54, 55 to 64, etc.) This plot gives me the number of people in a certain range. For instance, there are 245 people who have an IQ between 95 and 104, which is reflected by the big center bar. If I want to know the probability of selecting an individual in any range, all I have to do is create a probability density plot. The only difference is that the y-axis now has percent rather than a count. Each bar represents the percent of observations that are in each range. With this, I can answer very quickly the probability of selecting a person at random with an IQ between 105 and 115. It would be about 20 percent. There are also two different plots using probabilities that can be useful. Instead of plotting the percent of values that are in a certain range, it may be useful to plot the percent of values that are below a certain value. For instance, what percent of people’s IQs are below 90? This is the exact same as a percentile, and a plot of this is called a cumulative density plot or CDP. Though we can use bar graphs to do cumulative density plots, these are traditionally done using line graphs, like a frequency polygon. In this case, I would have the following: All CDFs have this shape where they start with zero on the left and end with 1 on the right. The way we read a CDF is that for each X value, the corresponding Y value reflects the percent of observations that are below a certain number. So if I want to know what percent of people have an IQ less than 125, I would see which Y value corresponds with an x value of 125, like on the plot below, where if I trace the red line, I can see that for an x value of 125, the corresponding y value is .95, reflecting that 95% of people have an IQ of less than 125. As mentioned above, tracing the y value gives me the percentile for each x value. To find out which X value corresponds with the 60th percentile, I just trace from the y axis like so: 5.4 Summary This chapter covered a lot! It gave a brief overview of probability and then applied it to sampling. In this chapter, we learned about the following: How is probability expressed and what does it mean? You should know that probability is a number that ranges from 0 to 1 and that it represents a subjective estimate of likelihood How is probability calculated? You should know what prior and posterior probabilities represent and how probability estimates are always changing based on new evidence. You should also know how we calculate probability using the AND and OR rule How does probability apply to sampling: You should know how we can use probability to calculate the likelihood of selecting an observation and what a probability density plot is. You should also know what a cumulative density plot is and how it corresponds to percentile ranks "],
["the-normal-distribution-and-sampling.html", "Chapter 6 The Normal Distribution and Sampling 6.1 The Central Limit Theorem 6.2 Properties of the Normal Distribution 6.3 Sampling and Sample Means 6.4 Summary", " Chapter 6 The Normal Distribution and Sampling In this chapter, you will learn: What the central limit theorem is and how it explains why the normal distribution is so common in nature The properties of the normal distribution, including its shape What Z-scoring (or scaling) data is and why it is important How to use z-scores to calculate percentiles based on the normal distribution What sample distributions are, what the law of large numbers means, what standard error is, and how to calculate standard error 6.1 The Central Limit Theorem If we measure a population and plot the distribution of the values we measured, what we find is many distributions in nature have the same shape. For instance, if you think about a person’s height or IQ, or how introverted a person is, or how anxious they are, the distribution will have the same shape. There will be a few values that are low, most values are in the middle, close to average, and a few are well above average. Why do they all have the same shape? There are two critical theorems in statistics, which we will discuss in this module. They are the foundations for why we can even do statistics at all, which is why they are so profound. The first principle we’ll discuss is the central limit theorem. The central limit theorem indicates that if you take the sum of many distributions, then that resulting distribution will have the same shape, regardless of the shapes of the underlying distributions that are added together. No matter what the shape of the original distributions, they will fit this new shape, called the normal distribution, or colloquially, a bell curve. So what does this mean? Take a person’s height. Whatever a person’s height will be is the product of many, many different processes. There are hundreds of genes which can make a person tall or short. There are hormonal factors, there are dietary factors (did you eat your vegetables?), and so forth. Each of those factors is its own distribution with an individual having different numbers for each of these factors. Whatever your height ends up being is the sum of all these variables put together. The central limit theorem shows up in all sorts of places. For instance, if you flip a coin, you have two possibilities, each with a 50% chance of happening. This would lead to a uniform distribution, in which all the possibilities have the same chance of happening. However, if you assign heads as 1 and tails as 0 and flip ten coins and add them up, each time leading to its own distribution, your new distribution will look normal. That’s what I did in the table below. I flipped ten coins, ten times, assigning 1 for heads and 0 for tails (since I’m lazy, I had a computer do it for me). Then I took the sum of the ten coins in each trial. Even though the outcomes were either 0 or 1, almost every time the mean of the 10 coin flips was in the middle. Most of the sums or means were around the middle value with a couple which were farther away. If you graph a histogram of the sum, it has a shape where most values are in the middle with only a few values on each side. Trial 1 2 3 4 5 6 7 8 9 10 Sum Mean 1 1 1 1 1 1 0 0 1 1 1 8 0.8 2 1 1 1 0 0 0 1 1 0 1 6 0.6 3 0 1 0 1 0 0 0 0 0 1 3 0.3 4 1 1 1 1 0 1 0 1 0 0 6 0.6 5 0 0 0 0 1 1 1 0 1 1 5 0.5 6 0 1 0 1 1 0 1 0 1 0 5 0.5 7 0 0 1 0 1 1 1 1 0 0 5 0.5 8 0 1 0 0 0 1 1 0 0 1 4 0.4 9 0 1 1 0 1 0 1 1 0 0 5 0.5 10 1 0 0 0 0 1 0 0 0 1 3 0.3 There are a few points to add about the central limit theorem. What it means is that as you take the sum of many distributions, the result will approach a normal distribution. It won’t look exactly like a normal distribution, unless there are many distributions added together with many different observations. This is why if you graph the histogram of the example above, it won’t look precisely like a normal distribution. The power of the central limit theorem is in how general it is. We don’t have to know the underlying variables which make up a distribution. We also do not have to know how many variables there are. In the example of height or IQ, we don’t know all the genes and environmental factors which influence these variables. However we can know they will fit a normal distribution because we know there are a lot of variables. There are some cases where normal distributions may not occur. In some cases, a normal distribution will be skewed. For instance, we discussed in Module 3 about how grades tend to be negatively skewed, with most people having high grades and a few people having very low grades. This is caused by a ceiling effect. A ceiling effect is when a distribution is constrained from having very high values. Essentially, there is a maximum grade you can score. You can’t do better than perfect on any assignment. The reverse of this is called a floor effect. A floor effect is the idea that there is a minimum value that you can’t get below. We talked about income as having a positive skew, with most people having relatively low incomes and a few people having very high incomes. This is because it is impossible to make less than zero dollars as income. However, there is no maximum income. Distributions that have ceiling and floor effects are often normal distributions but the ceiling or floor is preventing the normal distribution from happening. So if the ceiling or floor were removed, the distribution would approach a normal distribution. If for instance my assignments in a class did not have a maximum grade of an 100, I might get a more normal distribution in a class. But this is impossible in many cases, such as income, so those distributions always end up skewed. In the next sections, we’ll talk about this normal distribution and precisely why it is so powerful. 6.2 Properties of the Normal Distribution The beauty of the central limit theorem is that, with a few exceptions, the sum of distributions will always approach the same distribution, which we call the normal distribution. So what that means is that if we know about the normal distribution, we can know a lot about a population’s, even without measuring all the population. The normal distribution is a very interesting distribution. It is symmetrical, with the mean, median, and mode all being the same value. It will always have the same symmetrical shape. Because it has the same shape, we can figure out the entire distribution if we only have two numbers, the mean and the standard deviation. Since we know the shape of the normal distribution, we can generate percentiles for the distribution, by just knowing the population’s mean and standard deviation. Since the normal distribution is always the same shape, and the area beneath a distribution reflects the percentage of observations which are in a certain range, we can know what percentile a specific value is as well as what percent of the distribution are between two values. The easiest way to do this is by using an idea called z-scoring or standardizing. Since all normal distributions are the same shape, we can convert one distribution to another distribution by just varying the mean and the standard deviation. In order to calculate information about a distribution, we often convert a specific distribution to one where the mean is zero and the standard deviation is 1. This distribution is called the standard normal distribution. Look at the example below. For instance, IQ is a normal distribution with a mean of 100 and a standard deviation of 15. Likewise, SAT subtest scores are a normal distribution with a mean of roughly 500 and a standard deviation of 100. If we wanted to compare IQ scores with SAT scores, we need to put them in the same scale. That way, we can compare the scores. In order to put scores on a z-score distribution or the standard normal distribution, we do the following formula: \\[z = \\frac{x - \\mu}{\\sigma}\\] For each score, take that value and subtract the mean from it. Then we divide that value by the distribution’s standard deviation. If we do this for each item on a distribution, this creates a new distribution with new values. The new distribution will have a mean of 0 and a standard deviation of 1. Every observation is still in the same relative place, but the new distribution is now on a standard scale. Original SAT Score Original IQ Score Z-score SAT Score Z-score IQ Score 620 105 1.2 0.33 520 95 0.2 -0.33 470 90 -0.3 -0.67 580 120 0.8 1.33 630 125 1.3 1.67 440 98 -0.6 -0.13 540 100 0.4 0 380 90 -1.2 -0.67 500 120 0 1.33 510 88 0.1 -0.8 490 100 -0.1 0 520 96 0.2 -0.27 460 90 -0.4 -0.67 The z-score essentially tells me how many standard deviations a score is from the mean. A z-score of 2 is two standard deviations above the mean. A z-score of -1.5 is 1.5 standard deviations below the mean. With z-scores it is a simple step to then find out one’s relative percentile rank. Since we have the z-score, we know where someone is on the distribution. Percentile is the percent of people below a certain score, so all we have to do is calculate the percent of people below a certain score. This is equal to the area under the standard normal distribution below a certain value. For instance, in the figure below, the z-score is -1.5. The area below -1.5 is shaded in red and reflects the percentage of observations below a z-score of -1.5. Now calculating this area under a curve involves finding an approximation of the integral of the normal distribution function, which are extremely hard to calculate. Traditionally, we would look up these calculations in a table. However, we can do this easily using R. We can do this easily in R using the pnorm() function. We just type pnorm(z), replacing z with our z-score. and that gives us the area below a certain value on the normal distribution. This is equal to the z-score. For example, to get the probability of a z-score of -1.5 or less, we would type: pnorm(-1.5) ## [1] 0.0668072 This indicates that a z-score of -1.5 is in the 6.7th percentile. 6.7% of observations are below this value. If we want to know the percent of observations which are above this value, we can just do the following: 1-pnorm(-1.5) ## [1] 0.9331928 Since the area under the normal distribution is equal to 1, 1-pnorm(z) gives us the percent of observations that are above a certain value. When people calculate percentile ranks, this is often the method they use. Most distributions, such as height, follow roughly normal distributions. So using this method, you can calculate the percentile rank of any value if you know the mean and standard deviation of the distribution, using the following steps: Calculate the score to a z-score using the distribution’s mean and SD Type pnorm(z) to find out the percent of the distribution below a certain value. Percentile rank comes up in a lot of settings. For instance, many standardized tests will report a student’s performance in terms of their percentile rank. Rather than knowing that a student scored a 142 on a standardized test, it is much more informative to know that they scored in the 74th percentile. Another instance is determining what is normal weight for a newborn baby. It’s important to know whether a newborn is a healthy weight. Since, there is no absolute standard for what constitutes a healthy weight, what doctors do is use studies where they collect the weight and height of lots of babies and determine what the average weight and height of newborns born at a certain week in pregnancy would be. Then they can use this to determine how big a certain newborn is. The average weight of a newborn girl is approximately 7.5 pounds, with a standard deviation of .6 pounds. If I assume that newborn baby weights are normally distributed, I can answer the question as to what percentile rank a certain newborn is. For instance, if a newborn girl is 6 pounds, 8 ounces (6.5 lbs), what percentile rank is she? Using what we learned before, first we would figure out the z-score associated with that weight. \\[z = \\frac{x - \\mu}{\\sigma} = \\frac{6.5 - 7.5}{.6} = -1.67\\] Now to find the area under the normal curve for a z-score of -1.67 or below, we would type the following into R: pnorm(-1.67) ## [1] 0.04745968 So we get .048, or the 4.8th percentile. This means that 4.8% of newborn girls have a weight lower than this newborn girl. The things we give us a way to solve another problem. If we take a distribution and select one observation by chance, what is the chance that this observation is above a certain value, or below a certain value, or in a certain range. The mean IQ is 100, with a standard deviation of 15. If I select a person from random, what is the chance that this person has an IQ above 115? We can use what we learned in the last module to answer this question. Remember, when every outcome has an equal chance of being selected, probability is the number of outcomes we want divided by the total number of outcomes. Another way of saying this is by saying that probability is the percent of the outcomes we want, if all outcomes have an equal chance of being selected. In the example above, this would mean that the probability of selecting a person with an IQ above 115 is equal to the percent of people who have an IQ above 115. From what we learned above, we can figure this out. To find out the percent of the distribution with an IQ above 115, what we have to do is to take the z-score of an IQ of 115. This z-score equals: \\[\\frac{115-100}{15} = 1\\] Now we need to find the percent of the normal distribution with a z-score above 1. We can use the pnorm command to do this. Remember that the total area of the distribution is 1, and that pnorm(z) gives the area of the distribution below a certain value. If we know that, we can figure out that the area above a specific z-score is 1-pnorm(z). In this case, we would type in 1-pnorm(1) into R, and that would give us the answer, .159. So 15.9% of the population has an IQ above 115, which means that we have a 15.9% chance of selecting an individual with an IQ above 115. We can use this logic to answer other questions as well. For instance, what’s the chance we select a person with an IQ between 95 and 105? This problem might seem hard at first, but we can use a couple of tricks to solve it. To solve this, we just need to figure out the area under the normal distribution of an IQ between 95 and 105. Remember again that the total area under the total distribution is 1. So what we can do is find out the percent of the distribution that has an IQ below 95 and the percent of the distribution with an IQ above 105. Then, the rest of the distribution is the percent with an IQ between 95 and 105. Solving the IQ problem I worked out this problem above. If you look at the figure, I have the normal distribution broken into three parts. The green area is the percent with an IQ below 95. The red area is the percent with an IQ between 95 and 105. The blue area is the percent with an IQ above 105. First I find out the area of the green part, which I get by getting the z-score of an IQ of 95, which equals -.33. Typing pnorm(-.33) leads to 37% of the distribution. Then I find out the area of the red part. I find the z-score of 105, which is .33. Then I have to do 1-pnorm(.33) because I want to find the area with a z-score above .33. This leads also to 37% of the distribution, which is not surprising since the normal distribution is symmetrical. Now what’s left is the red part of the distribution, or 26%. If 26% of the distribution has an IQ between 95 and 105, then I have a 26% chance of selecting a person with an IQ between 95 and 105, given that I select a person randomly. 6.3 Sampling and Sample Means This is where we can start to talk about the idea of sampling and sampling distributions. A sample is selecting one or more observations from a population. In the section above, we learned how to calculate the probability of selecting a single sample with a certain value. However, most of our samples are samples of more than a single observation. Here’s an example. Above, we discussed the chance of selecting a single person with an IQ above 115. We might want to answer a different question: what is the chance of selecting four people whose mean IQ is 115 or above? This is a different question. First, let’s think about this conceptually. Which is more likely: I select four people who have a mean IQ of 115 or greater between all four of them or the chance I select one person with an IQ of 115 or greater? It’s much more likely to select one person with an IQ of 115 or greater than to select four people with a mean IQ of 115 or greater. No, because instead of just selecting one person, I have to select four people. The probability I select one person at random with an IQ greater than 115 is 15.9%; But to select four people with a mean IQ of 115 or greater, I have to select people with very high IQs more than once. If I randomly selected one person with an IQ below 115, the other three people need to have a mean IQ even higher than 115 so that the overall mean is still 115 or greater. To calculate the exact probability of selecting 4 people with a mean IQ of 115 or greater, we are going to consider a new distribution, called a distribution of sample means. Every distribution we’ve seen so far involves the distribution of single scores. However, what happens when instead of taking a distribution of single numbers, we take a distribution that is made up of the mean of repeated samples. So, I take a sample, take the mean of that sample, and then do that again and again. This makes a new distribution. The distribution of sample means is a theoretical distribution that would occur if we took an infinite number of samples of a certain size from a population, took the mean of each sample, and plotted the frequency of getting each value for the mean. We can’t take an infinite number of samples, but we can use math and probability to know what would happen if we did take an infinite number of samples and calculated the sample mean. To go back to my example above, instead of taking the sample of a single person’s IQ, I am taking the sample of the means of groups of four individuals and looking at their mean IQ. When I do this, several different things happen. Each sample mean is different. If I take four people at random, I will get a different mean IQ every time. Sometimes I’ll select four people and each of their IQ will be high, and the mean will be high. Sometimes, I’ll select four people and each of their IQs will be low, and so the mean will be low. But most of the time, I’ll select some people with high IQs, some with low, and some in the middle, which leads to a mean IQ in the middle. This new sample distribution will be normally distributed. This is the central limit theorem in action. Remember, the mean of several distributions will be normally distributed, regardless of the individual distributions. In this case, the sample distribution is the mean of the same distribution taken repeatedly. So the new sample mean distribution will be normally distributed. The new sample mean distribution will have the same mean as the mean of the original distribution. This is called the law of large numbers. As we take more samples, of any size, the mean of the samples will approach the population mean. This can be shown with a nice mathematical proof, but here’s a way to think about it. When selecting samples, some of the samples will be above the original mean and some will be below the original mean. Over time, the mean of the sample means will have the same mean as the original distribution. The sample mean distribution will have a new standard deviation that is different than the original distribution. This standard deviation will be smaller than the original distribution. This relates to the point discussed above. If I take a sample of four people, their average IQ is more likely to be closer to 100 than if I take a single person at random. For instance, to select four people with an IQ of above 115, I’d have to randomly select four pretty smart people. Or I might select three very smart people and one average person. Either way, this is much less likely than just selecting one pretty smart person. The new standard deviation of the sample distribution is smaller than the original distribution’s standard deviation, and the standard deviation gets smaller with larger samples. The new distribution has two important properties to know based on what is discussed above. The sample mean distribution is a normal distribution that has the same mean as the original distribution, but a different standard deviation. This new standard deviation is called the standard error and it is the calculated by taking the standard deviation of the old distribution and dividing it by the square root of the sample size. \\[SE = \\frac{\\sigma}{\\sqrt{n}}\\] Since standard error is calculated by dividing by the square root of the sample size, the sample mean distribution shape changes with different size samples, getting smaller as samples get larger. The sample mean distribution becomes narrower with larger sample sizes. Law of large numbers As the size of the sample gets bigger, the sample mean distribution gets smaller, and is normal, regardless of the shape of the original distribution. Back to IQ, if we take samples of 4 individuals, we will create a new distribution of sample means. This distribution of sample means will have a mean of 100, which is the same mean of the original distribution. However, the standard deviation of this distribution of sample means, the standard error, will be 7.5, or 15 divided by the square root of 4. If we take samples of 9 individuals, the standard error is now 5, or 15 divided by the square root of 9. So now we get to answer the problem we started with. What is the chance that we select a sample of 4 people with a mean IQ of 115? What we have to do is calculate a new z-score, based on the sample mean distribution. The sample mean distribution of a 4-person sample has a mean of 100 because the sample mean distribution mean equals the original distribution’s mean. The standard error is as follows: \\[SE = \\frac{15}{\\sqrt{4}} = 7.5\\]. Now to find the chance of selecting 4 people with a mean IQ of above 115, we use the same ideas we used for selecting 1 person, only with a different distribution, using the standard error instead of the standard deviation. In this case, we have to find the z-score of 115 from a distribution with a mean of 100 and a standard deviation of 7.5. In this case, the z-score would be 2. Then we would take the chance of selecting a z score above 2, which would be 1-pnorm(2) or .022. So there is a 2.2% chance if we select three people, their mean IQ will be above 115. This is much lower chance than the chance of selecting just one person, which is 15.9%. To summarize, here is how we calculate the chance of selecting a sample of of \\(n\\) individuals with a mean of \\(\\bar{x}\\) or greater (or less) from a distribution with a population mean of \\(\\mu\\) and a population standard deviation of \\(\\sigma\\). We find the z-score of the sample distribution. We can do this with the following formula: \\[Z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\] This combines the z-score formula and the standard error formulas indicated above. Then we find the likelihood of getting \\(\\bar{x}\\) or above by using the 1-pnorm(z) command in R or the likelihood of getting \\(\\bar{x}\\) or below 6.4 Summary This section covered a lot of important theory for statistics. The reason this is important is because we almost always deal with samples when we do research in behavioral sciences. It is rare to actually evaluate every member of a population, so we want to use samples to allow us to infer information about the population. The central limit theorem allows us to theorize that normal distributions will happen in a lot of places in nature. This allows us to know things about populations even though we never measure the entire population. Because we know the information about the normal distribution, then we can apply the techniques in this chapter to know about samples. After reading this chapter, you should be able to: Articulate what the central limit theorem is and why it is important Know properties about the normal distribution, including its shape Apply what you know about the normal distribution and z-scores to calculate the probability of drawing an observation with a certain value from a normal distribution Know what sample mean distributions are and how the law of large numbers applies to them Be able to determine standard error, or the standard deviation of the sample mean and calculate the z-score and the probability of selecting a sample with a certain value, given a certain sample size. "],
["making-decisions-with-statistics.html", "Chapter 7 Making Decisions with Statistics 7.1 Basics of Decision Theory 7.2 Null Hypothesis Testing 7.3 Effect sizes 7.4 Summary", " Chapter 7 Making Decisions with Statistics In this chapter we will discuss how we use statistics and numbers to make decisions. Before we can talk about the techniques we use for these tests, we need to discuss the idea of decision theory or a branch of science and math that discusses how we make decisions based on what we know. In this chapter, you will learn: How do we use statistics to make decisions? This will include discussing the basics of decision theory and how we determine accuracy. The idea of null hypothesis testing, which allows us to combine statistics and probability to make decisions about the validity of a hypothesis based on how likely the hypothesis is. The idea of effect sizes, or how strong an effect is. This is different than the probability that a hypothesis is true. 7.1 Basics of Decision Theory You’ve made a lot of choices in your life. Some of those were good (like taking this class) and some choices were bad. You may not have used statistics specifically when you are making these decisions, but the idea of statistics plays a role in how you decide what you will do. We do this in science too. When we want to decide which conclusion to make, we are making a decision based on the evidence. This occurs in almost any field of science, including psychology. Here is one example: When I am writing this chapter, fidget spinners have become all the rage, the popular toy of the moment. When you read this, it is very likely that fidget spinners will be a forgotten memory, so I should explain what they are. They are little disc-shaped toys that have ball bearings in the middle of them so that a person can hold the middle of the toy pinched between their thumb and forefinger, and spin the rest of the toy. One reason people like fidget spinners is because they claim the spinners can help them pay attention, especially if a person has ADHD. This is a claim. There may be good reasons to believe this claim is true. People who support this claim say that the low levels of background noise provided by fidget spinners helps people to focus. Also, many people like to do movements to help them pay attention. There’s even real psychological evidence that doodling helps some people remember material from a lecture Andrade (2010). However, there may be good reasons to believe this claim is false. As a professional curmudgeon, I find the claim that spinning little toys will make people perform better sounds incredibly absurd. The point of statistics is that we collect data to test this claim and make an accurate decision about whether fidget spinners help people pay attention. By using statistics, we can help overcome some of our natural biases which may make us more likely to either believe things without evidence or disbelieve things despite compelling evidence. In order to make decisions with data, we have to first consider what we mean by accuracy or determining how good we are at making the right decision. Accuracy may seem like a simple concept but it can be very confusing. At its simplest, accuracy is measuring the percent of times I make the right decision. For instance, I have 100 questions and I get 90 right, I have 90% accuracy. However, accuracy alone can give some counter-intuitive results. Imagine I have two metal detector machines which are designed to go off whenever someone carries metal into a building. One metal detector is working and 95% of the time it detects metal when someone is carrying metal and 95% of the time it doesn’t detect metal when a person is not carrying metal. This means, 5% of the time, it misses metal that a person has and 5% of the time, it falsely says a person has metal when they don’t have metal. The second metal detector is is unplugged but no one knows this. So it never detects metal in any case and always says there is no metal present. In this example, 1000 people walk through each metal detector and 5 of them are carrying knives. The first metal detector would catch all 5 people who are carrying knives, and of the 995 people who are not carrying knives, it would falsely say that 50 of them are carrying knives on average (5% of 995 is 50). So using the formula above, it has an accuracy of 95% or \\(\\frac{950}{1000}\\). The second metal detector lets all 5 people who are carrying knives get through. But because it’s unplugged, it correctly lets all 995 people who are not carrying knives through. Its accuracy is \\(\\frac{995}{1000}\\), or 99.5%. In this case, the metal detector which is unplugged has higher accuracy than the metal detector which is on and has 95% accuracy. How can that be? This is because accuracy has two components: the ability to detect something which is true or present and the ability to rule out something which is false or not present. Signal detection theory is a theory that was developed to evaluate accuracy. In the 1930s and 1940s, the invention of radar made it possible to detect whether planes were in the distance, even when they couldn’t be seen. However, radar wasn’t perfect. There is noise in radar and other things, such as birds or weather phenomena could mimic what looked like a plane. The operators of radar had to test whether their radar worked by evaluating the accuracy of radar in different situations. Signal detection theory breaks accuracy into two factors. The first factor is the absolute truth, whether something is present or absent whereas the second factor is our decision, whether we decide it is present or decide something is absent. Each of these two factors leads to four different situations, each depending on two factors. If we say something is present and it is actually present, this is a hit. If we say something is absent and it is actually absent, this is a correct rejection. However, if we say something is present and it is really absent, this is a false alarm, also called a false positive, whereas if we say something is absent but it is really present, it is a miss, or also called a false negative. We say object is present We say object is absent Object is present Hit False Negative (miss) Object is absent False Positive (false alarm) Correct Rejection Another example is the fable of the boy who cried wolf. In this fable, there is a shepherd boy who is watching sheep and is supposed to cry if he sees a wolf. He cries out that a wolf is coming, even though he does not see a wolf. This would be a false positive, saying something is true when it is really false. However, after several false positives, the boy cries when he actually sees a wolf, which would be a hit. Unfortunately, in the fable, the townspeople were so tired of the boy’s cries that they ignored this cry. In that case, the townspeople would be committing a false negative, since they concluded the wolf was absent when the wolf was really present. The reason this is important is because various tests or decision criteria vary in their ability to either detect something that is present or rule out something that is not present. This leads to two important calculations: false positive rate and false negative rate. The false positive rate is the proportion of all negative results that yield false positive. It is calculated as: \\[ \\text{False Positive Rate} = \\frac{\\text{Number of False Positives}}{\\text{Number of False Positives + Number of Correct Rejections}}\\] False negative rate is the reverse of this, the number of False Negatives when the results are positive (or true). It is calculated as: \\[ \\text{False Negative Rate} = \\frac{\\text{Number of False Negatives}}{\\text{Number of False Negatives + Number of Hits}}\\] False positive rate and false negative rates often reflect a tradeoff. This is especially the case in medical screenings, which are simple tests used to screen for the possibility of a disease. One example of this are mammograms, which are x-rays used to detect breast cancer. If a radiologist sees an abnormality on a mammogram, they have to decide whether to suggest additional testing, such as a biopsy which is a test with very high accuracy. If the abnormality is cancerous, additional testing is obviously a good thing. The radiologist may then decide to make sure that they suggest additional screening any time something is abnormal, because they do not want to have a false negative result and miss a cancer which is present. This would lead to a very low false negative rate since there would be very few false negatives when cancer is present (hits and false negatives). But there is a tradeoff. The lower the false negative rate, the higher the false positive rate. Recommending biopsies and additional screenings any time there is even a slight abnormality will cause more false positives and a higher false positive rate. This may not seem like a big deal because we might think that missing a cancer which is present is much worse than a false positive. However, biopsies have side effects and complications, they cost a lot of money, and false positives cause a lot of worry. If you have ever been told you have an abnormal test result, you know how worrisome this may be. At a certain point, the cost of false positives outweighs the risk of a false negative. In science, we have to make this tradeoff. If we only require a little bit of evidence to believe a hypothesis is true, we have a higher risk of false positives, and a lower risk of false negatives. This is called a liberal threshold However, if we require a lot of evidence to believe a hypothesis is true, we have a higher risk of false negatives and a lower risk of false positives. This is a conservative threshold. For instance, if I have a liberal threshold, I may be more inclined to believe that fidget spinners help people concentrate. I would only require a little bit of statistical evidence to make that decision. I would have a higher risk of a false positive, believing that fidget spinners work when they don’t work. On the other hand, I might have a conservative threshold and be less inclined to believe that fidget spinners help people concentrate. As a professional curmudgeon, this is my general perspective. In this case, I require a lot of evidence to change my belief. I have less of a chance of a false positive, but more of a chance of a false negative. This kind of a tradeoff is what we will talk about in the next section. 7.2 Null Hypothesis Testing Signal detection theory is a great analogue for how we make decisions in statistics. We always have uncertainty in our data but we want to know specific answers. Did our experiment work? Does watching violent TV cause kids to be more aggressive? Will taking a multivitamin every day improve health? Is studying in smaller chunks of time better than studying in one large chunk of time? Many of our theoretical questions come down to yes/no answers about truth. However, statistics is only based on probability. If I run a study and find out that children who watched violent TV are more violent than children who did not watch violent TV, it could be the case that there is no difference in violence and I just happened to sample particularly violent children in the violent TV watching group and particularly nonviolent children in the no violent TV group. Statistics is all about uncertainty, but at what point do we make decisions? Many statisticians in the mid 20th century tried to answer this problem by using the idea of null hypothesis testing. What these statisticians developed was a technique where we evaluated a pair of hypotheses and chose one based on the evidence. We’ll talk about this through an example. A friend of yours says that he has gone to the future and has come back with knowledge about the future, a book that says the winner of a series of horse races. He wants to make a lot of money betting on horse races, but he doesn’t have any money to get started (probably because going to the future costs a lot of money). He says that if you give him money, he will tell you who will win and you both can make a fortune betting on these races. Now, for good reason, you’re skeptical. But he says: come with me to the track and I’ll prove it to you. He looks in his book and he says that Chester will win the first race. And Chester wins the first race. Then he says Longfellow’s Pride will win the second race. And he’s right again. So now he says: do you trust me? What’s your answer? Now based on what you know about probability, he could just be getting lucky. There might be a 20% chance he got the first race right just by guessing. And there might be a 20% chance he got the second race right by chance. Combined, that’s a 4% chance to get both right (20% * 20%). But if he keeps getting answers right, then it becomes less likely he’s getting lucky and more likely he’s telling the truth. Null hypothesis testing sets up these questions with two hypotheses: the null hypothesis and the alternative hypothesis. These hypotheses are mutually exclusive, which means one has to be right and the other has to be wrong. Both can’t be true and neither can’t be true. The null hypothesis generally asserts there is no relationship between two variables, or there is no special effects, there is no difference between two groups, or there is nothing abnormal about a situation. It is our baseline in science because science is skeptical and as scientists, we try to believe that nothing is true until proven otherwise. The alternative hypothesis is the opposite: it asserts the relationship we want to test. Since it is mutually exclusive with the null hypothesis, it asserts that there is a relationship between the variables or that there is a correlation or that something is present. In our example above, the null hypothesis would be at its simplest: there is no relationship between the horses your friend picks and the horses who actually win. The alternative hypothesis would be: there is a relationship between the horses your friend picks and the ones who actually win. This is better than stating this a different way, such as saying a null hypothesis: your friend did not go to the future and an alternative being your friend did go to the future, because there are several other reasons your friend might be right about the horses, such as knowing secret information about which horses are better. Good null and alternative hypotheses are specifically testable with statistics. The first set of hypotheses I listed above can be tested with probability. We can calculate the probability the horses your friend picked would win by chance alone. If that probability is somewhat low, you might think it’s a coincidence. But if the probability is very low, then we would conclude it’s likely your friend did go to the future. Null and alternative hypotheses allow us to do statistical tests and make decisions based on the results of those tests. When we do statistical tests, we are finding out the probability that we would get the results we get if the null hypothesis is true. If that probability is high, we retain the null hypothesis because we do not have sufficient evidence to reject the null hypothesis. However, if that probability is low, we reject the null hypothesis because it becomes increasingly unlikely that the null hypothesis is true. When we reject the null hypothesis, we say that an effect is a significant effect. This requires us to set a threshold where we consider the probability low enough to reject the null hypothesis. This is called the alpha level, and is traditionally set at .05, or 5%. If it is less than a 5% chance we would get our results if the null hypothesis is true, then we reject the null hypothesis. The alpha level is very similar to the false positive rate we talked about earlier. Here is an example: your friend comes back to you and says he has a six-sided die that will usually land on 3. He says you can take this die and use it to win money betting on the results. He rolls the die eighteen times and 8 times it lands on 3. To use null hypothesis testing to evaluate this, we would do the following: We set up null and alternative hypotheses. In this case, the null hypothesis would be that the die is no more likely to land on 3 than predicted by chance alone (that is, 3 is just as likely as any other number). In this case, the chance the die would land on 3 is 1/6. The alternative hypothesis would be the opposite of this: the die is more likely to land on 3 than would be predicted by chance alone. Your friend runs the test and we find out the chance we would get the results we got or more extreme results if the null hypothesis is true. If there is a 1 in 6 chance of rolling a 3, there is a 5 in 1000 (.0053) chance that we would get 7 or more rolls of 3 in 18 rolls. When we do this, we don’t calculate the probability that we would just get 7 rolls because we are interested in outcomes that are more extreme than the outcome we’re interested in. So we conclude there is a 5 in 1000 chance that this outcome would occur if the null hypothesis is true. If we use the alpha level of .05, we would reject the null hypothesis since it is less than 5% likely we would get these results if the null hypothesis is true. There is one important distinction about hypothesis testing we must make. The probability we calculate is the chance we would get these results if the null hypothesis is correct. But, the opposite of the probability we calculate is not the chance the alternative hypothesis is correct. So in the example above, it is true that there is a 5 in 1000 chance that we would get those results if the null hypothesis is true, but it is not true that there is a 995 in 1000 chance that the alternative hypothesis is correct. For instance, let’s go back to the example with the time-traveling friend. We calculate there is a 3% chance the null hypothesis is true, that the friend’s picks are not associated with the horses which actually win. That doesn’t mean there is a 97% chance that he is telling the truth about time traveling. Since the chance your friend went back into time is so remotely low (like one in a billion or more), it’s much more likely that he’s just getting lucky. When we do null hypothesis testing, there are four possibilities, which reflect the same ideas we talked about in the last section. These are listed in the table below. If we reject the null hypothesis and the null is false, we made the right decision. However if we reject the null and the null is true, we have made a false alarm. In null hypothesis testing, this is called a Type I error. If we retain the null hypothesis and the null is true, we made the right decision. However, if we retain the null and the null is false, we have missed finding the truth. In null hypothesis testing, this is called a Type II error. Type I and Type II errors depend on the type of testing we make and represent a tradeoff in accuracy. We set the chance of making a Type I error by using our alpha level. If we set a 5 percent chance for our alpha level, we are setting a 5 percent chance of making a Type I error if the null hypothesis is true. However, if we are more concerned about making a Type I error and want to be extra careful, we might reduce the probability that we require to reject the null hypothesis. It is harder to calculate the probability of a Type II error because that depends on the magnitude of the effect. This addresses a concept called effect size, which we will talk about in the next section. 7.3 Effect sizes Null hypothesis testing is a good approach for making decisions but it has a lot of limitations. It has so many limitations that many people don’t use this approach any more. In fact, when I started to write this book, I was going to remove this approach and talk about other ways of doing statistical testing. However, it was only when doing this that I began to appreciate null hypothesis testing again. Null hypothesis testing allows us to make a dichotomous yes/no decision. But many effects are more than yes/no decisions. For instance, I want to see if 5 minutes of guided meditation every morning would reduce people’s stress levels. I assign people randomly into two groups, one group which does guided meditation every morning and another group which spends 5 minutes doing whatever they want. Then I examine their stress levels. In this case, the null hypothesis at its simplest would be that there is no difference between the two groups. The alternative hypothesis would be that there is a difference between the two groups. Imagine if you did a study and rejected the null hypothesis, concluding there is a difference between the two groups. You would say that you are confident that meditation helps to reduce stress. However, there’s an important factor missing here: how much does meditation reduce stress? This is a concept called effect size and is separate, but related to the decision we talked about above. An effect size is a measure of how big an effect is. It answers the question: if I do X, how strong of an effect will X have. In the above example, it is a measure of how strong of an effect does meditation have on reducing stress. Does meditation reduce stress a little bit, or does it reduce stress a lot? Effect sizes are related to our certainty of decisions, but they are not the same thing. In many cases it is easier to find bigger effects, but this isn’t true in all cases. So we might come up with situations like this: We can be 99.9% sure that meditation reduces stress, but does it matter if meditation only reduces stress a tiny bit? Imagine we have two diets to treat blood pressure and we want to see if they actually work. After testing these using an experiment, we are 99.9% sure that people on diet 1 reduce their blood pressure. But on average, the diet only makes their blood pressure go down 2 points. However, we are only 95% sure diet 2 reduces blood pressure, but we found out on average diet 2 made their blood pressure go down 10 points. Which diet is better? This is an open-ended question which is hard to answer but the point I want to make here is that how certain we are about the truth is not the same as how strong effects are. In medical literature, scientists often talk about clinically significant effects. This is a measure of the practical importance of an effect. A significant effect, like as mentioned in the last section, says we rejected the null hypothesis. However, rejecting the null hypothesis only tells us that we are sure something happened. It does not tell us that what happened was enough of an effect that it matters in the real world. Giving someone a drug or a treatment has a cost. Knowing whether the benefit of that treatment is worth the cost is more than just evaluating whether we know the treatment makes a benefit. This is what is measured by effect sizes. In future chapters, we’ll learn how to calculate effect sizes as an additional piece of data to evaluate along with hypothesis testing. 7.4 Summary In this chapter, we covered ideas involving decision theory, null hypothesis testing, and effect sizes. After reading this chapter, you should know the following: What is meant by hits, correct rejections, false positives, and false negatives. You should be able to calculate false positive rate and false negative rate and know what it means that there is a tradeoff between false positive rate and false negative rate What is null hypothesis testing and how do we set up null and alternative hypotheses. Know specifically we are testing the probability we collect our data given that the null hypothesis is true Know what it means to reject or retain a null hypothesis and how this relates to alpha and our p-value we calculate Know some of the weaknesses of null hypothesis testing and what effect sizes are References "],
["frequency-claims-1.html", "Chapter 8 Frequency claims 8.1 What are frequency claims? 8.2 Binomial tests 8.3 Chi-square tests 8.4 Summary", " Chapter 8 Frequency claims In this chapter, we will learn about frequency claims. This will cover: What are frequency claims and when do we use them to evaluate hypotheses? What is the binomial test and how can we use it to test the likelihood that a sample proportion is from a given population? What is a chi-square test and how do we use it to generate hypotheses about whether two sample proportions are from the same population? How do we apply the principles of null hypothesis testing to binomial tests and chi-square tests in order to generate null and alternative hypotheses based on freuqency claims? 8.1 What are frequency claims? If you read a lot of news, like I do, you will often see an article about a scientific study that says that group X is more likely to get some sort of disease than group Y. Or something like “people who do X are so many times less likely to suffer from Y than other people”. These frequency claims are everywhere in research, in psychology, biostatistics, and in lots of other fields as well. A frequency claim is a claim, or a hypothesis, about the frequency that something happens in a sample. It may involve questions like if something is more frequent in one group versus another group. Or it could be a claim that a frequency of events is so unlikely to happen by chance, that something special is going on. What makes something a question involving frequency is that the outcome or dependent variable (the variable we measure) is a categorical variable. A categorical variable is a variable where the different levels, or possibilities for the variable, are unordered and do not have cardinality, i.e., no group is more than any other group. In many cases, the outcome variable is dichotomous, which means there are only two possible outcomes. Many variables end up this way: yes/no, hit/miss, dead/alive, in/out. All of these involve frequency claims, which we will consider in this module. In this module, we will consider two types of statistical techniques to evaluate frequency claims. The first is a binomial test. This is a test which examines the chance a given outcome will happen, given a baseline probability. The second technique is a chi-square test. This is a case which investigates proportions given one (or more) variables. The binomial test is what we would consider if we want to see if a sample came from a population. For instance, we may know that in a population, 7% of people are diagnosed with depression in a given year. We may want to know the likelihood it would happen, that if we selected 30 people, what is the chance we would find that 6 or more (20% or greater) would be diagnosed with depression in the last year? In the binomial test, the critical comparison is comparing a sample to a population. The chi-square test is most often used as a way of comparing whether two proportions are equal to one another. For instance, I may want to examine if children who watch a certain violent TV show are more likely to get in trouble than children who do not watch a certain popular violent TV show. I find that of the 80 children who watch the violent TV show, 60 did not get in trouble whereas 20 did. However, of the 125 children who did not watch the violent TV show, 118 did not get in trouble whereas only 7 did. As described below, in the case of the chi-square test, we are testing the likelihood that those two proportions (the children who watch the show versus the children who did not watch the show) are from the same population. 8.2 Binomial tests I did a study recently when I wanted to investigate whether court judges who were running for election were more likely to get elected if they looked more competent than their opponent. I had participants who did not know who these judges were see the faces of two candidates running in a judicial election and determine which judge they thought was the more competent-looking judge. If given a pair of people to evaluate, people generally agree as to which person looks more competent, even if people are unable to describe what specific facial features make a person look more competent. For each pair of elections, one candidate was perceived as more competent than the other candidate. If facial competence did not play a role in how judges get elected, then we would expect that 50% of the time, the more competent judge would be elected and 50% of the time the less competent judge would be elected. However, if more competent judges are more likely to be elected, then they would get elected more than 50% of the time. They would not get elected 100% of the time because there are other factors at play, such as their stances on issues, their campaigning, and so forth. To analyze this, I could do what is called a binomial test. A binomial test is a test which examines the likelihood of getting a sample of a certain size with a certain number of successes from a population where there is a certain chance of success. People use the word “success” to refer to the outcome they want to investigate, and it does not necessarily mean a good outcome. So for instance, I may investigate the chance that I get 8 or more heads (successes) if I flip a coin ten times with a 50% chance of heads. Then I apply null hypothesis testing to this logic. The idea is that if the probability of getting that sample is sufficiently low, below the alpha level, then I would conclude that the sample did not come from the population. I set the null hypothesis to be that my sample came from the same population and that the proportion of successes in my sample is not significantly different from the population. The alternative hypothesis is that the proportion of successes is significantly different from the population. For instance, I might test whether the coin I mentioned in the last paragraph is biased and lands on heads greater than 50% of the time. Since I got heads 8 out of 10 times, I would find the probability I would get 8 or more heads when flipping a coin ten times. To find out the chance that I would get 8 or more heads if I flip a coin ten times, I would need to find out all the possible outcomes of flipping a coin 10 times and then count how many of those outcomes involve 8 heads. I could start out by listing them, with H indicating heads and T indicating tails. I could list HHHHHHHHHH, HHHHHHHHHT and so on. However, with \\(2^8\\) or 256 possible outcomes, this would take forever. Fortunately, I can use the binomial distribution to calculate this for me. Like we mention in module 6, a probability distribution plot is a plot showing each outcome on the X axis and the probability of that outcome on the y-axis. There is a formula which calculates the chance of getting X successes in n trials given a p chance of success. However, using R, we can just calculate this using the dbinom() command. For example, the chance we get 8 heads in 10 trials, with a likelihood of success of .5 would be: dbinom(8,10,.5) ## [1] 0.04394531 There is a \\(p = .044\\) or 4.4% chance of getting 8 heads if we flip a coin ten times. We can plot the results for each coin flip below: To find out the chance I would get 8 or more heads from a coin, I can add up the chance I would get 8 heads, the chance I would get 9 heads, and the chance I would get 10 heads. This would add to \\(p = .055\\), or 5.5%. This would be greater than the traditionally accepted alpha of \\(p = .05\\), so I would fail to reject the null hypothesis. Even though I fail to reject the null hypothesis, the p value is practically very close to significant, so I would still be wary of the coin. As noted in the module on null hypothesis testing, failing to reject the null is not proof of the null hypothesis being correct. With all of this, there is a simpler way to do a binomial test in R using the binom.test() command. This command has four parts: binom.test(x, n, p, alternative = (&quot;two-sided&quot;, &quot;less&quot;, &quot;greater&quot;)) The first part is x, or the number of successes. The second is n, or the number of trials. The third is p, or the probability of success. The last part is alternative and it is where I list one of three options for the alternative hypothesis. If I want to test what the chance is I get x or more successes, I would use “greater”. For instance, to see the chance I get 8 or more heads out of 10 trials, I would type: binom.test(8,10,.5,alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 8 and 10 ## number of successes = 8, number of trials = 10, p-value = 0.05469 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.4930987 1.0000000 ## sample estimates: ## probability of success ## 0.8 In the output above, I can see the p-value in the second line, which is the p-value of success. Note in the third line, my alternative hypothesis is that the probability of success is greater than .05. If I want to see the chance I get 2 or fewer heads, I would type the following: binom.test(2,10,.5,alternative = &quot;less&quot;) ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.05469 ## alternative hypothesis: true probability of success is less than 0.5 ## 95 percent confidence interval: ## 0.0000000 0.5069013 ## sample estimates: ## probability of success ## 0.2 Notice in the output, that the chance to get 2 or fewer heads is the same as getting 8 or more heads. This is because the binomial distribution is symmetrical if \\(p=.5\\). The final option is called a two-tailed test. This is the probability that I get significantly different than the population probability, whether it is higher or lower than the original probability. For instance, I may suspect that a coin is biased but not know what direction it is biased, whether it gives more heads or more tails than normal. In this case, my null hypothesis is significantly different from .05 though I do not care whether it is higher or lower. If I do a two-tailed test, I am computing the probability I would get a result of a certain probability or more extreme in either direction. For instance, if want to do a two-tailed test with my coin that gave me 8 or more heads in 10 trials, I would calculate the probability of getting 8 heads in 10 trials and the probability of getting 2 or fewer heads in 10 trials. See the plot below, noticing the red bars: A two-tailed test is appropriate if you want to examine whether a sample is different from a probability but you have no hypothesis whether the sample probability is more likely or less likely than the baseline probability. It is more conservative than a one-tailed test, meaning is less likely to reject the null hypothesis given the same circumstances. For instance, if I calculate a two-tailed test using the binom.test() command, I would get the following: binom.test(2,10,.5,alternative = &quot;two.sided&quot;) ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.02521073 0.55609546 ## sample estimates: ## probability of success ## 0.2 Notice how the probability is \\(p = .109\\), which is double of the one-sided test. So I can take this logic back to my study on whether competence affected electoral outcomes. For example, I may have found that in 200 elections, the more competent judge won 120 elections. Since I have the hypothesis that competent judges are more likely to win elections, a one-sided test testing whether the probability is greater than \\(p = .05\\) is most appropriate. I would type the following: binom.test(120,200,.5,alternative=&quot;greater&quot;) ## ## Exact binomial test ## ## data: 120 and 200 ## number of successes = 120, number of trials = 200, p-value = ## 0.002843 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.5396838 1.0000000 ## sample estimates: ## probability of success ## 0.6 In this case, there is a \\(p=.003\\) chance that more competent judges would win elections 120 or more times if facial competence plays no role in electoral outcomes. 8.2.1 Using different probabilities All of the above examples have a probability of \\(p = .5\\) for the outcome. However, this is not always the case. For instance, I may want to investigate the probability that individuals in a certain town are more likely to suffer from depression than the population at large. In the example above I note that in a population 7% of people are diagnosed with depression each year. However, in a certain stressful company, 6 of the 30 individuals working there suffer from depression. I want to test if that rate is significantly larger than the population. I would set the null hypothesis as the rate of depression in that company as being not significantly more than the population and the alternative hypothesis is that the rate of depression in that company is significantly more than the population. I could test this using the binom.test function, with 6 successes (note again successes does not mean good things in this context) out of 30 trials and a probability of success in the population of .07. As noted by tradition, I set my alpha level as .05, and would reject the null if the probability is below alpha. binom.test(6,30,.07,alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 6 and 30 ## number of successes = 6, number of trials = 30, p-value = 0.01623 ## alternative hypothesis: true probability of success is greater than 0.07 ## 95 percent confidence interval: ## 0.09087406 1.00000000 ## sample estimates: ## probability of success ## 0.2 In this case, I would reject the null hypothesis because the p-value is less than .05 and conclude that workers in this company are significantly more likely to suffer from depression than the population at large. 8.3 Chi-square tests The binomial test works well if we want to know whether a sample’s proportion fits a population’s proportion. However, in some cases we want to compare the proportion of two or more groups. This might occur because we don’t know the population’s proportion or because we are specifically interested in two groups that might be different from the population. For instance, I may want to test two interventions designed to keep at-risk children from trying illegal drugs. I randomly assign children to be in one program or the other program. I have one program which I call the Danger Program that uses a class to teach children about the dangers of drug use. The second program I call the Mentor Program which does not focus on the negatives of drug use but instead encourages children to have a positive, healthy life without drugs, and pairs children with teenage mentors who do not use drugs. After doing the programs, I wait a year and then I have a survey where I ask each child whether or not they tried illegal drugs in the last year. I find the data below: Program Used drugs Did not use drugs Danger 28 57 Mentor 11 58 Based on this table, I might think that my program worked. Drug use is lower in the Mentor Program. However, this could have happened by chance that I by chance randomly assigned the children more likely to use drugs to the Danger program. The chi-square test analyzes this problem. In the chi-square test, the null hypothesis is that the proportions in each of the groups are statistically the same. That would mean that my predictor variable (the program the child was assigned to) is unrelated to my outcome variable (whether or not a child used drugs). This is called independence and means that the two variables are independent. The alternative hypothesis is that the proportions in the two different groups are not statistically the same. This would imply the variables are dependent or that the predictor variable and the outcome variable. This means the two variables are related, but does not say how they are related. When we do a chi-square test, we usually have the idea that one variable is the one which is “causing” the change on the other variable. In this case, I think the program causes a change in drug use. I usually call the “causing” variable the predictor variable and the other variable as the outcome variable. However, based on the math about how a chi-square test works, it does not matter which variable is the predictor and which one is the outcome, or which order I enter the data. To do a chi-square test, we have to enter the data into R as a data frame with the frequencies for each condition. To do this, we have to make a variable for each predictor variable, with the values for each of the different outcomes, and then put them into a data frame. Then I do the chi-square test by typing chisq.test(). I would type the following: danger = c(28,57) mentor = c(11,58) d = data.frame(danger,mentor) chisq.test(d) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: d ## X-squared = 4.9553, df = 1, p-value = 0.02601 The chi-square test gives me my chi-square statistic, the degrees of freedom, and the p-value. As with the other null hypothesis tests, the p-value is testing whether the probability I would get these frequencies if the null hypothesis is true. If that probability is below alpha (usually .05), then I reject the null hypothesis. In this case, I would reject the null hypothesis and conclude that the two variables, program and drug use are related. A significant chi-square statistic does not say in which way they are related, but in this case, we can clearly see that it is because fewer children who did the mentor program used drugs than the danger program. 8.3.1 Chi-square tests with more than two outcomes We do not have to have dichotomous data when doing a chi-square test. We can have variables where there are more than two possible outcomes. In one example of a student experiment, the investigators wanted to see whether people preferred birth order personalities that were similar to their own or different from their own. There is a controversial idea that dates back to the psychologist Alfred Adler that first-born children will have a different personality than middle-born children and last-born children. The idea of this study was that participants would read personality profiles of people and pick which one they were most likely to date. The people had first-born and last-born children pick which personality they liked and found the following: Participant Birth Order First Middle Last First-born 32 40 11 Last-born 20 54 29 To analyze this, we have a participant’s birth order as the predictor and the type of personality they preferred as the outcome variable. I set up my null hypothesis that one’s birth order has no effect on the personality they pick and the alternate hypothesis that one’s birth order does have an effect on the personality they pick. To do a chi-square test, I have to enter the data like before, but instead of entering in two numbers for each condition of the predictor variable, I will enter three, the number who preferred the first-born personality, the middle child personality, and the last-born personality. first = c(32,40,11) last = c(20,54,29) d = data.frame(first,last) chisq.test(d) ## ## Pearson&#39;s Chi-squared test ## ## data: d ## X-squared = 10.93, df = 2, p-value = 0.004232 I find here that the p-value is less than .05, which leads me to reject the null hypothesis. That just means I conclude there is a relationship between one’s birth order and the personality they pick. Now, what I am really interested is in what that relationship may be. It could be the case that people are more likely to prefer the birth order most similar to their own. Or it could be that people are more likely to prefer the birth order that is different from their own. In order to interpret the data, then I look at the numbers themselves and try to figure out the story. In this case, it is clear that people tend to prefer the middle-born personality, probably because that personality is said to be laid-back and good at relating with people. However, people who do not prefer the middle-child personality tend to like their own birth order, suggesting that similarity is what matters. Interpreting the data like this may be subjective and the more variables and possibilities I have, the more complicated and subjective this gets. For instance, if I had middle-children and only-children in this study, I could have four different levels for my predictor variable. In this study, we omitted those variables because there were not many participants who fit that category, but it did make things simpler. 8.4 Summary In this chapter, we covered the idea of frequency claims. A frequency claim is a claim where we assert that how often something occurs in a population. The two types covered here are binomial tests and chi-square tests. Binomial tests examine whether the frequency something occurs in a sample is the same or different than in a populaton. Chi-square tests examine whether the frequency something occurs in one sample is the same or different than in another sample. After this chapter, you should be able to: Know what is meant by a frequency claim Know how to apply null hypothesis testing to binomial tests and generate null hypotheses and alternative hypotheses Know how to conduct a binomial test using R and how to interpret the output Know how to set up null and alternative hypotheses for a chi-square test and know when to conduct these tests Know how to do a chi-square test in R "],
["association-claims-1.html", "Chapter 9 Association Claims 9.1 What are association claims 9.2 Correlation 9.3 Using Linear Regression 9.4 Evaluating linear regressions 9.5 Linear Regression in R 9.6 Limitations to linear regression and correlation 9.7 Summary", " Chapter 9 Association Claims In this chapter, you will learn: What an association claim is and how to give examples of association claims What a correlation represents and how to calculate a correlation in R How to apply null hypothesis testing to run a correlation test, including generating null and alternative hypotheses, running the test in R, interpreting the output, and reporting a correlation using APA formatting What a linear a regression is, including what a regression represents, and how to use a regression equation to predict a dependent variable given an independent variable How to conduct a regression test in R, and understand the output to get the regression equation, the R-squared value, and the residuals What some limitations of correlation and regression are and when to apply these techniques. 9.1 What are association claims In many cases, we have experiments or data where we want to examine the relationship between two variables. Is one variable related with another? Does a change in one variable lead to a change in another variable? For example, a researcher might want to know if increased time studying leads to better grades in a class. Another researcher might want to see if there is a relationship between how much people exercises and their blood pressure. Or we may want to see if there is a correlation between a person’s education and their income. Association claims are very powerful because we can measure them in situations where we cannot do experiments. We might have the hypothesis that two variables are related but we cannot manipulate those variables. Therefore, to test these hypotheses, we would measure those two variables and see if they are associated. Claims of association only indicate that two variables are related. They do not say whether one variable causes another variable. Statisticians like to shout “correlation does not imply causation” for good reason, because a correlation or regression does not mean that one variable causes another variable. One example where this has happened in research is the association between violence on television and violent behavior in children. Several studies showed there was an association between these two variables. Children who watched more television violence behaved more violently. Many observers assumed that meant that television violence caused violent behavior. But they neglected other ideas. Whenever there is an association between two variables, there are three possible relationships: Variable one causes variable two. In this case, this is the assumption that watching television violence causes children to behave more violently. Variable two causes variable one. This is called reverse causality and many researchers miss this. In the example above, reverse causality would be the idea that violent behavior in children cause children to watch more television violence. This might sound like a strange idea at first, but it may be the case that children who are more likely to be violent also like to watch violent television. A third outside variable causes both variable one and variable two. Many other variables could cause both violent behavior and children to watch violent television. One possibility is parental neglect. Parents who neglect their children may be more likely to cause them to behave more violently because they are not disciplining their children and to be more likely to watch violent television because they are not preventing their children from watching violent television. Even though correlation does not imply causation, correlation and regression are very powerful because they can suggest a causal link. There are many experiments we can’t do because they are not ethical and so we have to do correlations to examine them. One example of this is the very strong correlation between how much someone has smoked and their likelihood to get cancer (many different kinds). We can’t do an experiment to test whether how much someone smokes causes them to get cancer because it’s unethical to assign some people to smoke for a long period of time. In this case we’re limited to correlation studies and the head of tobacco companies liked to claim that since all these studies were correlational, scientists could not prove that smoking causes cancer. However, in this case, correlation may strongly link to causation. We have to go through the three different possibilities here: Smoking causes cancer: this is very likely because there’s a lot of evidence that the chemicals in tobacco smoke cause cell damage in precisely the way that may cause cells to be cancerous. In addition, experiments with animals strongly suggested that smoking caused cancer in animals and it is not much of a stretch to say that this would be true in humans. Cancer causes smoking: this isn’t very likely. One reason for this is the idea of temporal precedence. Causes have to come before the thing they cause. If one variable comes after the second variable, then it is impossible that it caused the variable before it. The smoking occurs after the cancer so it is very, very unlikely this is the case. A third variable causes both smoking and cancer: There is some truth to this idea. People who smoke are more likely to do other behaviors that may lead to cancer, such as exercise less and have a generally unhealthy lifestyle. However, a lot of more advanced studies show that these relationships can’t accunt for the increased risk of cancer that comes with smoking. The two ways we can examine association claims are correlation and regression. Though these are two different techniques, the underlying math is the same so whether you use a correlation or a regression to answer a question, it won’t matter. In this book, I will talk about using correlation to examine the strength of a relationship between two variables and regression to talk about creating a model or a mathematical function to use one variable to predict another variable. 9.2 Correlation If we want to test association claims, we can use the idea of null hypothesis testing we discussed in Chapter 7. In order to do that, we will do the following steps: Set up a null hypothesis Set up an alternative hypothesis Choose our alpha level, or the chance of a type 1 error, that we falsely reject the null hypothesis (usually .05) Test what the probability is that we would get the data we got, if the null hypothesis is true If that probability is below alpha, we reject the null hypothesis. In Chapter 7, we learned how to complete all of these steps, except how to test the probability that we would get our data by chance if the null hypothesis is true. In order to do that, we can use correlations. Correlation is a measure of the co-relationship between two variables. It’s been around since the mid-1800s, when it was first discovered by the French mathematician Bravais. But correlations became most prominent when Sir Francis Galton began to use them to examine heredity. One of Galton’s biggest questions was the degree to which children took various traits from their parents. In this case, he wanted to see the co-relationship between a parent’s trait and a child’s trait. To do this, he developed the concept of correlation. He gave this quality the name \\(r\\) to stand for “regression” because it is related to regression, as we’ll see in the next section. Correlation is a standardized form of the property of covariance and is found using the following formula, assuming two vectors (lists of numbers) named x and y: \\[r = \\frac{\\text{covariance}(x,y)}{s_xs_y}\\] Correlations range from -1 to 1. Negative correlations imply a negative relationship, that is, an increase in one variable leads to a decrease in another variable. Positive correlations imply a positive relationship, where an increase in one variable leads to an increase in another variable. Negative correlations indicate a negative relationship, where an increase in one variable leads to a decrease in another variable. One researcher suggested the following guidelines for correlations and what kind of relationship they indicated: Correlation Type of Relationship less than -0.7 or greater than 0.7 Strong Between -0.7 and -0.4 or 0.4 to 0.7 Moderate Between -0.4 and 0.4 Weak or no relatonship Based on the table above, if we calculated a correlation of \\(r = 0.8\\), then we would assume it is a strong relationship. Likewise, a correlation of \\(r = -0.2\\) would be a weak relationship. As noted in the table, very low negative numbers, or negative numbers with high magnitude, those close to -1 are also strong correlations. They are just strong negative associations. One example of a negative correlation may be the amount of exercise a person does in a weak and their blood pressure. Research studies show that a person who does more exercise may be more likely to lower their blood pressure. 9.2.1 Calculating correlations A correlation is a mathematical property of two sets of numbers. If I took two random sets of numbers that should be unrelated, such as the day of the month each person in a class was born and their shoe size and calculated the correlation between those numbers, it is likely that correlation will be very low, because there should be no association between those two numbers. But as we learned in the other modules, there is a chance that by chance alone, a correlation would between those two numbers. These patterns are even called illusory correlations, or correlations which seem to appear just by chance, even if there is no association between the two variables. For instance, if I randomly generate two lists of 10 numbers and take the correlation between the two numbers, I would expect a correlation of 0. There is no relationship between the two lists. But if I do this, I don’t always get zero. In the figure below is a histogram of the results of the correlation of two randomly generated lists of 10 numbers. On average, the correlation is zero, but many of the correlations are rather far from zero. From the bars, you can see on average, there are over 150 instances where the correlation is either below -0.5 or above 0.5. So there is a chance that a high correlation could happen just by chance alone, and not because the two variables are related. So to address this, what we have to do is to find the correlation between two variables and see what the chance is that we would get a correlation that large, or stronger, if there is no relationship between those two variables. If that correlation is high enough, the chance you would get a correlation that strong, or stronger, is very low. If that chance is below the alpha we set, then we reject the null hypothesis. Imagine if we want to test whether there is a relationship between how long a person studies and their grade on an exam. We can use the logic we discussed with null hypothesis testing to figure this out. First we would set up null and alternative hypotheses. In this case, the null hypothesis would be “there is no relationship between the number of hours a person studies and their grade on the exam”. The alternative is the opposite of this: “there is a relationship between the number of hours a person studies and their grade on an exam”. Second, we would choose our alpha level. By tradition, we use an alpha of .05, which indicates a 5% chance we would have a type 1 error and falsely conclude there is a relationship when none is present. Then we would calculate the correlation between the two numbers. Assume in this example, I found out that there is a .5 correlation between hours studied and the grade on the exam. Now I have to calculate the probability I would get a correlation of .5 or more extreme, given that the null hypothesis is true. Calculating the probability of getting a certain correlation requires a lot of mathematics which go beyond this book, but fortunately we can find this probability out easily. R has the function cor.test(x,y) which gives the correlation and the probability of finding out that correlation. For this function, we give R two lists of numbers and then use this function to get the output. Imagine I have two lists, x which is the list of how long a person spent studying and y which is their grade on an exam x = c(3,8,11,4,9,6,8,12,7,5) y = c(78,85,93,68,79,94,85,93,76,71) cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 2.6397, df = 8, p-value = 0.02973 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.09232484 0.91768842 ## sample estimates: ## cor ## 0.6822895 The output above gives me a lot of information. The most important value is the correlation, which is the last number given (the one under “cor”). In this case, \\(r = .68\\). Based on the guidelines above, this is a moderate relationship that is very close to being a strong relationship (\\(r &gt; 0.7\\)) The other important information is the degrees of freedom (listed as “df”) and p-value, both listed in the second row. In this example, the p-value is .030, which reflects a 3% chance that a correlation of .68 or more extreme would occur by chance alone in a sample of 10 individuals. This is below our alpha of .05, so we would reject the null hypothesis. The probability of getting a correlation by chance depends on two factors, the strength of the correlation and the number of pairs of observations being correlated, or the sample size. The more observations you have, the less likely you will get a large correlation by chance. Look at the figure below. I randomly took two lists of numbers and correlated them 1000 times. Since each list was randomly generated, there should not be a correlation between the two lists. But as I noted above, there is a chance of getting high correlations by chance alone and this chance depends on how many pairs of observations we are correlating. In each list, I varied how many observations were in each list. In the top part, I made the list of numbers 10 observations long and had 10 pairs of observations. In the second, each list only had 5 observations, and on the bottom, each list had 20 observations. As you can see, as the number of observations goes up, the frequency of more extreme values goes down. For lists of 10, there are occasional values that are greater than 0.5 or less than -0.5. For lists of 5, values more extreme than 0.6 are very common. However, there are almost no correlations above 0.6 or below -0.6 when using lists of 20 individuals. The more observations we have, the less likely it is to get a high correlation by chance alone. However, it is not exactly the number of observations we have that matters. Instead, it is a concept called degrees of freedom. If you note the R output from the cor.test() function, it gives degrees of freedom. In the example above, I took the correlation of 10 pairs of values. However, the degrees of freedom are not 10, they are 8. Degrees of freedom are technically defined as the number of possibilities used to compute a statistic that are free to vary. That may sound complicated, so I’ll give you an example. Imagine you take a standard deck of playing cards, which has 52 different cards. If you draw the first card, it can be any card. Let’s say it’s the 5 of spades. Then you draw a second card. It can be any card except the 5 of spades. Each card you draw can be one of many different values. However, when you have drawn 51 cards and there is one card left in the pile, you do not have to turn it over to know what that card will be, if you’ve been paying attention. So a 52 card deck of playing cards has 51 degrees of freedom. You can arrange the cards any way you want to, but once you’ve arranged the first 51 cards, you have no freedom to choose a different card for the 52nd card. A second example is this. Imagine we have a class of students and we know the mean grade for the entire class is a 90. I pick one student and ask their grade. Their grade can be anything; there is no way I can guess this. I write this grade down. I do this for every person in the class except one person. When I get to the last person, I don’t need to ask them their grade. That is, based on all the numbers I’ve been given, there is only one possible grade that will make the entire class’s average equal 90. With correlations, due to various mathematical reasons, if I know the correlation and I know all but two pairs of observations, I know what the other observations must be. Therefore, the degrees of freedom for correlations is n – 2 with n meaning the number of pairs. Remember this, because this is a test question that most of my students get wrong. Fortunately, if you use the cor.test() function, R will remember this for you and give you the correct degrees of freedom. However, you do need to know this if you report correlation values using APA formatting. You report a correlation using APA formatting using the following format: r(df) = r-value, p = p-value In the example above, I would report r(8) = .69, p = .03 9.3 Using Linear Regression Correlation tells us that variables are associated. Regression is a technique where we use one (or more) predictor variables to predict a variable using a mathematical function. The variable(s) doing the predicting are called predictor or independent variables; the variable being predicted is the outcome or dependent variable. Regression works by fitting a mathematical function that best approximates the relationship between the predictor variables and the outcome variable. It is based on the following idea: \\[y = f(x_1...x_n) + e\\] In this case, we have a predictor variable y which is the outcome of a function of a series of one or more input variables (the \\((x_1...x_n)\\)) and error. Simply, a regression or any model is this: \\[\\text{outcome} = f(\\text{inputs}) + \\text{error}\\] For instance, I may want to predict college GPA by using high school GPA and standardized test scores as predictors. I take a student’s high school GPA and standardized test score and then “do math” to them and get an output, which is my prediction about the student’s performance. Another example of a model is present when I watch Netflix. Netflix takes a bunch of ratings I’ve made about various movies I have seen (the inputs), does math to them, and then gives me an output, saying that I am 98% likely to like a certain movie. However, these functions always have error. The model predicting a person’s college GPA will not always get it right. Netflix definitely doesn’t get it right all the time when guessing which movies I’ll like. The goal of regression is to make a function that minimizes this error. Whether a model works depends on three factors: Are the inputs good data? The predictor variables should be related to the output variables. If they aren’t good data, then the model won’t work well. If I use a person’s favorite ice cream flavor and their time running a mile as predictors for college GPA, I probably won’t do well. Do we have the right number of inputs? More inputs mean better models, as long as they are good inputs. In this chapter, we’ll only talk about using one input value, but in multiple regression we can expand our model to have a lot more inputs. If we don’t have enough inputs, our predictions won’t be very good. However, too many inputs will throw off our model. Do we use the right mathematical function? In this chapter, we’ll talk about linear regression. That uses a straight line to predict linear relationships. Not all relationships are linear and in those cases, linear regression is not a good idea. Linear regression uses one or more input variables to predict an outcome variable if you assume a linear relationship (that is, a straight line). If you remember in algebra class, a line can be modeled as an equation of the form: \\[y = a + bx\\] In this case, a and b represent constants and x and y represent variables. If you graph this on a Cartesian plane, you will get a line. Linear Functions from Wikipedia For each value x that is on the line, you will get the indicated value of Y. With linear regression, what we want to do is find a line that best models our data. Imagine we have a scatterplot like the one below. We want to find out which line best fits the data in the scatterplot. Note that the points don’t all fit on the line, so no line is perfect. But we want to find out which line is the closest to capturing these points. Linear Regression from Wikipedia The degree to which a line fits the data is a conceptual question, meaning there is no right answer to it. People disagree as to how to do that. But the way most commonly used is one where a line minimizes the squared deviations from the line. What that means is we want the line that minimizes the total squared distance each point is from the line. There are mathematical ways for doing this. In order to figure out this line, we need to figure out two coefficients, a and b. The coefficient a represents the y-intercept of the line, or the value we would predict for the outcome variable y when the predictor variable x is equal to zero. Then we need to figure out the slope b, which represents how much we would predict y would increase or decrease when x increases by 1. Here is an example measuring how many hours a person slept the night before and their score on a 12 question memory test. In this case we predict a positive relationship between the two variables, more sleep equals better memory. Sleep Items remembered 7 8 7 6 6 7 7 9 5 3 9 10 5 7 6 5 9 10 8 11 If we look at the scatterplot, we see some evidence about this. Now, we can calculate a correlation to examine if there is a relationship between the two concepts. If we do this, we would find a correlation of r(8) = .81, p = .004. If we were testing this using null hypothesis testing, we would reject the null hypothesis that there is no relationship between these two variables. However, we may want to answer the question of: if a person sleeps X number of hours a night, how many items would we predict that they would remember? To do this, we would use linear regression. In order to do linear regression, we first need to calculate the slope of the line. We know that the slope of the line is the coefficient b. There are mathematical proofs that say \\[b = \\frac{\\text{cov}(x,y)} {var(x)}\\] In this case, b is the covariance of the two variables divided by the variance of x. We can find the covariance in R by typing cov(x,y) In this case, the covariance of x and y is 2.96 and the variance is .81. So: \\[b = \\frac{2.96}{.81} = 1.41\\] This gives us the slope. However, what we would need is the y intercept, or the coefficient a. Another mathematical proof tells us that the point \\((\\bar{x},\\bar{y})\\), or the mean of x and the mean of y is on the line of best fit. In this case, the mean of x is 7.6 and the mean of y is 6.9. Given that we know this point is on the line, we can do the following and solve for a: $$y = a + bx$$ $$\\bar{y} = a\\bar{x} + b$$ $$\\frac{\\bar{y} - a}{\\bar{x}} = b$$ After solving for b, we can paste in our values and then calculate a: $$\\frac{\\bar{y} - b}{\\bar{x}} = a$$ $$\\frac{7.6 - 1.41}{6.9} = .90$$ Now we have our regression equation: \\[y = .90 + 1.41x + e\\] What this means is that the line indicated by this equation is the best linear predictor of our data. I added the e at the end of this because for each x value, the y value is not exactly right. 9.4 Evaluating linear regressions The statistician George Box wrote a section of a book called “All models are wrong but some are useful”. This saying is one of the most important in statistics. If after this class you would like to get a stats-themed tattoo, this would be good quote to use. What this means is that all of our regressions are wrong. They never predict reality exactly. When presenting the linear regression equation in the last section, we used the formula: $$y = a + bx + e$$ The e in this equation refers to error, or the fact that our regression equation does not fit our data. We use math to try to minimize this, but we will still have error. So, how do we measure error and know how good our equation is? If the predictor variables do not relate to our outcome variables, the error will be high. However, if the predictor variables are very related to outcome variables, the error will be low. For instance, I want to predict someone’s grade in this class. If I use shoe size as a predictor, my regression will not be very good, since shoe size tells me little about stats performance. However, if I use people’s grades in math in high school, that would be a good predictor. We assess predictors by looking at residuals. Residuals are defined as the difference between the observed value for y or the outcome variable, and our predicted variable. For instance, we calculated the following regression equation in the last section. \\[y = .90 + 1.41x + e\\] In this equation, we would predict that if we have a value of 6 for x, a person who slept six hours the night before, we would get 9.36 for y, our best guess about how many items the person remembered. Residuals are a measure of how far our predictions are from our data. To examine them, we have to use the regression equation and input every value we have for the x variable and see what we would predict for the y value given our regression line. Then we compare those predictions with the actual values in the y variable by subtracting the predicted value from the observed value For each set of observations or for each person, use the value of x to find out what we would predict for y Find the difference between the observed and predicted value. Do this for all your observations. Here’s an example: I wanted to correlate grades in the introduction math class in a college with grades in a stats class. I had each person’s grade in the math class scored using a 4 point scale, with an A being a 4, a B being a 3, a C being a 2, and a D being a 1 (a passing grade is required for stats). I have as the outcome variable the grade in the stats class on a 100 point scale. Based on this, I calculated the following regression: Person Math grade Stats grade Residuals Squared Residuals Squared Y Deviations 1 3 90 3 9 29.7521 2 4 95 2 4 109.298 3 3 85 -2 4 0.20661 4 2 80 -1.2 1.44 20.6612 5 3 90 3 9 29.7521 6 4 95 2 4 109.298 7 3 80 -7.5 56.25 20.6612 8 2 75 -6.7 44.89 91.1157 9 1 75 -0.5 0.25 91.1157 10 1 80 4.4 19.36 20.6612 11 2 85 3.8 14.44 0.20661 Mean 2.545454545 84.545455 0 Sum = 166.6 Sum = 522.7 Notice that the mean of the residuals is zero. This is because for some values, our prediction of their stats grade based on their math grade is higher than they scored, and so they have a negative residual. Some people’s predictions is lower than we expected, and they have a positive residual. So the sum and mean of the residuals will always be zero. But notice that even for the extremes, we are only off by 6 or 7 points and for most people, our prediction is within 5 points of their final grade. This is a good predictor. One way we can assess how good a regression is at predicting data is by calculating the R-squared value. R-squared is also called the coefficient of determination, and it is the percent of variance in the outcome variable that is explained by the predictor variables. For instance, an R-squared value of .25 means that 25% of the variance in the outcome variable is explained by our regression and 75% is not explained, or is error. When we calculate an R-squared value from a regression, we will never be able to have a regression that explains 100% of the variance in the outcome variable. This is why Dr. Box said that all models are wrong. But the better the model explains the variance, the higher the R-squared value is, and the more useful the model is. R-squared is calculated using the following formula: \\[R^2 = 1 - \\frac{SS_e}{SS_y}\\] In this case, the \\(SS_e\\) represents the sums of squares error, which is the sum of the squared residuals. \\(SS_y\\) is the sums of squares of the outcome or y variable and as mentioned in Module 3, can be found by adding the squared deviations of the y variable, which is for each y variable the squared difference between that observation and the mean of all the y variables. Alternately, for a single linear regression, with a single x variable predicting a single y variable, the R-squared value is the correlation squared, which is why it is called R-squared. In the table above, I have included the squared residuals as the second from the right column and the squared Y deviations as the rightmost column. Based on this, if I do the following, I can find R-squared. \\[R^2 = 1 - \\frac{166.6}{522.7} = .68\\] Based on this example, grades in a math class predict 68% of the variance in performance in a stats class. What this means is that our regression predicts 68% of the variance in the data we provided. What this does not mean, however, is that performance in the math class causes 68% of the variation in the stats class. As with correlation, there are many other causal pathways to consider. For instance, overall intelligence would cause higher scores in both a math class and a stats class. Studying ability can cause higher scores in both. Basically, those factors, called common causal factors, can cause a higher R value as well as the predictors causing a change in the outcome. Finally, the outcome can cause a predictor as much as the predictor can cause an outcome. When we do a regression, we decide based on our theory which variable we think is the predictor and which one we think is the outcome. If our predictor comes before our outcome in time (like yesterday predicting today), this makes sense. However, sometimes it is hard to consider what variables are causing and what variables are being caused. This goes back to the quote at the beginning of this section. Models are useful. They can tell us how variables are related but they 9.5 Linear Regression in R Now, I want to show you how to calculate this in R. Regression is very simple in R and it uses the lm() function. First we enter our data: sleep = c(7,7,6,7,5,9,5,6,9,8) items = c(8,6,7,9,3,10,7,5,10,11) d = data.frame(sleep,items) Now we can use the lm() function: x = lm(items~sleep, data = d) There are a few things to note. First, I save the output of the lm function to the variable x. This is important because the function lm() gives us a lot of outputs and we want to use some of it. Second, I enter in the two variables I’m interested in, entering in the dependent variable first, a tilde, and then an independent variable. In this case, I believe that sleep predicts items remembered, so I enter “items~sleep”. Finally, I type data = d, giving R the data frame I’m interested in. When you type this into R, you will get no output. To see the regression, I have to type: summary(x) ## ## Call: ## lm(formula = items ~ sleep, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9259 -1.1389 -0.1482 1.1111 2.0741 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.1111 2.4916 -0.847 0.4215 ## sleep 1.4074 0.3541 3.974 0.0041 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.54 on 8 degrees of freedom ## Multiple R-squared: 0.6638, Adjusted R-squared: 0.6217 ## F-statistic: 15.79 on 1 and 8 DF, p-value: 0.004095 This gives us all sorts of information! The first line tells us the regression equation we used. The second part tells us about the residuals. Instead of giving all the residuals, it gives us the minimum, first quartile (25th percentile), median, third quartile (75th percentile), and maximum. If we want to see the actual residuals, we can type x$residuals. The next section tells us about the coefficients. The first row is the intercept, reflecting the \\(a\\) value we talked about above. The first column, labeled “Estimate”, is the value of the y-intercept. Then the other columns tell us more information about the estimate, which we will talk about in a future chapter. The final row tells us the estimate for b, which is the sleep variable. The first column tells us the value for b. Finally, in the next to last row of text, we can find the R-squared value. R also gives an adjusted R-squared value, which takes into account some limitations about regression and how linear regression should only be used in certain contexts, which we’ll talk about in the last section. 9.6 Limitations to linear regression and correlation There are several warnings where you shouldn’t use linear regression. Linear regression assumes that both of your variables are normally distributed. Even if this might be the case, there are several caveats where linear regression may not be a good idea. Linear regression and correlation assume a linear relationship, assuming the relationship should fit a straight line. However, not all data fit a straight line. If the relationship is curvilinear, a linear regression doesn’t work. For instance, the relationship between caffeine and performance may be curvilinear. A little bit of caffeine causes performance to go up, but then too much may cause performance to go down, causing an inverted U shape. A linear regression can’t fit this kind of curvilinear relatinonship. Look at the following graph, from a hypothetical study where I tested how caffeine (measured as cups of coffee over the 4 hours before a test) affected memory on a test. As you can see, as a person drinks more caffeine, their memory improves up unto a point, where the caffeine starts to impair memory. Caffeine = (1:500)/120 Items = (-4*Caffeine^2 + 18*Caffeine + 80) + rnorm(500)*4 d = data.frame(Caffeine,Items) graph = ggplot(d, aes(x = Caffeine, y = Items)) graph + geom_point(size = 2) + labs(x = &quot;Caffeine&quot;, y = &quot;Items Remembered&quot;) rm(Caffeine,Items,d) If I did a linear regression, I would have a line that fits this data like the following, which suggests almost no relationship. Caffeine = (1:500)/120 Items = (-4*Caffeine^2 + 18*Caffeine + 80) + rnorm(500)*4 d = data.frame(Caffeine,Items) graph = ggplot(d, aes(x = Caffeine, y = Items)) graph + geom_point(size = 2) + labs(x = &quot;Caffeine&quot;, y = &quot;Items Remembered&quot;) + geom_smooth(method = lm, se=F, size = 2, color = &#39;darkred&#39;) rm(Caffeine,Items,d) In this case you could use a different function to predict your data, which is beyond the scope of this chapter. Outliers can have too much effect on correlation and regression. An outlier can make a regression be much more significant than it already is. If your relationship disappears if one data point is removed, then it may not be a real relationship. A regression line can only give valid predictions in the range of data that it is given. If I am looking for the relationship between hours of sleep and exam performance, I might get a regression like like \\(y = 65 + 3x\\). When calculating the regression my participants slept between 5 and 9 hours. I can’t say then “if I get 15 hours of sleep the night before the exam, I should predict a 100”. I can only make valid predictions for values of sleep between 5 and 9 hours. created a series of datasets that illustrate how regression can get conclusions wrong. Here are four datasets which all have the same regression line: In this example, the first plot shows a basic linear relationship. The second shows a curvilinear relationship where the line does not fit the curve very well at all. The third and fourth examples show what outliers can do to data. In example 3, there is a perfect linear relationship between X and Y except for one point. In Example 4, there is no relationship between X and Y except for one point. In both cases, the line does not reflect the data. The important thing with regression is to visualize your data. Look to see if there are outliers in your data which might throw things off. 9.7 Summary Correlation and regression are powerful ways to examine the relationship between two or more variables. After reading this chapter, you should be able to: Know what an association claim is provide examples of possible association claims Know what a correlation represents and be able to calculate a correlation in R Be able to apply null hypothesis testing to run a correlation test, including generating null and alternative hypotheses, running the test in R, interpreting the output, and reporting a correlation using APA formatting Be able to understand what a regression is, including what a regression represents, and how to use a regression equation to predict a dependent variable given an independent variable Be able to conduct a regression test in R, and understand the output to get the regression equation, the R-squared value, and the residuals Understand some limitations of correlation and regression and when you should not use these techniques. "],
["one-sample-mean-claims.html", "Chapter 10 One Sample Mean Claims 10.1 One sample mean claims 10.2 The Z-test 10.3 The One-Sample T-test 10.4 Confidence Intervals with One Sample 10.5 Calculating T-tests and Confidence Intervals in R 10.6 Summary", " Chapter 10 One Sample Mean Claims In this chapter, you will learn: What a one-group mean claim is, why we would use it, and be able to give some examples of a one-group mean claim How to apply what you learned in Modules 6 and 7 to calculate a z-test by hand and evaluate the p-value associated with the null hypothesis by using the pnorm() command in R What the t-distribution is and how to conduct a one-sample t-test by hand and in R by applying null hypothesis testing What a one-sample confidence interval is and how to calculate it by hand and in R. 10.1 One sample mean claims I broke up the claims in this book into three categories: frequency claims, association claims, and mean claims. The next three chapters will evaluate different types of mean claims. A mean claim is a claim that we are testing about the mean of a group. I break them down into three types: claims about one mean, claims about two means, and claims about two or more means. One sample mean claims are claims where we test whether a sample mean is equal to a population mean. This is testing the question of whether a sample is part of a population. If the sample is part of the population, then the sample mean (\\(\\bar{x}\\)) should be equal to the population mean (\\(\\mu\\)). Or mathematically: \\[\\bar{x} = \\mu\\] Two sample mean claims involve comparing whether two sample means are from the same population mean. If the two sample means come from the same population, then the sample means are equal. Mathematically, this is: \\[\\bar{x}_1 = \\bar{x}_2\\] Two or more sample mean claims involve comparing two groups or more. They test the hypothesis that all of the means are from the same population. Mathematically, this is: \\[\\bar{x}_1 = \\bar{x}_2 = \\bar{x}_3 = \\ldots\\ = \\bar{x}_n\\] Two sample mean claims and two or more sample mean claims will be covered more in Chapters 11 and 12. In this chapter, we are focusing on claims about a population and a sample. What this means conceptually is that we have a sample and we want to test whether that sample came from the population by comparing the sample mean to the population mean. Here are some examples of a one sample mean claim. For instance, I may know that the average life expectancy of females in a certain country is 78 years. I may want to test whether people in a coastal village (the sample) lives significantly longer on average than the population. Another example: I know that the mean IQ in the population is 100 (in fact the IQ test is defined so that this is the case). I want to test whether preschoolers at a certain preschool (the sample) have a higher IQ than the population at large. A third example: I know that the mean number of items that people can remember on a certain memory test is 30 items. I want to test whether people who learned a mnemonic technique can increase their performance on the test. I teach a sample of people the technique and then test whether they remember more than 30 items. In each of these examples, I am comparing a sample to see whether that sample came from the a population. Combining this with null hypothesis testing and decision theory, I can apply this technique to examine inferences about experiments. The logic of this section will be a bit weird, so be warned. The concepts I am going to talk about here are not hard, but they tend to be complicated because they seem backward and counter-intuitive. We’ll talk about this with the z-test 10.2 The Z-test In the module on decision theories, we talked about the idea of a null hypothesis and an alternative hypothesis. This logic can be used to make decisions about data. It uses the following logic: we set up a null and alternative hypothesis, we examine our data to determine the chance our data would occur if the null hypothesis is true, and if that chance is sufficiently low, we reject the null hypothesis. We can use this logic to answer other questions as well. Here’s one example. Many researchers have suggested that smoking during pregnancy can reduce birth weight. We can test this by using some of the logic we’ve learned so far. This uses a technique called a z-test. The z-test is a way of testing the probability that we would get a sample with a certain mean from a population with a certain mean and standard deviation. Combining this with null hypothesis testing, we would assert a null hypothesis that the sample is part of the population. The alternative hypothesis is that the sample is not part of the population. If the chance of getting this sample is sufficiently low, given the null hypothesis, then we reject the null hypothesis. If I reject the null hypothesis, I conclude that the sample is not likely part of the population. This means that there is some reason that the sample that makes it different from the population, and if I set up an experiment carefully, I can decide that this reason is due to my experiment. Let’s do an example looking if maternal birth weight affects smoking. Based on data from thousands of babies, full term babies from non-smoking mothers have a birth weight of 3600 g and a standard deviation of 400 g. Assume I measured the birth weight of 25 babies whose mothers smoked during pregnancy and found their birth weight to be 3350 g. If smoking affected birth weight, then the sample of babies born to mothers who smoked would not be from the same population as babies born to non-smoking mothers. Here are the steps of the z-test: I set null and alternative hypotheses. In this case, the null hypothesis is that the sample is part of the population. This means, that the sample is part of the population, which would imply that there is no relationship between maternal smoking and birth weight, since there is no significant difference between smoking and nonsmoking mothers. The alternative hypothesis is that the sample is not part of the population. This would mean that babies born to smoking mothers are a different population than babies born to nonsmoking mothers, which suggests a relationship between smoking and birth weight. Find out the probability of getting a sample with the sample mean or more extreme, given the null hypothesis is true. To do this, we find out the z-score of the sample using the same method we learned about in Module 6. We answer the question: what is the chance we would get a sample of 25 babies whose mean weight is 3400 g, given the population mean is 3600 g and has a population standard deviation of 400g. To do this, we find the z-score using the following formula: \\[ Z = \\frac{\\bar{X} - \\mu}{\\text{SE}}\\] where \\[\\text{SE} = \\frac{s}{\\sqrt{n}}\\] In this case, we would plug in the numbers and find: \\[Z = \\frac{3400 - 3600}{\\frac{400}{\\sqrt{25}}} = 2.5\\] So we get a z-score of -2.5. Now to find out the probability of getting a z-score of -2.5 or more extreme, we would do the following in R: pnorm(-2.5) + (1 - pnorm(2.5)) Why the complicated formula? The first part of this gives us the chance of getting a sample with a z-score below -2.5. The second part gives us the chance of getting a sample with a z-score above 2.5. We want to see the chance we would get a z-score more extreme than 2.5. That could be negative or positive. This is because if the sample is not part of the population, the sample mean could be significantly higher or significantly lower. In order to avoid this complicated formula, we can just do the following, since the z-distribution is symmetrical: pnorm(-2.5) * 2 ## [1] 0.01241933 In this case, we would get a p of .012. That means, if we sampled 25 individuals from the population with a mean birth weight of 3600g and a standard deviation of 400g, the chance of getting a sample of 25 individuals with a mean birth weight of 3400g is 1.2%. That is pretty unlikely, and would be below \\(\\alpha = .05\\), so we would reject the null hypothesis. This gives us the conclusion: smoking mothers have babies with significantly lower birth weights than nonsmoking mothers. 10.3 The One-Sample T-test The z-test is a useful tool trying to determine whether a sample is part of a population. In order to do it, we must know the population mean and the population standard deviation, as well as the sample mean. In many cases, however, we do not know what the standard deviation is. Let’s take an example from animal research. I want to examine if rats can learn to press a blue lever for food and ignore a green lever, that is identical. In this experiment, I train 16 rats to press a blue lever and to ignore a green lever. In order to see if they know color and not location, I place them in a different box with levers in different places and count how many times they press the new blue lever and the new green lever. Since rats press lots of things, I use as my dependent variable the percent of times that the rat pressed the blue lever. For each rat I take the percent of time that rat pressed the blue lever. In this, I find out that the rats pressed the blue lever .58 (58%) of the time with a standard deviation of .14 (14%). If the rat is only pressing the levers randomly, then the rat should press the blue lever 50% of the time. However, if the rat can tell the difference between blue and green, the rat should press the blue lever a different value than 50% of the time. They will likely press the blue lever more often than the green lever, but there is a chance they will press the green lever more than the blue lever. I want to test whether the rats push the lever 50% of the time or if they press the lever a significantly different amount of time than 50%. In this experiment I set the null hypothesis as the rats press the lever at a level no different than expected by chance (50%) and the alternative hypothesis is that the rats press the lever at a rate different than expected by chance, different than 50%. I want to do a z-test but one thing is missing. I know the population mean, .5 (50%), but not a population standard deviation. In that case, we can use the sample standard deviation as a proxy for the population standard deviation. Using the sample standard deviation can be a good substitute, but there is a problem. Our sample standard deviation, just like the sample mean, is an estimate. The bigger our sample, the more precise the estimate. This leads to two concepts. First, sample standard deviations are imprecise, so we have to use a different distribution than the z-distribution. Second, that different distribution has to have a different shape with different sample sizes, because the accuracy of the sample standard deviation as an estimate gets bigger as the sample size increases. The new distribution we use is called the t-distribution. It is like the normal distribution and looks similar, but it has a different shape given different sample sizes. The smaller the sample size, the flatter and wider the t-distribution is. As the sample size goes up, the t-distribution gets narrower and starts to look more like the normal distribution, such that for very large samples, the t distribution becomes almost identical to the normal distribution. Notice in the figure below that the t-distribution gets closer to normal With the exception of the different distribution, the logic of a one-sample t-test is the same. It uses a very similar formula as the z-test: \\[t = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}}\\] Notice the only difference is that we swapped out the population standard deviation \\(\\sigma\\) for the sample standard deviation \\(s\\). If we go back to the example with the rats, we have a sample with a mean of .58, a standard deviation of .14, and we want to test whether that is significantly more than the mean of .5 we would expect by chance. We would do the following: \\[t = \\frac{.58 - .5}{\\frac{.14}{\\sqrt{16}}} = 2.29\\] In this case, we get a value of t= 2.29. Now we have to figure out the chance we would get a t of 2.29 or more by chance alone, as predicted by the null hypothesis. We can use the t-distribution to figure this out. However, as mentioned above, the t-distribution is different with different sample sizes. Our estimate of the population’s standard deviation is more precise with a larger sample. To examine this, we have to determine the degrees of freedom with the t-test. For a t-test, the degrees of freedom are equal to n-1. This is the value we will use for the rest of the t-test. In our example, with a sample size of 16 rats, the degrees of freedom are 15. I can do this by typing the following command into R, substituting the value I got for t for t and the degrees of freedom for df. (1 – pt(t, df))*2 This is a complicated command. If you just want the answer, feel free to skip the next couple of paragraphs, because they may be confusing. The reason this R formula is complicated is for several reasons. We want to calculate the area under the edges of the t-distribution, or the chance we get a value more extreme than 2.29. The R command pt() gives us the chance of getting a t-value of less than t. Because of that, we have to calculate 1-pt() this command because we are interested in values more than t. But this doesn’t explain why I multiply it all by 2. This is because we use a concept called two-sided hypotheses. When we set an alternative hypothesis, we have set it to say that the value is not equal to a certain value. For instance, in the rat example above, we set the alternative hypothesis that the rats did not push the lever 50% of the time. This could mean that they pressed the lever more than 50% of the time, or less than 50% of the time. In this case, we would consider our results to be significant if it is the case they push the lever more or less than 50% of the time. This is why we are interested in finding the chance we got a t of more than 2.29, or a t of less than -2.29. However, we might only be interested in setting a null hypothesis that the rats pressed the lever more than 50% of the time and unconcerned if the rats pressed the lever less than 50% of the time. This is called a one-sided hypothesis and is generally more powerful. In this case, you would use the formula 1 – pt(t, df) One sided hypotheses are not often used in psychology so I do not present them here. They make a lot of sense in some cases, but generally, we would be interested if we find results that are very large and opposite of what we expect. For instance, in the rat study, if the rats pressed the green lever more than the blue lever, that would be an interesting hypothesis. So this is why generally most people use two-sided hypotheses regardless. If this is confusing to you, just stick to the two-sided hypotheses. In this case, if we enter the following into R, we get the p value associated with our test: (1 - pt(2.29, 15))*2 ## [1] 0.03693081 This would equal .037. So we get a p-value of .037, which given it is less than .05, would lead us to reject the null hypothesis. Now, how do we report this in APA format? APA format, as you may know, is very, very picky and many times doesn’t seem to make sense. We use the following format, which is similar to the way we report correlations: t(\\(df\\)) = t-value, p = p-value (note that t and p are italicized) In this case, we would report: t(15) = 2.29, p = .037. 10.4 Confidence Intervals with One Sample The logic of t-tests can be used to assess another important question with samples. If we draw a sample with certain values, what does that tell us about the population? Or another way of putting it: how reliable is our sample as an estimate of the underlying population. Imagine I want to know the average reaction time of college students is. I conduct a study with a sample of 100 randomly-selected people and find an average reaction time of 450 milliseconds (ms), with a standard deviation of 80 ms. I want to know what this tells me what the mean reaction time of the population of all college students. I know my sample is not an exact estimate, and that the population mean is likely to be different from my estimate. But how much variability should I expect. Confidence intervals give us a way of determining this. Because of the standard deviation, I can have an idea of a range in which I would expect the population mean to be Based on what we’ve learned before, the best guess of the population mean is the sample mean. However, we know that our sample could have randomly picked people with very fast reaction times and the population mean is a lot slower than 450 ms. Or I could have picked people with very slow reaction times and my real population mean is a lot faster than 450 ms. Confidence intervals give me a range of certainty about the estimate. They have two values, a lower bound and an upper bound, that give me a range of certainty about my estimate. For instance, we typically talk about 95% confidence intervals. A 95% confidence interval is a procedure that asserts that if the true population mean is outside of our confidence interval, then the sample that caused that estimate had a 5% or less chance of happening. A confidence interval gives us certainty about how likely we are to find the population mean in other experiments. To calculate a confidence interval, I have to do the following: I have to get a sample mean and standard deviation. I have to calculate what is called a critical T-value. That is the t-value that is associated with the confidence interval size. For instance, for a 95% confidence interval, we need to figure out what value of t covers 95% of the t-distribution. This would mean, that the area of the t-distribution between –t and +t would equal .95. To figure this out, we can type the following into R: -(1*qt((1-p)/2, df)) This is another long formula, but all you have to add is the value for p, which is the size of the confidence interval, .95 for a 95% confidence interval, and the degrees of freedom of the sample. In this case, we would type the following -(1*qt((1-.95)/2, 99)) This gives us a value of 1.98. The 95% confidence interval is then solved by doing this: \\[\\bar{x} \\pm t_\\text{crit} \\mathbin{\\cdot} \\text{SE}\\] \\[450 \\pm 1.98 \\mathbin{\\cdot} \\frac{80}{\\sqrt{100}}\\] \\[ 450 \\pm 15.8 \\] Based on this, we can get a range of values where we are 95% certain by using this method, we would find the population mean. This gives us a degree of certainty we have about our estimate. For instance, we would not be very surprised if the reaction time in the population was really 440 ms, because that is in our confidence interval. However, it is very unlikely the population reaction time is really 500 ms., which is well outside our confidence interval. Based on this, we can get a range of values where we are 95% certain by using this method, we would find the population mean. This gives us a degree of certainty we have about our estimate. For instance, we would not be very surprised if the reaction time in the population was really 440 ms, because that is in our confidence interval. However, it is very unlikely the population reaction time is really 500 ms., which is well outside our confidence interval. This would equal .037. So we get a p-value of .037, which given it is less than .05, would lead us to reject the null hypothesis. 10.5 Calculating T-tests and Confidence Intervals in R R does not have a built-in function to do z-tests. This isn’t a bad idea because z-tests are rare. In fact, in my several years of doing research, I can only think of a few times where I did a z-test. Those times, I had to look up the steps and do it by hand. However, t-tests are much more common, and are very easy to do in R. For instance, we may have the following numbers representing the number of hours people slept in a class and we want to see if our class slept significantly differently than 8 hours, which might be the average: sleep = c(4,7,3,9,7,8,5,6,5,9) This enters the data into R. To do a t-test, we would use the t.test() command. This command has two parts. The first part is the sample data, given as a list of numbers. In this case, we’ll use the sleep data we just entered. The second part is the value we give for mu. Mu is the value we want to test as the population parameter (\\(\\mu\\)), which is 8 hours of sleep. This gives us the following: t.test(sleep, mu=8) ## ## One Sample t-test ## ## data: sleep ## t = -2.6128, df = 9, p-value = 0.02814 ## alternative hypothesis: true mean is not equal to 8 ## 95 percent confidence interval: ## 4.828148 7.771852 ## sample estimates: ## mean of x ## 6.3 The second line here gives us our value for t, degrees of freedom, and the p-value. Note in this example that t is negative. R gives negative values for t if the sample mean is below the population mean. However, people usually report t-values as positive and so you can change the sign since t is a symmetrical distribution. Below this is the 95% confidence interval, which ranges from 4.82 to 7.77. In the last part, it gives the mean amount of hours of sleep, 6.3 10.6 Summary After reading this chapter, you should know and/or be able to do the following: Know what a one-group mean claim is and why we would use it Know how to calculate a z-test by hand and evaluate the p-value associated with the null hypothesis by using the pnorm() command in R Know what the t-distribution is and how to conduct a one-sample t-test by hand and in R Know how to use null hypothesis testing with a one-sample t-test and how to generate null and alternative hypotheses and make the correct decision given Know how to calculate the degrees of freedom for a t-test and how to report a t-test statistic using APA formatting Know what a confidence interval is and how to calculate a one-sample confidence interval. Know how to get the critical t-value for a given confidence interval using the qt() command and calculate the confidence interval "],
["two-mean-claims.html", "Chapter 11 Two Mean Claims 11.1 What are we testing with two-mean claims? 11.2 Paired Samples T-test 11.3 Independent Samples T-test 11.4 Effect sizes 11.5 The logic of t-tests and experimental design 11.6 Doing two-sample t-tests in R 11.7 Summary", " Chapter 11 Two Mean Claims In this section, you will learn: What a two-group mean claim is and how to test it Know the difference between an independent and paired samples (dependent) t-test and when to use each of them How to calculate a paired samples (dependent) t-test and independent samples t-test by hand How to calculate each of these t-tests using R 11.1 What are we testing with two-mean claims? Many hypotheses in research use the same premise: is the mean of one group different from the mean of another group? This occurs in a diverse number of situations. For instance, you may wonder if a low-fat diet leads to more weight loss than a low-carb diet. Or you might ask whether anxious individuals’ mean level of anxiety is lower after taking a medicine than before taking a medicine. This might also involve testing whether people are more honest when they think they are being watched than when they think they are not being watched. The experimental method in science involves making a hypothesis and testing that hypothesis. In many cases we are testing whether one group is different than another group. Often this logic involves taking a sample of individuals, randomly assigning those people to two groups, and then treating those groups in different ways. The idea about this is that the two groups are equal in every way except the one factor we want to measure. A classic way of doing this is by having one group be a control group which does not get a manipulation and then having an experimental group which does receive a manipulation. For instance, one group might receive a drug whereas the other group receives a placebo that does not contain the drug. Another way of doing this is having two different groups which get two different kinds of manipulations. Here’s an example. There is a theory that the color red might make people feel more threatened because it is a “hot” color. Based on this, a scientist may develop the theory that comments written in red will seem more negative than comments written in blue. To test this idea, the experimenter takes 100 participants and then has them give a speech. After the speech, the experimenter gives each person the same feedback, with some positive attributes and some negative attribues. The only difference is that 50 people get the feedback written in red ink and 50 get the feedback written in blue ink. Then the experimenter asks the participants to rate on a 10 point scale “how negative or positive does this feedback seem”? with a 1 rating meaning very negative, 5 being neither negative or positive, and 10 indicating very positive. In this example, the experimenter wants to see if the participants rate the feedback more negatively, giving a lower rating, when it is red. The experimenter takes the average rating for the 50 participants reading the feedback in red and the average rating of the 50 participants reading the feedback in blue. The reason the researcher takes the average is that in both of the conditions there may be people who rate the feedback as very positive, just because those people are naturally biased to see anything more positively. Likewise, there are some people who rate the feedback very negatively in both conditions for the opposite reason; they see everything as very negative, regardless of color. By using the mean of 50 participants, those individual differences will cancel out since by randomly assigning people to each group, we should have a relatively equal number of positive and negative people in each group. Imagine the researcher finds out that the 50 people rating the feedback in red has a mean rating of 4 out of 10 and the 50 people rating the feedback in blue has a mean rating of 5. Does that mean the experimenter’s idea worked? You may be thinking that this might just have happened by chance. For some reason it just so happened that the people in the red group rated the feedback as more negative than the people in the blue group, not because of the color of the ink but due to random chance that people who were more likely to rate feedback as more negative happened to be assigned to the red group. Just as there is a chance I can flip a coin and get heads 20 times in a row, there’s a chance this could happen as well. However, the lower this chance is, the more likely the effect is due to my hypothesis; there is something different with reading feedback written in red versus feedback written in blue. This is the idea of testing two-group hypothses. We can apply null hypothesis testing to these questions if we think of this in terms of populations. When we select two groups and do an experiment to treat those groups differently, we are trying to create two populations. That means, when people are in group A, they should be fundamentally different than group B. The logic is that if our experiment is working, it would take people who are in one population in the beginning and then make them two different populations. In the example above, if the color of feedback affects how people respond to the feedback, people reading feedback written in red should be a different population than those who read feedback written in blue. If color does not matter, than the red group should be no different than the blue group. When I have two groups, I set up a null hypothesis that the two groups are part of the same population. This implies that the thing that I argue makes those two groups different does not matter or is so irrelevant that it would not make the two groups different populations. Conversely, the alternative hypothesis is that the two groups are not part of the same population. Now this can be for one of two reasons. It can be that the manipulation I did, such as printing the feedback in red or blue ink makes the groups different. Or it could be that something else I am not aware of, called a confound, makes the two groups different. For instance, I may have had a bad experimental design where I smiled when I gave people the feedback in blue whereas I frowned when I gave people the feedback written in red. In that case, the two groups are from a different population, but for reasons different than what I was aware of. Two-group hypotheses Hypothesis This implies: Null hypothesis The two groups are part of the same population The thing I think makes the two groups different is irrelevant or does not make the groups different. Alternative hypothesis The two groups are not part of the same population The thing I think makes the two groups different (or something else I am not aware of) makes the two groups different. There are two types of two-group hypotheses. In one case, we are measuring two groups that are independent of each other. Group A is not related to Group B. In this case, there are different people in group A versus group B. This is called an independent sample and will be the foundation of an independent samples t-test we will discuss in section C. However, it may be the case that I have the same participants and measure them twice. Imagine if I wanted to test whether a 60 minute lecture on self-esteem increases people’s performance in a class. I could measure their performance on an exam before the lecture and then an exam after the lecture. In this case, the same participants are measured twice. This is called a paired samples or dependent samples t-test and will be discussed in section B. 11.2 Paired Samples T-test I saw a study once that suggested that eating chocolate can help you lose weight. This sounded like an exciting study, but it turns out the entire study was fabricated as a way to demonstrate problems with the scientific process and science journalism. However, the idea still might be true. If we wanted to test whether eating chocolate would help you lose weight, or any other diet-related hypothesis, we might design an experiment like this: We take a group of people, measure their weight, have them eat the new diet we wanted to test for a period of time, and then measure their weight again. We could take 50 people, have them eat normally but eat a bar of chocolate each day, and weigh them again after 4 weeks. This is a kind of design called a pre-test/post-test design. It involves taking a group of people, testing them before doing an experiment, doing the experiment, and testing the people after the experiment. It is a very powerful design because we can control for many extraneous factors which might be different in each group. Since the same people are being measured twice, we can ensure that each group has the same personality and other factors which might vary if we have two groups with two different sets of people. However, many designs cannot be done using a pre-test/post-test design. For instance, it would be difficult to do the red versus blue colored feedback experiment discussed in the last section, since we are interested in feelings after getting feedback. It would not make sense to receive feedback in blue ink, rate your feelings, and then repeat the study and receive the same feedback in red ink. When doing a pre-test/post-test design, the null hypothesis is that the pre-test observations and the post-test observations are from the same population, which suggests nothing has changed during the period between the pre-test and post-test. If chocolate does nothing for weight loss, there should be no difference between pre-test and post-test designs. Some people would have an increase of weight, some would have a decrease, and some would have no change. There would be no mean change in the difference. Conversely, the alternative hypothesis is that the pre-test and post-test are from different populations, which indicates that something changed between the pre-test and post-test. The test used to examine pre-test/post-test designs is the paired samples (or dependent samples) t-test. The paired-samples t-test is a variant of the one-sample t-test we discussed in the last module. In the last module, we discussed that the one-sample t-test tests whether a sample is different from a population mean. The paired-samples t-test extends this logic by using difference scores. If we take the difference between the post-test and pre-test for each individual, we will create difference scores. In the instance of the chocolate weight loss study, we would take each individual’s post-diet weight minus their pre-diet weight. This would give us a difference score for each individual. With these difference scores, we can start to examine our hypothesis. If chocolate does not affect weight, the difference scores should have a mean close to zero. Some people’s weight might go up, some people’s weight might go down, and some people would stay the same. However, if chocolate does affect weight, the difference scores should be different from zero. If chocolate helps people lose weight, then most people would see a negative difference score (lower weight after the diet). The opposite could be true, where chocolate would actually increase weight, because it is chocolate after all. In that case, most of the difference scores would be positive with higher post-diet weight. In either case, the difference scores are different from zero. The paired-samples t-test takes difference scores and then does a one-sample t-test comparing the difference from zero. As mentioned above, the null hypothesis is that there should be no difference between the pre-test and post-test samples. This is the same thing as saying the difference scores should be zero. The paired-samples t-test essentially is dividing the mean difference by the standard error of the difference. Remember the t-test formula is as follows. \\[t = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}}\\] In the case of the paired-samples t-test, we take the difference between the post-test and the pre-test and generate the following, with the mean of the differences and the standard deviation of the differences. \\[t = \\frac{\\bar{X}_\\text{diff} - 0}{SE} \\text{ where } SE = \\frac{s_\\text{diff}}{\\sqrt{n}} \\] This gives us a t-statistic with degrees of freedom equaling n – 1, or the number of participants minus 1. If we had 50 people do the diet study, this would be 49 degrees of freedom, even though we have 100 observations (50 observations before the diet and 50 observations after the diet). Imagine I did this study where I had 12 people do the diet study discussed above and got the results listed in the table below. Before Diet After Diet Difference 191 183 -8 188 175 -13 200 204 4 180 176 -4 185 184 -1 174 170 -4 155 136 -19 164 159 -5 128 126 -2 225 215 -10 211 213 2 233 218 -15 Mean -6.25 Standard Deviation 6.95603 Notice in this example, I take the weight after the diet minus the weight before the diet, and then take the mean and the standard deviation of those difference scores. Then I take those numbers and do a one-sample t-test using those difference scores. \\[t = \\frac{6.25 - 0}{\\frac{6.95}{\\sqrt{12}}} = -3.125 \\] A couple of notes here: If the post-test is smaller than the pre-test, t will be negative. However, convention usually dictates that t-values are reported as positive, and since the t-distribution is symmetrical this does not change our results. When finding the p-value in R, the value for t must be positive. Second, there are 12 pairs of individuals, so we have 11 degrees of freedom. Using the command in the last chapter, we can find out the p-value of getting a t-value of 3.125 or more extreme by the following: (1 - pt(3.125, 11))*2 ## [1] 0.009663434 This \\(p=.009\\), which indicates there is a .009 chance (.9%) chance that these results would have occurred if the null hypothesis is true, that is if there is no difference between pre-test and post-test. Thus, we would reject the null hypothesis. 11.3 Independent Samples T-test If we have two groups that are independent, that is, the two groups are not the same individuals, we can do an independent samples t-test. The formula for this test is the same t-test formula, but it has a few important tweaks. We’ll use an example from the psychology of numbers for this study. A researcher may have the idea that people think underestimate how big something is if the numbers presented use fewer digits. For instance, $4 million may seem like less money than $4,000,000. The researcher believes that because people often see large money values like $2.4 trillion as written as numbers with words rather than as only digits (e.g. $2,400,000,000,000), they may not appreciate how big it is. The researcher has participants read a news article about Social Security and how the Social Security program will be facing a $1 billion dollar shortfall. One group of participants reads the number as $1 billion whereas the other group reads the number as $1,000,000,000. After reading the article, participants rate on a 7 point scale how much of a problem they think the Social Security shortfall is. The researcher predicts the participants will think it is a bigger problem in the condition where the number is presented as only digits. In this case, there are two groups of participants and each participant is only in one group. This is called a crossed design, because a given participant is only in one group. To analyze the differences between the groups, we would do an independent samples t-test. Before we get started, we’ll use a few definitions. The groups are arbitrarily assigned. One group is called group 1 and the other is called group 2. Each group has a mean and a standard deviation. The null hypothesis is that the two groups come from the same population and means that the mean of group 1 is equal to the mean of group 2. The alternative is that the two groups are from two different populations and that the mean of group 1 is different from the mean of group 2. Again, since groups are assigned arbitrarily, it will not matter if it is a negative t-statistic or a positive t-statistic. Finally, the two groups do not need an equal number of people, as we’ll see below. The basic t-test formula for independent samples t-tests is this: \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{SE}\\] Generally, standard error is the standard deviation divided by the square root of n. However, in an independent-samples t-test, we have two standard deviations, one for each group. If I had my experiment above, I would have the standard deviation for the group that read the story with the number expressed as words versus the group that read the story with the number expressed as digits. To calculate the standard error for the t-test, I have to somehow combine the two standard deviations into a “pooled” standard deviation. Standard deviations cannot be averaged together, so I use a more complex formula for this. \\[s_p = \\sqrt{\\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}}\\] This formula creates the pooled standard deviation. It is long because it weights each standard deviation based on the number of people in each sample. Since each group may have a different number of observations, we have to multiply the standard deviation of each group separately. Combining this to the other formula, we get this formula for an independent samples t-test. \\[t = \\frac{\\bar {X}_1 - \\bar{X}_2}{s_p \\cdot \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\] These formulas are complicated but if you have the right numbers, calculating a t-test by hand is simple, just plugging the correct numbers in the correct spots. Here is how we do it: Take each group, label one group “Group 1” and one group “Group 2”. Whichever one you pick as group 1 or 2 is arbitrary, but I generally assign the group I expect to have a larger mean as group 1 and the group I expect to have a smaller mean as group 2. Then set up the null and alternative hypotheses. Find the following for each group: n (number of samples), , (that sample mean) and s (that sample standard deviation. In the formulas, which group each sample reflects is indicated with a subscript. Plug the sample standard deviations and n into the pooled standard deviation formula to get the pooled standard deviation (sx1x2) Plug the rest of the numbers into the equation to get t. Find the degrees of freedom, which is n1 + n2 – 2 (total number of observations in both groups minus 2) Use the following command in R to get the p value, given you have the t-value and degrees of freedom: (1-pt(t, df))*2 I’ll work through an example here. Imagine I wanted to see whether students in a traditional seated class or in an online class learned more material. I give both groups of students a final exam and use their scores on this exam for my dependent variable. I will give you the mean and standard deviations for this example: Seated class Online class Sample mean 82 75 Sample SD 9 13 n 21 33 Let’s go through the steps: Assign groups and set up null and alternative hypotheses. I will assign seated class to group 1 and online class as group 2. The null hypothesis is that the two groups are from the same population implying there are no differences between the two groups. The alternative hypothesis is that the two groups are from different populations, implying the two groups are from the same population. Calculate means, SDs, and n for each group. Done. If you need more help with this, consult the module on summary statistics. Find the pooled SD. Here is how that may work: \\[s_p = \\sqrt{\\frac{(21-1)9^2+(33-1)13^2}{21+33-2}}\\] \\[s_p = \\sqrt{\\frac{20 \\cdot 81 +32 \\cdot 169}{52}} = 11.6\\] Step 4: plugging this in the t formula: \\[t = \\frac{82-75}{11.6 \\cdot \\sqrt{\\frac{1}{21}+\\frac{1}{33}}} = 2.23\\] Step 5: Find degrees of freedom. These are equal to the total number of observations minus 2. In this case it is 52. Step 6, find out the P value associated with T. To calculate this, we would type the following into R: (1-pt(2.23, 52))*2 ## [1] 0.03008958 This gives us a \\(p = .030\\). This is below the generally accepted \\(\\alpha = .05\\), so we would reject the null hypothesis and conclude the groups are different. Given that the mean for the seated class is higher than the mean for the online class, we would conclude that the seated class performed significantly better on the exam. Whether that was due to the seated versus online nature of the classes or due to another confound is a question for research design. 11.4 Effect sizes When we investigate hypotheses using null hypotheses testing, we are examining how likely our data are given the null hypothesis is true. When used in a two-sample hypothesis, this is generally saying whether the two groups are the same or different. Generally this is because we want to say whether the characteristic that makes the two groups different, or the independent variable, causes a change in the dependent variable. This characteristic might be two different types of diet, or two different types of classes, or examining differences between girls and boys. This approach tells us how sure we are that there is a difference between the group but it says nothing about an important question: whether there is a small difference between the groups or a large difference between the groups. Here’s an example: research shows that a diet where a person reduces their sodium intake may reduce one’s blood pressure. In this case, the null hypothesis is that those who have the low-sodium diet are from the same population as the group who does not have a low-sodium diet. The alternative hypothesis is that the two groups are not from the same population, which suggests that one group has a lower blood pressure than the other group. When I use null hypothesis testing, I will either reject or retain the null hypothesis, which helps me to answer the question of: does having a low-sodium diet help me to reduce blood pressure? However, this ignores a related question which may be of interest: how much does a low sodium diet reduce blood pressure? This is the idea of effect size, as we discussed in Module 7. Effect sizes in two group hypotheses can be represented by the difference between the groups. One way of measuring effect size is by using a concept called Cohen’s D. Conceptually, Cohen’s D is a measure of the difference between two groups in terms of standard deviations. A Cohen’s D of 1 means that the two groups are one standard deviation apart. To calculate this, you can use the following formula, assuming you have t, based on Rosenthal &amp; Rosnow (1984): \\[d = \\frac{2t}{\\sqrt{df}}\\] \\[d = \\frac{2 \\cdot 2.23}{\\sqrt{52}} = .52\\] Cohen suggested the following criteria to evaluate D: Effect size Type of effect d &lt; .3 Small d &gt; .3 and d &lt; .7 Medium d &gt; .7 Large What this means is that if the difference between two groups is less than .3 standard deviations, the effect was small and generally not interesting. If the effect is between .3 and .7 standard deviations, the effect is a medium effect, which may be interesting 11.5 The logic of t-tests and experimental design One important element of designing experiments is designing our experiments so that they work. If we are testing whether there is a difference between two groups, we want to do our best to find a difference if a difference exists between the two groups. Using a t-test, there are three important elements you can do to make sure your experiment has the best chance of working. This is a concept called statistical power or the ability to find an effect if one is present. We want to make sure our designs are as powerful as possible. To have more powerful designs, we want to make t-statistics bigger, because the bigger the t-statistic, the smaller the p-value and the more likely we reject the null hypothesis. How do we make the t-statistic bigger? As noted in the section above, the t-test formula is: \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{SE}\\] We can describe this another way: \\[t = \\frac{\\text{Effect}}{\\text{Error}}\\] The effect is the difference between the two groups. We’ve measured this in terms of effect size. If we want to find an effect, we want to make the effect as strong as possible. This means making the difference between the two groups as clear as possible. From a research point of view, we want to make the characteristic that divides the two groups as obvious as possible. The second characterisic is error. To make t bigger, we want to make the error smaller. Error is measured by \\(\\frac{s}{\\sqrt{n}}\\) and there are two ways to make this smaller. First, we can reduce \\(s\\) or the standard deviation. This is done by making our experiment as controlled as possible. The last way is by increasing \\(n\\). Bigger \\(n\\) leads to bigger t-statistics and make it more likely that an effect is significant. What does this mean to you? When designing an experiment, do the following: 1. Make your effect as big as possible. Make it as clear as possible what you are testing. For instance, if you are testing the effect of red ink versus blue ink, make sure the participants see a lot of red ink and a lot of blue ink. Sometimes a manipulation has to be subtle so that participants don’t catch on to the experiment, but making manipulations as obvious as possible helps make experiments more likely to work 2. Reduce error. Try to make sure you reduce as many sources of error as you can. 3. Use large sample sizes. Try to use as many people as you can in order to test an experiment. Generally, you should have at least 30 participants in a study, unless you have a strong reason not to. 11.6 Doing two-sample t-tests in R T-tests in R are very simple and use the same t.test() command we used for one-sample t-tests. There are two ways of doing this, depending on how you set up your data. Imagine I have a dataframe that looks like the following: GroupA GroupB 1 3 3 4 5 8 6 2 2 13 3 15 I could put this into R using the following notation: GroupA = c(1,3,5,6,2,3) GroupB = c(3,4,8,2,13,15) d = data.frame(GroupA,GroupB) To do a t-test comparing the groups, I would use the t-test command. If my data are set up where I am comparing two different columns, I would do the following commands. However, I have to tell R if it is a paired-samples or independent samples t-test. In the commands below, assume I have two vectors or columns, one named A and one named B. For the paired samples t-test: t.test(A, B, paired = T) For the independent samples t-test: t.test(A, B, paired = F) In the example given here, if I wanted to do a paired samples t-test, I would type: t.test(d$GroupA, d$GroupB, paired = T) And if I wanted to do an independent samples t-test, I would type: t.test(d$GroupA, d$GroupB, paired = F) If I do this, (for the paired samples t-test), I will get the following output: ## ## Paired t-test ## ## data: d$GroupA and d$GroupB ## t = -1.652, df = 5, p-value = 0.1594 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.64999 2.31666 ## sample estimates: ## mean of the differences ## -4.166667 11.7 Summary After doing this chapter, you should be able to know and/or do the following: Know what two-sample mean claims are and when to do them. Also know what the difference is between a paired and independent-samples t-test. Know how to apply null hypothesis testing to two-sample mean claims Know how to calculate a two-sample t-test by hand, whether it is a paired or independent-samples test. Know some ways to increase t-statistics in order to make experiments more powerful Know how to calculate two-sample t-tests in R. "],
["claims-with-more-than-two-means-anova.html", "Chapter 12 Claims With More Than Two Means: ANOVA 12.1 How to do an ANOVA 12.2 Calculating the rest 12.3 Doing an example 12.4 ANOVA in R 12.5 Summary", " Chapter 12 Claims With More Than Two Means: ANOVA The t-test is a powerful way of testing whether two groups are different. However, one limitation of this test is we might have experiments or data where we have more than two groups. For instance, I knew some researchers who wanted to see whether firstborn children prefer romantic partners with different birth order personalities. They set up their data where firstborn children rated how romantically interested they would be in a person who had a firstborn personality, a middle child personality, a youngest child personality, or an only child personality. So they had four groups of participants, one who saw each of the personalities. They wanted to test the hypothesis that firstborn children would prefer firstborn personalities differently than they preferred middle child, youngest child, and only child personalities. If they wanted to use t-tests to do this test, they would have to do many t-tests, one where they compared each group to every other group. They would compare the group who read first-born personalities to the group who read middle child to the group who read last child, and the middle child to the last child, and so on. That’s a lot of comparisons. If I want to test every group against every other group, I would have 6 t-tests. One problem we discussed with any statistical test is the possibility of a Type-1 error, or a false positive, where we falsely conclude there is a difference between the two groups when in fact there are not. We set the alpha value to correct for that. For instance, if we set \\(\\alpha = .05\\), we are setting a 5% chance of a Type-1 error. However, if we do multiple tests, the chance of making an error increases. This is a concept called family-wise error. Family-wise error is the idea that if we test several different statistical analyses, the chance we have a false finding increases. If you do n t-tests, the chance of a family-wise error is equal to \\(1-\\alpha^n\\). In this case, it equals: \\[\\mathit{FWE} = 1- .05^6 = .265\\] There is over a 25% chance that we would find one group being different from another group, even if there are no differences between all the groups. One way we can correct for family-wise error is by changing alpha. If we wanted to make sure we only have a 5% chance of making an error and want to do 6 t-tests, we can lower alpha accordingly so that the overall error rate is .05. If we want to set the chance of one false-positive at a probability of \\(\\alpha\\) with n tests, we can get a new value for alpha we will call p and use that as our new alpha value by using the formula: \\[p = 1- \\sqrt[n]{(1 - \\alpha)}\\]. So if we wanted to make sure we have an overall error rate of .05, we would do: \\[p = 1-\\sqrt[6]{1-.05} = .008\\] In this case, we would set an overall alpha of .008 as our cutoff value as to whether to reject or retain the null hypothesis. This approach is very conservative (increasing the chance of a miss or type-2 error) and there are other alternatives to set up the p-value that are not as conservative. We can correct for this by doing what is called an omnibus test or a test where we examine all the possible hypotheses at once in one test. The most common of this is a test called the ANalysis Of VAriance, or ANOVA. The ANOVA is a test where we test whether there are any differences between groups. In ANOVA, the null hypothesis testing with n groups is \\(\\bar{x}_1 = \\bar{x}_2 = \\bar{x}_3 = \\ldots\\ = \\bar{x}_n\\). Essentially, this states that all of the sample means come from the same population. Since the alternative hypothesis is the inverse of the null hypothesis, that means the alternative hypothesis is at least one sample mean is from a different population. So if there are 3 groups, the null hypothesis would be \\(\\bar{x}_1 = \\bar{x}_2 = \\bar{x}_3\\). The alternative hypothesis is that at least one group is different. That could mean \\(\\bar{x}_1 \\neq \\bar{x}_2 = \\bar{x}_3\\) or \\(\\bar{x}_1 = \\bar{x}_2 \\neq \\bar{x}_3\\) or \\(\\bar{x}_1 \\neq \\bar{x}_2 = \\bar{x}_3\\) or \\(\\bar{x}_1 \\neq \\bar{x}_2 \\neq \\bar{x}_3\\) or any other combination in which at least one mean is not equal to another mean. So if an ANOVA is significant, all we can conclude is that at least one mean is not different from the other means. This is a limitation we will have to keep in mind. ANOVA stands for ANalysis Of VAriance. The idea of ANOVA is a comparison of two kinds of variance, the within-group variance and the between-group variance. Within-group variance is conceptually a measure of how much each group varies from its mean. Between-group variance is a conceptual measure of how much each mean varies from the overall mean, called the grand mean. The idea is if the between-group variance is higher than the within-group variance, this implies that at least one of the group means is different from the rest. Imagine the following examples. In the first example, you have three groups all drawn from the same population, which would be the case if the null hypothesis is true. Notice that each group has its own mean and variance. The means are not exactly the same, but the difference between the group means, marked by vertical lines, is very close relative to the variance within each group. In the second example, the groups are from different populations with different means. Notice the groups are much more spread out and the group means are spread further, relative to the variance within the groups. In this example, it’s clear at least one, if not more, of the group means are different from one another. ANOVA captures this by measuring a quantity called the F-ratio which is a statistic like the t-value or z-value you have calculated already. The F-ratio is a measure of the between-groups variance divided by the within-groups variance. If \\(F = 1\\), the between-groups variance is equal to the within-groups variance, and this indicates the groups are very likely from the same population. However, as the between-groups variance increases relative to the within-groups variance, F will increase. If there is twice as much between-groups variance than within-groups variance, \\(F = 2\\), and so on. As we will discuss below, we will use \\(F\\) to find a p-value that allows us to test the probability we would get the data we got if the null hypothesis is true. Like with T-tests, if that \\(p &lt; \\alpha\\), we would reject the null hypothesis. If not, we retain the null hypothesis. 12.1 How to do an ANOVA As we mentioned in an earlier chapter, one way to measure variance is the concept of sums of squares, which is defined for each observation \\(x_{i}\\) in \\(1 \\ldots\\ n\\) as \\(\\mathit{SS} = \\sum (x_{i} - \\bar{x})^2\\). Essentially, this is the each observation minus the group mean squared, summed up. When we have data in one group, sums of squares is a simple concept. However, in ANOVA, we have two or more groups which may or may not be from the same population. So we can take this idea of sums of squares and create three different types of sums of squares. Before talking about this, there are some terms and concepts to describe. When doing an ANOVA, the data consist of \\(n\\) observations are divided in \\(k\\) groups. The mean of each group are called the group means and is referred to as: \\(\\bar{x}_{k}\\). As mentioned above, the mean of all the data is called the grand mean and is referred to as: \\(\\bar{x}_{g}\\). The sums of squares correspond to each of these groups. The sums of squares within is a measure of how much each observation deviates from its own group mean. The sums of squares between is a measure of how much each group mean deviates from the grand mean. Sums of squares total are a measure of how much each individual observation deviates from the grand mean. Because of this, sums of squares total equals sums of squares between plus sums of squares within. 12.1.1 Calculating Sums of Squares Between Sums of squares between can be calculated by using the following formula. \\[\\sum n_k(\\bar{x}_{k} - \\bar{x}_{g} )^2\\]. For each group, we take that group’s mean minus the grand mean, and then square that number. Then we multiply it by the number of observations in the group. We do this for each group and then we add those numbers up. 12.1.2 Calculating Sums of Squares Within Sums of squares within can be calculated by the following formula. \\[\\sum(x_{i} - \\bar{x}_{k} )^2\\] Essentially, we are taking the each observation minus its group mean, squaring that number, and then adding all of the resulting numbers up. 12.1.3 Calculating Sums of Squares Total Sums of squares total can be calculated by the following formula, which is very similar to sums of squares within \\[\\sum(x_{i} - \\bar{x}_{g} )^2\\] Essentially, we are taking the each observation minus the grand mean, squaring that number, and then adding all of the resulting numbers up. Remember that sums of squares total equals sums of squares between plus sums of squares within. Because of that, we can check to make sure we calculated this correctly. Type of Sums of Squares Symbol Formula Sums of Squares Between \\(\\mathit{SS}_{B}\\) \\(\\sum n_k(\\bar{x}_{k} - \\bar{x}_{g} )^2\\) Sums of Squares Within \\(\\mathit{SS}_{W}\\) \\(\\sum(x_{i} - \\bar{x}_{k} )^2\\) Sums of Squares Total \\(\\mathit{SS}_{T}\\) \\(\\sum(x_{i} - \\bar{x}_{g} )^2\\) 12.2 Calculating the rest Sums of squares is only the beginning. What we want to find is a ratio that compares between-group variance to within-group variance. Since the number of observations in a group is important, we need to get the mean of the sums of squares. This quantity is called mean squares and is found by taking sums of squares and dividing by degrees of freedom. Degrees of freedom is a bit complicated in ANOVA. The overall degrees of freedom are the same as in a t-test, namely \\(\\mathit{df}_{T} = n-1\\). However, these degrees of freedom can be split into degrees of freedom between and degrees of freedom within. Degrees of freedom between are calculated by \\(\\mathit{df}_{B} = k-1\\) which is the number of groups minus 1. Degrees of freedom within are calculated by \\(\\mathit{df}_{W} = n-k\\), which is the number of observations minus the number of groups. These two quantities add up to the total degrees of freedom: \\(\\mathit{df}_{B} + \\mathit{df}_{W} = \\mathit{df}_{T}\\). With sums of squares, we can calculate mean squares, which are simply \\(\\frac{\\mathit{SS}}{\\mathit{df}}\\). We only calculate mean squares between as \\(\\frac{\\mathit{SS}_{B}}{\\mathit{df}_{B}}\\) and mean squares within as \\(\\frac{\\mathit{SS}_{W}}{\\mathit{df}_{W}}\\). The final step is calculating the F-ratio, which we will discuss later. This is calculated as \\(\\frac{\\mathit{MS}_{B}}{\\mathit{MS}_{W}}\\). We use the F-ratio to find the p-value that will test the probability that we would get the data we got if the null hypothesis is true. 12.3 Doing an example Imagine we have the following situation. I want to test whether the color words are printed in affects how well they are remembered. Each group received a list of 15 words to remember and I analyzed how many correct words they wrote down in a recall task where they wrote down which words they remembered. The only difference between the groups was that one group received a list of words written in red, another group received a list written in black, and a third group received a list written in blue. These are the data: Participant Group Score 1 red 9 2 red 10 3 red 12 4 red 11 5 red 13 6 blue 7 7 blue 9 8 blue 6 9 blue 8 10 blue 5 11 black 4 12 black 8 13 black 6 14 black 5 15 black 7 mean red 11 mean blue 7 mean black 6 mean grand 8 12.3.1 The null and alternative hypothesis The first step is to set up the null and alternative hypothesis. The null hypothesis is that all the group means are from the same population. This would imply that color has no effect on what is remembered. The alternative hypothesis is that at least one group mean is from a different population. When doing an experiment, we may have 12.3.2 Calculating Sums of Squares To calculate sums of squares between, we take each group mean minus the grand mean, squaring the result, and then multiplying by the number in the group. See the formula below: \\[ 5*(11-8)^2 + 5*(7-8)^2 + 5*(6-8)^2 = 70 \\]. To calculate sums of squares within, we would take each number minus its group mean, square it, and then sum up all the results. This would look like the following table, where I calculate the score minus the group mean in the fourth column and then that number squared in the fifth column. To calculate the sums of squares within, we add up the last column, getting \\(\\mathit{SS}_{t} = 30\\).. Participant Group Score \\((x_{i} - \\bar{x}_{k} )\\) \\(x_{i} - \\bar{x}_{k} )^2\\) 1 red 9 -2 4 2 red 10 -1 1 3 red 12 1 1 4 red 11 0 0 5 red 13 2 4 6 blue 7 0 0 7 blue 9 2 4 8 blue 6 -1 1 9 blue 8 1 1 10 blue 5 -2 4 11 black 4 -2 4 12 black 8 2 4 13 black 6 0 0 14 black 5 -1 1 15 black 7 1 1 To calculate sums of squares total, we would use the same procedure for sums of squares within, but instead of taking each score minus the group mean, we do each score minus the grand mean. See the table below: Participant Group Score \\((x_{i} - \\bar{x}_{g} )\\) \\(x_{i} - \\bar{x}_{g} )^2\\) 1 red 9 1 1 2 red 10 2 4 3 red 12 4 16 4 red 11 3 9 5 red 13 5 25 6 blue 7 -1 1 7 blue 9 1 1 8 blue 6 -2 4 9 blue 8 0 0 10 blue 5 -3 9 11 black 4 -4 16 12 black 8 0 0 13 black 6 2 4 14 black 5 -3 9 15 black 7 -1 1 If we add up the last column, we get \\(\\mathit{SS}_{T} = 100\\). Since \\(\\mathit{SS}_{B} + \\mathit{SS}_{W} = \\mathit{SS}_{T}\\), we can check our work. Indeed we find \\(70 + 30 = 100\\). 12.3.3 Calculating Degrees of Freedom and Mean Squares The second step is degrees of freedom. In this experiment, we have 15 observations and 3 groups. We calculate overall degrees of freedom as \\(\\mathit{df}_{T} = n-1 = 15-1 = 14\\). Degrees of freedom between are calculated by \\(\\mathit{df}_{B} = k-1 = 3-1 = 2\\) Degrees of freedom within are calculated by \\(\\mathit{df}_{W} = n-k = 15 - 3 =12\\), which is the number of observations minus the number of groups. At this point, we are getting a lot of numbers. It’s useful to build up an ANOVA table to report all these numbers and to keep us sane. We would have the following numbers: Variance SS df Between 70 2 Within 30 12 Total 100 14 As mentioned above, mean squares are \\(\\frac{\\mathit{SS}}{\\mathit{df}}\\). We can calculate these in our table and expand it. Notice, I only do this for between and within group variance, since it is meaningless for total variance. Group SS df MS Between 70 2 35 Within 30 12 2.5 Total 100 14 The last step is calculating F, which is \\(\\frac{\\mathit{MS}_{B}}{\\mathit{MS}_{W}}\\). I add F to the table on the top row. Group SS df MS F Between 70 2 35 14 Within 30 12 2.5 Total 100 14 In this case, we get an \\(F = 14\\), which indicates we have conceptually 14 times as much between-group variance as within-group variance. This is quite high, so we should expect to reject the null hypothesis. To find out the probability we would get an F-value of F or greater, if our null hypothesis is true, we can use the following R command. In this case, we have to give the command the F value we received marked as F, and the two different types of degrees of freedom, first the between-group degrees of freedom marked as dfB and the within-groups degrees of freedom marked as dfW. 1- pf(F,dfB,dfW) In our case, we would type: 1-pf(14,2,12) ## [1] 0.000729 As noted above, this gives us a value of \\(F = .00073\\). This is much less than the traditionally assigned alpha value of \\(\\alpha = .05\\) and we would reject the null hypothesis. 12.4 ANOVA in R Conducting ANOVAs in R is very easy, as long as you have your data set up correctly. This is one case where it is very important to have data imported in “tidy” format. As noted in the chapter on R, tidy format means that each column contains a variable and each row contains an individual with related observations. When doing an ANOVA, we need two variables. The first is the independent variable and that tells us which group a subject is in. The second variable is the dependent variable and this tells us what the value is for each person. For instance, if we take our table from the example above, the column labeled “Group” is the independent variable and the column labeled “Score” is the dependent variable. Participant Group Score 1 red 9 2 red 10 3 red 12 4 red 11 5 red 13 6 blue 7 7 blue 9 8 blue 6 9 blue 8 10 blue 5 11 black 4 12 black 8 13 black 6 14 black 5 15 black 7 To do an ANOVA in R, we have to first enter the data into R. Below is how to do this by hand, but as you can see, it may be easier to do this using a spreadsheet program and import the data into R. Group = c(&quot;red&quot;,&quot;red&quot;,&quot;red&quot;,&quot;red&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;black&quot;,&quot;black&quot;,&quot;black&quot;,&quot;black&quot;,&quot;black&quot;) Score = c(9,10,12,11,13,7,9,6,8,5,4,8,6,5,7) d = data.frame(Group,Score) This imports the data and puts it into a dataframe named “d”. You can view this dataframe by typing View(d). Now to do the ANOVA, we have to use the aov() command, which stands for Analysis Of Variance. To do this, we need 3 parts: The dependent variable The independent variable The data frame The dependent variable and the independent variable are separated with a tilde ~. Now we type the command. In this case, we need two commands. the first one does the ANOVA and saves it as an object named x and then the second command does the summary() command using the object named x. x = aov(Score~Group, data = d) summary(x) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Group 2 70 35.0 14 0.000729 *** ## Residuals 12 30 2.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This gives us the between degrees of freedom and sums of squares in the row listed with the name of the independent variable. In this case that row is named “Group”. Then the row below gives us the within degrees of freedom, sums of squares, and mean squares. The last two columns are the F-value and the p-value associated with the F value. In this case, we get the same p-value as before. 12.5 Summary After reading this chapter, you should know or be able to do the following: Understand why multiple comparisons leads to increased type 1 error and what we can do about this Know what is being compared in ANOVA, specifically within-group and between-group variance through the F-ratio Know how to conduct an ANOVA by hand and in R, calculating the sums of squares, degrees of freedom, mean squares, F-ratio, and p-value. Know how to interpret an ANOVA, including what the null and alternative hypothesis is as well as how to know whether to reject or retain the null hypothesis. "]
]
