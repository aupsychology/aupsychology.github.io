[
["index.html", "Workbook for Statistics 2 Preface", " Workbook for Statistics 2 Robert G. Franklin Jr. 2017-09-05 Preface This is the second workbook for Statistics: The Story of Numbers and contains exercises for the second set the chapters listed there (chapters 13 onward, which are not written yet). "],
["interactive-assignment-1-introduction-to-r.html", "1 Interactive Assignment 1: Introduction to R 1.1 Doing basic math in R 1.2 Objects 1.3 Creating data frames 1.4 Part 2: inputting data from other sources. 1.5 Terms and concepts to know:", " 1 Interactive Assignment 1: Introduction to R The objectives of this lab is to learn the basics about using R. You will learn the following in this lab: How to enter data into R by hand How to do basic math functions How to enter data into variables and data frames How to do basic statistics on data frames, such as mean, median, mode How to import data into R from csv files This covers the learning objectives in Part 1: A, Understand the R language and how to use R to conduct basic math and statistical functions. This lab is a review as an introduction to R. For more review, please read Chapter 4 of Statistics: The Story of Data. Before starting this lab, you need to install R and RStudio. R is a command-line language, so all your commands are entered as text on the command line, called the “console”. Try typing the following commands. 1 3+4 x If you type the things above, you should get output for the first two and an error for the third one. The error is because we haven’t defined x or told R what x is. When you see output, you may see a number in brackets, like if you type the output below: 3*5 ## [1] 15 For now, ignore the number in brackets. This is a way of telling which element is output. The number in brackets helps us if we have a long list of numbers, like if I type the following which lists the numbers 1 through 100. 1:100 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 The number in brackets tells me which value in the list is to the right of the number in brackets. So if I see, [20], I know that the number to the right is the 20th item in the list. 1.1 Doing basic math in R You can use R like a big calculator, using math notation. To add, use the plus key. To subtract, use the minus key. To multiply, use the asterisk key, and to divide, use the forward slash key. Exponents are used by the caret key. See below: 3+7 4-11 4*21 18/3 2^2 Now in the space below, write how you would do the following: 11 times 22 32 divided by 3 11 minus 4 Finally, R follows the basic order of operations, with information in parentheses done first, then exponents, then multiplication and division, followed last by addition and subtraction. So the following are different: 3 + 11*5 ## [1] 58 (3+11)*5 ## [1] 70 1.2 Objects The power of R though is that you can do a lot more than that. You can use things called objects which are, containers for data. An object can be anything from a single number to thousands and thousands of rows of data. Depending on what an object is holding, it can have different types. The simplest type of object is a variable. It is simply a letter or word that stands for a number. To make a variable you use the following (replacing the bracketed number with an actual number (like 42). To assign a value to an object, we simply use equals, with the object on the left and the value to assign on the right. x = 1 When we assign something to an object, we don’t get an output, since there is nothing for R to say. To see what an object says, we simply type the object. x ## [1] 1 The first type of object is called numeric and contains any number. Objects can also be strings, which are lists of text. They are entered by putting quotation marks around the data. Try typing the following. number = 4 name = &quot;Michael&quot; day.of.week = &quot;Tuesday&quot; question20 = 5 A third type of object is called a “logical” argument, and it has two choices: TRUE or FALSE (written in all caps). To shorten this, you can just use the letters T or F. Try the following: x = TRUE y = F We can also do math on objects, just like numbers. x = 3 y = 7 x*y ## [1] 21 Do the following in R and write your commands below: Make an object that is named name as your first name and have that object be a string containing your first name. Make a second object that is called age and have it equal your age. Create a third object called neighborage and have that equal the age of the person sitting beside you. If you are alone, just make up a number. Add the objects age and neighborage together. What would you type? The power in R is that objects can have multiple values, called lists. To make a list, we use the concantenate function c(). Type the following into R. x = c(1,3,5,7) ages = c(19,22,37,41,18) IQ = c(97, 132, 88, 101, 94, 107) presidents = c(&quot;George&quot;,&quot;John&quot;,&quot;Thomas&quot;) With lists, we can do functions which need more than one number, such as taking the sum of a list, the mean, and so forth. Try the functions below, which give the mean, the sum, and the standard deviation: mean(IQ) sum(x) sd(ages) 1.3 Creating data frames Lists are nice, but sometimes we want to hold a lot of organized data together, like a table. In R, these are called dataframes, and they have variables as separate columns with each row being an observation. Person Name Age Worth 1 Alice 45 $5,000,000 2 Bob 39 $432,000 3 Cara 29 $87,000 4 David 66 $11,000 5 Ethan 54 $33,000 Each row refers to a set of observations, which is a single person. Each column is a different set of data, or a variable. Now we are going to make a dataframe. For example, I may want to make a dataframe with how many hours they slept and their number of items they got correct two different 25 question memory tasks. Name Sex Sleep Memory Test 1 Memory Test 2 Zoe Female 9 22 19 Yannick Male 10 17 20 Xavier Male 6 15 23 Wendy Female 5 13 18 Vance Male 5 10 8 To do this, I have to make a list for each column and then combine them using the data.frame() command into the dataframe called “results”. To do this, type the following (note I keep all the variable names in lower case; this is my convention but make sure you are consistent with capitalization): name = c(&quot;Zoe&quot;, &quot;Yannick&quot;, &quot;Xavier&quot;, &quot;Wendy&quot;, &quot;Vance&quot;) sex = c(&quot;Female&quot;,&quot;Male&quot;,&quot;Male&quot;,&quot;Female&quot;,&quot;Male&quot;) sleep = c(9,10,6,5,5) memory1 = c(22,17,15,13,10) memory2 = c(19,20,23,18,8) results = data.frame(name,sex,sleep,memory1,memory2) To see the dataframe, I would just type the dataframe’s name, by using the View() command, which has a capital V View(results) The View() command opens up a new window in the top right which lets me view the dataframe. If we want to look at a single column, we would select it by first listing the data frame name, then a dollar sign, then the column name. To select the column for names, I would type: results$names ## NULL We can do math to each of the columns. For instance, if I want to create a new column which is the sum of each of the three tests, I would type: results$memory.total = results$memory1 + results$memory2 Now I have created a new column called memory.total. Notice that I used a period in the middle of the column name to separate the words. This is something you can do to avoid using spaces. However, the column name shouldn’t start or end with a period. What would you type to select the memory.total column: How would you construct a column called memory.mean, which is the mean memory score for the two memory tests. Remember, you could just create this column by dividing the memory.total column by 2. 1.4 Part 2: inputting data from other sources. For the second part of this assignment, we’ll talk about how to input data into R using other programs. As mentioned in the reading, it’s hard to put data into R manually, like what you just did with grades. So what we’ll do is input data that has already been entered using another program. In this case, I have already entered the data into a spreadsheet and saved it using the name “lab1.csv”. So please download this file off of Canvas and save it to your computer, preferably in the downloads folder. Whenever I give you data to enter into R, I will also give you a key that tells you about each of the variables. When you make your own data (for a research methods project) it is a good idea to make your own key for when you share it with other people. Here is the key below. Variables: Person: person’s name Education: highest level of education completed: none, HS for high school, college for college, and graduate for graduate work Sex: F for female, M for male Test: type of test: Math or English Score: score on a 50 question test You can read this data into RStudio by going to the “Import Dataset” under the environment tab. Do this, loading the data into R with the variable name x. I like short variable names because they save typing In the R terminal, type: x = read.csv(file.choose()) This command should pop open a window that you can use to select the data you downloaded. Find the lab1.csv file you downloaded (probably in your downloads folder) and select it. This saves the data into R as a variable named x. To see what class of data it is, type class(x) What class is x? To view the data in the RStudio viewer, type View(x) or you can click on X in the environment pane in R-studio (top right pane). Note that View has a capital V. On the top right, you should be able to see as summary of your workspace. This will tell you how many observations and how many variables are in your dataframe x. How many observations are in x? How many variables? You can also find out how many rows and columns are in x by using the nrow(x) and ncol(x) command. Another command to find out the names of the columns is the colnames(x) command. What are the column names in dataframe x? An interesting thing about colnames is that you can use it to change the column names as well. Let’s say we want to change the name of the Person to subject. Type the following to do this: colnames(x)[1] = &quot;Subject&quot; This command changes the column name of the column specified with the brackets. If you would want to change the name of the column called “Person” to “Name”, how would you do that? Now we will review how to do summary statistics in R. The basic way to do this is by using the commands mean(), median(), and sd() for mean, median, and standard deviation. We have to give each of these commands a list of numbers. To calculate the mean scores, we would type: mean(x$Score) What is the mean score? What is the median score? Often, when we are interested in data, we have to calculate what are called aggregated statistics. These are stats which are grouped by a different variable. One way to do this is to use the describeBy() function in the psych package. To install the psych package, type: install.packages(&#39;psych&#39;) Once you’ve installed a package, you have to load a package by typing: library(psych) Each time you start R again, you have to reload a package. You only have to install the package once, unless you have to reinstall R, for instance on a new computer. Now that we’ve installed the psych package, we can use the describeBy() function. This function has two inputs, the first input is the variable you want to summarize and the second input is the variable that tells which group you want to use to group the data. For instance, if I want to see score broken up by gender, I would type: describeBy(x$Score, x$Sex) How would you get the average score divided up by person, so that you get Alex’s overall average, Bob’s overall average, etc. How would I get the average score for each type of education? Sometimes I want to break up data by more than one variable. I can enter in multiple grouping variables by nesting them in the list() function, like this: describeBy(x$Score, list(x$Test, x$Sex)) Remember this because it may be useful for other data analysis. 1.5 Terms and concepts to know: After completing this lab, you should know what the following terms are: Objects Object classes: including numeric and string Lists Data frames, including observations and variables *Packages You should also be able to do the following skills. It is okay if you don’t remember how to do them without looking them up. Understanding how to enter data into R using the c() command Understanding how vectors and data-frames work and how to select data from these Understanding what functions are in R, what arguments are, and how to use them. Understanding different data types in R Understanding what packages are, how to install and load them, and how to use them Knowing how to enter data into R from csv files and from Google sheets using the googlesheets package *Knowing how to do basic summary statistics in R using the following: mean, median, mode, etc functions, using the describe function in the psych package "],
["interactive-assignment-2-reviewing-inferential-statistics.html", "2 Interactive Assignment 2: Reviewing Inferential Statistics 2.1 Summary Statistics in R 2.2 Examining Gender Differences Using T-tests 2.3 Correlation with R", " 2 Interactive Assignment 2: Reviewing Inferential Statistics In this lab, we will review how to do t-tests and correlations in R. This lab assumes that you have covered the theory behind these tests in a previous class. After completing this lab, you should be able to: Import data into R from Google Sheets using the gsheet package Know how to conduct a t-test in R and interpret the output Know how to conduct a correlation in R and interpret the output. In this lab, we will use some data that some students collected for a research project previously. In this project, the researchers gave students a series of questionnaires looking how personality traits affected a participant’s likelihood to conform. First we are going to import the data. In this case, the data were entered on Google Sheets. To import data straight from Google Sheets, we can use the gsheet package. To install this package, we would type: install.packages(&#39;gsheet&#39;) And to load the package, we would type: library(&#39;gsheet&#39;) Now to load the package, we have to use this link. You can paste in this link and then type the command below. link = &#39;https://docs.google.com/spreadsheets/d/1zZjEhkT4ROACTM4YsW1LTNn8al_scyu9l4_Qkzqvsew/edit?usp=sharing&#39; d = gsheet2tbl(link) Whenever you share data, you should always include a guide that tells people what each variable is and what the variables represent. In this case, the variables are named as follows: Participant: a participant number E: extraversion score A: agreeableness score C: conscientiousness score N: Neuroticism score O: Openness score Gender: gender coded F and M Age: participant age Conformity: their score on a survey about how much they would conform RATING: a participant’s rating to the question “how much do I think I conform to others’ behaviors” SelfEsteem: a score on a self-esteem inventory with higher scores indicating more self-esteem 2.1 Summary Statistics in R We are going to use the describe() and describeBy functions in the psych package to first look at some summary statistics. You can see more about these functions in the last assignment. First, we’ll load the psych package. library(&#39;psych&#39;) In the psych package, you can use the describe function to get a summary about a variable, with several summary statistics. For instance, to look at a summary of the extraversion variable, we would type: describe(d$E) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 60 25.67 7.16 26 25.67 9.64 10 40 30 -0.01 -0.94 ## se ## X1 0.92 Notice that this tells us the mean for extraversion is 25.67 with a standard deviation of 7.16. It also gives us the median, minimum, maximum, and other statistics. Use the describe function to find the mean and standard deviation of the other 4 personality tests (agreeableness, conscientiousness, neuroticism, and openness, along with the Conformity variable). Create a table below with the means and standard deviations of each of these variables. 2.2 Examining Gender Differences Using T-tests Now we are going to see if there are any differences between male and female participants in their scores on the personality inventories and in conformity. To conduct t-tests, we can use the t-test() function in R. There are two ways to do t-tests in R. The first way is to give R two different variables as two different arguments. For instance, if we wanted to compare to see whether extraversion is higher than neuroticism, we would type the below code. t.test(d$N, d$E) However, comparing how high extraversion is to neuroticism doesn’t make sense. We want to compare a single personality trait with another variable that tells us which gender a person is. To do that, we have to enter data different way. In R, we use what is called a formula notation to describe when we want to input one variable by another variable. What this means is that we have a dependent variable or an outcome variable, and then we have one or more variables that tell us what groups or how to divide the dependent variable. This function has one input, but it has two variables divided with a tilde. The first variable is the dependent variable and the other is the the grouping variable that tells what category an observation is in. The independent variable must have exactly 2 levels, because a t-test only compares 2 groups. This makes the Gender variable a good candidate. To do a t-test for the Extraversion variable with Gender as an independent variable we would type: t.test(d$E~d$Gender) ## ## Welch Two Sample t-test ## ## data: d$E by d$Gender ## t = 0.20057, df = 25.11, p-value = 0.8426 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.740950 4.548403 ## sample estimates: ## mean in group F mean in group M ## 25.76087 25.35714 This is the output you should get with this test. The second line of the output is the one you are most interested in. This tells us the t value, the degrees of freedom (abbreviated df), and the p value. The degrees of freedom is a corrected value, so it is often a decimal. The 95% confidence interval is on the 5th line, and the last line has the mean of the variable for each group. Notice in this case the mean for group F, for female, is very close to the value for group M, for male, which is why this t-test is not significant. Using this function, do a t-test examining whether men and women vary in the other 4 personality variables and whether they vary in conformity. Report your t-value and p-value for each of the tests below, using APA formatting: t(df) = tvalue, p = pvalue, or t(25) = .20, p = .84. Remember that gender should be the independent variable here. 2.3 Correlation with R The researchers were interested in whether there was a correlation between the personality variables and conformity. To test whether a correlation is present, we can use the cor.test() function in R. This function requires two input variables, which will be correlated. To see the correlation between Extraversion and Conformity, we would type: cor.test(d$E, d$Conformity) ## ## Pearson&#39;s product-moment correlation ## ## data: d$E and d$Conformity ## t = 1.2445, df = 58, p-value = 0.2183 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.09660536 0.39886592 ## sample estimates: ## cor ## 0.1612755 The last number gives us the correlation. The significance of the correlation is listed as a p-value in the second row, along with the degrees of freedom. In this case, we have a small and insignificant correlation between Extraversion and Conformity, r(58) = .16, p = .21. Using the correlation function, examine if there are any correlations between the other 4 personality variables and conformity. Report your r-value and p-value for each of the tests below, using APA formatting. Using the cor.test() function, examine if there are any correlations between SelfEsteem and conformity? Is the Conformity variable correlated with the RATING variable? How so? "],
["free-assignment-1-reviewing-r.html", "3 Free Assignment 1: Reviewing R 3.1 Basic descriptive statistics", " 3 Free Assignment 1: Reviewing R Free assignments are assignments where I give you a data source and ask a series of questions about it with minimal guidance. You should use what you’ve learned in the interactive assignments to complete these assignments. In this assignment, we will review the principles in Interactive Assignment 1 and Interactive Assignment 2. You can feel free to look back in those assignments to figure out how to answer these questions. For this assignment, we are going to look at data from a study examining how mentalizing, or trying to figure out what another person is thinking, affects face memory. The idea of this study is that if you mentalize more about a person, then you will remember their face better. One way to study memory is by giving people a list of items to remember (in this case faces), and then testing them later by showing them the faces they saw before (the previously seen faces) along with new faces they have not seen before. Then what we do is ask people to indicate whether the face is a face they have seen before or a new face that they have never seen before. During this testing period, people have to make one of four choices. They have to say they are very confident they have seen the face before, somewhat confident they have seen the face before, somewhat confident that the face is new and they haven’t seen it before, and very confident that the face is new and they haven’t seen it before. To test our hypothesis, we had participants see a face presented with an adjective and the people had to answer a question of how much they thought that adjective fit the person pictured. For instance,the adjective might be “excited” and they would answer how much they thought “excited” fit the person pictured. Afterward, the participants had to complete a memory test where the people they saw were mixed with people they had never seen before. The idea was that when a person was paired with a complex mental state adjective, that required more mentalizing, the person was more likely to be remembered than when they were presented with a basic mental state adjective which required less mentalizing. The data for this study are available as the FA1memdata.csv file. The first step is to import that data using the read.csv() command. Import the data to the data frame named “d” Once you import it, you should see the dataframe in your environment. It has 81 observations of 14 variables. This dataframe has several variables. You can see the variable names by clicking the data frame in the “Environment” window in the top right. Here is a guide to what the variables mean. Subject: Subject number Age: Participant Age Sex: Participant Sex novelACC: accuracy for novel items (faces in the memory test which were not seen before) seenACC: accuracy for seen items (faces in the memory test which were previously seen) overallACC: accuracy for all faces. ComplexHCH and BasicHCH: High confidence hits for faces shown with complex adjectives and basic adjectives respectively. A high confidence hit is when a person sees a face they have seen before and answers that they are very confident they saw that face before. ComplexLCH and BasicLCH: Low confidence hits for faces shown with complex adjectives and basic adjectives respectively. A low confidence hit is when a person sees a face that they have already seen and says they are only somewhat sure they have seen that face before. ComplexLCM and BasicLCM: Low confidence misses for faces shown with complex adjectives and basic adjectives respectively: A low confidence miss is when a person sees a face they have previously seen before and says that they are somewhat confident that they did not see that face previously. ComplexHCM and BasicHCM: High confidence misses for faces shown with complex adjectives and basic adjectives respectively. A high confidence miss is when a person sees a face they have previously seen before and says they are very confident they did not see that person before. Our theory is that memory for faces presented with complex adjectives will be better than memory for faces presented with basic adjectives. Therefore we expect the following: Complex HCH will be higher than Basic HCH - since high confidence hits is the strongest form of memory. Complex LCH may be higher than Basic LCH - low confidence hits is a weaker form of memory, so this might be the case Basic LCM will be higher than Complex LCM - misses reflect faces that were forgotten, so we expect fewer faces are forgotten when paired with complex adjectives Basic HCM will be highter than Complex HCM - high confidence misses are rare, but they should be more likely when a face is paired with a basic adjective. 3.1 Basic descriptive statistics Using the psych package, create basic descriptive statistics to create a table to answer the following question Mean and standard deviation for all variables (except Subject and Sex) Use the describeBy function to evalulate the means and standard deviations for the novelACC, seenACC, and overallACC variable for males and females separately, using Sex as a grouping variable. Create a table to report your answers below: Using a t-test, see whether there are any differences between males and females in their scores in the novelACC, seenACC, and overallACC variable. Report the t-values using APA formatting below: Now we are going to test the main hypotheses of interest. Examine whether there is a significant difference between ComplexHCH and BasicHCH. This will be a paired-samples t-test, so you will type the following into R: t.test(d$ComplexHCH, d$BasicHCH, paired = TRUE) Is there a significant difference? Which variable has a higher mean? Do the results support the researchers’ hypothesis? Now do the following for the LCH, LCM, and HCM variables. Are there any significant differences between the basic and complex adjectives for each of these situations? Report the t-values and p-values below When looking at all the results, do these results support the theory that memory is stronger for faces presented with complex adjectives than for faces presented with basic mental state adjectives? Why or why not? You should use your answers to the questions above in order to answer these questions. "],
["interactive-assignment-3-the-tidyverse.html", "4 Interactive Assignment 3: The tidyverse", " 4 Interactive Assignment 3: The tidyverse This assignment explores the use of the “tidyverse”, or a set of R packages designed to import, format, and analyze data in “tidy” formats. In this lab, we will: How to use filter() to select part of your data How to arrange and select data using the arrange() and select() funtions How to use mutate() to make new columns in a dataframe. To start with the tidyverse, you need to install the package ‘tidyverse’, which is a collection of useful packages used to analyze data. Do the following to install it: install.packages(&#39;tidyverse&#39;) Then load the tidyverse package with the following: library(&#39;tidyverse&#39;) For this assignment, you are going to work through Chapter 5.1-5.5 of R for Data Science. It is available at this link: http://r4ds.had.co.nz/transform.html Read the chapter at the attached link and then answer the questions that are below when you get to them. Place your answers on this sheet by writing down the code or commands you would do in order to solve these problems. If you’re ambitious, you can work through sections 5.6 and 5.7 on your own. Find all flights that: Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United, American, or Delta Departed in summer (July, August, and September) Arrived more than two hours late, but didn’t leave late Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges? How many flights have a missing dep_time? What other variables are missing? What might these rows represent? Section 2: arrange How could you use arrange() to sort all missing values to the start? (Hint: use is.na()). Sort flights to find the most delayed flights. Find the flights that left earliest. Section 3: select Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. What happens if you include the name of a variable multiple times in a select() call? What does the one_of() function do? Why might it be helpful in conjunction with this vector? vars &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;dep_delay&quot;, &quot;arr_delay&quot;) Section 4: Mutate Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? "],
["interactive-assignment-4-graphs-with-ggplot.html", "5 Interactive Assignment 4: Graphs with ggplot 5.1 Using ggplot to create histograms", " 5 Interactive Assignment 4: Graphs with ggplot After completing this lab, you should be able to do the following: Know how to use the ggplot function to create plots in R Know how to use ggplot to create a scatter plot and to fit a line of best fit to the scatter plot Know how to use functions in ggplot to change the appearance of a plot Know how to add functions to plots to change the labels on the plots This assignment will discuss some of the basics about creating a plot with ggplot. GGplot is very powerful but it can be very complicated. Before doing this assignment, you should read through Chapter 3 of R for Data Science. The first thing we have to do is to install the ggplot package and to load it. The ggplot package is part of the tidyverse, so we can load it by using the same command as in Interactive Assignment 3. library(&quot;tidyverse&quot;) The data for these assignments are available as a download on Canvas as the file IA4graph.csv (Files are also available at github ) These data are a series of face ratings and were used in several studies looking at how older adult (older than 65 years) and younger adult (18-30) raters perceive older and younger adult faces. The raters rated the faces on 6 characteristics: aggressiveness, attractiveness, babyfaceness, competence, health, and untrustworthiness. The ratings were on a 7 point scale with 1 indicating very low and 7 indicating very high. Each row represnts a different face. The variables (columns) are as follows: FaceSex - the sex of each face rated (F for female, M for male) FaceAgeGroup - the age group of each face rated (OA for older adult faces, YA for younger adult faces) FaceAgeNumber - the numeric age of the individual rated ag.old and ag.young - ratings of aggressiveness by older adult and younger adult raters respectively at.old and at.young - ratings of attractiveness by older adult and younger adult raters respectively ba.old and ba.young - ratings of babyfaceness by older adult and younger adult raters respectively co.old and co.young - ratings of competence by older adult and younger adult raters respectively he.old and he.young - ratings of health by older adult and younger adult raters respectively un.old and un.young - ratings of untrustworthiness by older adult and younger adult raters respectively Load the file into the dataset face by typing the following command and selecting the .csv file when the file select dialog appears face = read.csv(file.choose()) If you look in the environment window on the top left part of the screen, you should see that the data.frame “face” is loaded. Let’s look at the data by typing View(face) In this study, the authors were interested in testing whether older adult raters would agree with younger adult raters when rating a face. To test this question, the authors would look at whether there was a correlation between older adult ratings for a trait and younger adult ratings of a trait. To calculate a correlation test, we can use the cor.test() function like mentioned in Interactive Assignment 2. Before doing this, we would like to look at the descriptives of our data in order to see whether there are any outliers and what the means are for each rating. To do this, we can load the psych library and use the describe command. Load the psych library and use the describe command to calculate the mean and the standard deviation of the ag.old, ag.young, at.old, at.young, ba.old, ba.young, co.old, co.young, he.old, he.young, un.old, and un.young variables. Use this to fill out the following table: Rating Mean SD ag.old ag.young at.old at.young ba.old ba.young co.old co.young he.old he.young un.old un.young Based on these means, what pattern do you see? Which traits do older adults rate more highly on average? Which traits do younger adults rate more highly, on average? Many researchers have speculated older adults show a positivity bias where they tend to rate events as being more positive than younger adults. Given that these were the same faces rated by older adults and younger adults, do you think there’s evidence of the positivity bias here? 5.1 Using ggplot to create histograms Now we are going to use ggplot to visualize our data. The first thing we are going to do is to describe the parts of ggplot. When generating each plot, the plot command has three parts: The ggplot() command, which tells us our data frame and the data we are going to plot Geoms, which are elements that we plot on the main plot. A geom might be a line graph, a bar graph, or any other graphic element we put on the plot Aesthetics which are elements used to modify the plot. These are extra commands which can modify the plot or geoms (like saying how thick a line is) GGplot uses several different functions which are all technically one command. To make it easier to read, many people put the different commands on different lines and separate each line with a + (plus sign) at the end of each line. When a line ends with a +, R knows that the command is not complete and that there are more functions to add. Note that when we type the commands for ggplot, we end each line with a +, except for the last line. The first thing we are going to do is to build a histogram of each rating. We will start with ag.old to create a simple histogram. Type the command below. ggplot(data = face) + geom_histogram(aes(x = ag.old)) This is one command split across two lines. In the first line, I indicated the data by using the ggplot() command. The first argument is the data frame, the data = face. The second part is the geom we want, which is the type of graph we have. In ggplot, we can put many geoms on the same plot, like layers on top of one another. However, in this case, we just have one layer, which is a histogram. In geom_histogram(), I want to tell ggplot which data to graph. To do this, I have to use an aesthetic. This is why I type aes(x = ag.old). When you type this command, you should see a plot appear in the bottom left window in RStudio, under the “Plots” tab. When calculating a histogram, sometimes we want to set how many bars or bins we have in the histogram. GGplot sets 30 as a default. We can change that by adding an option to the geom_histogram() part. Try typing the following: ggplot(data = face) + geom_histogram(aes(x = ag.old), bins = 10) You might also want to edit the graph as well. We can feed some options to geom_histogram() in order to edit the graph: ggplot(data = face) + geom_histogram(aes(x = ag.old), bins = 10, color = &#39;red&#39;, fill = &#39;darkblue&#39;) Here I used the color argument to set the color of the lines and the fill argument to set the color of the bars. There are a lot of colors built into R and to see all your options, you can type colors(). Using and modifying what is written above, make a histogram with 15 bars with each bar having a black outline and a gray fill. (You can pick what shade of gray you’d like). It should look like this. Note below what code you would use to write this: Using what is written above, make histograms for all the other variables (ag.young, at.old, at.young, ba.old, ba.young, co.old, co.young, he.old, he.young, un.old, and un.young). Do they look like they are normally distributed? Do any seem to have any strange patterns or outliers? 5.1.1 Creating scatterplots using ggplot A histogram graphs the distribution of one variable whereas a scatterplot graphs the relationship of two variables. This allows us to look at whether two variables are associated or correlated and whether there are any outliers or other anomalies in our data. In this study, the researchers were examining whether the ratings by older adults and the ratings by younger adults would be correlated, indicating that they agreed on which faces were high in aggressiveness, competence, and so forth. To create a scatterplot, we will use the geom_point() option, but most of the rest of the commands will be similar. The only other difference is that I have to give the geom_point() argument two variables, rather than one. ggplot(data = face) + geom_point(aes(x = ag.old, y = ag.young)) This plot has all the information we need, but it looks very simple. To make the plot look nicer, we can add options to the geom_point() function. Here are some options you can use to make it look nicer: size: sets the default size of the points (default is 1) shape: a number which sets the shape of the points (you can find which points are associated with which shapes here) *color: sets the color of the outside of the point Here, I use each of the options to change the plot: ggplot(data = face) + geom_point(aes(x = ag.old, y = ag.young), size = 3, shape = 20, color = &#39;red&#39;) Using what is written above, make a scatterplot looking at the association between he.old and he.young. Try out using a different color and a different type of point. Does the relationship look like the ratings are correlated? 5.1.2 Editing other plot elements In ggplot, we can also edit the other elements of a plot which are on the outside using different options. We can use the labs() option to set the labels of the plot and change the ones that ggplot gives us by default. For instance, we may want to change the labels of the plot above ggplot(data = face) + geom_point(aes(x = ag.old, y = ag.young), size = 5, shape = 20) + labs(title=&#39;Aggressiveness Ratings&#39;, x = &quot;Older Adult Ratings&quot;, y = &quot;Younger Adult Ratings&quot;) Finally, we can use the theme() function to change a lot of other options in our plot. In this example below, I’ll change the size and the type of the text: ggplot(data = face) + geom_point(aes(x = ag.old, y = ag.young), size = 3, shape = 20) + labs(title=&#39;Aggressiveness Ratings&#39;, x = &quot;Older Adult Ratings&quot;, y = &quot;Younger Adult Ratings&quot;) + theme(text=element_text(size=16, family=&quot;Arial&quot;)) As you can see in the examples, you can keep adding options to the plot to change different elements to make the plot look nicer. There are options to change each of the elements, such as changing the size of the x axis font, the y axis font, the x label font, the y label font, etc. Using what is written above, make a scatterplot looking at the association between co.old and co.young. Change the title and the labels for the plot, x-axis, and the y-axis to something else and use the theme() option to change the size of the font to something different. Write or paste your code below: 5.1.3 Outputting your plot When you create a plot in ggplot, you may also want to take that plot and use it in a paper or presentation. To do this, there are several ways to get the plot you should see. In the “Plots” window in RStudio, there is a button which says “Export”. If you click this, you have the option to save the current plot as an image or as a pdf file. I usually save plots as an image and then insert them into a paper or presentation. You can also copy the plot to your clipboard and then paste it into another program. "],
["free-assignment-2-using-the-tidyverse-and-graphs-with-ggplot.html", "6 Free Assignment 2: Using the Tidyverse and graphs with ggplot", " 6 Free Assignment 2: Using the Tidyverse and graphs with ggplot This assignment will build on a lot of the work in Interactive Assignments 3 and 4. We will use the same data that we used in Interactive Assignment 4 where we examined how older and younger adults rated the facial appearance of older and younger faces. In this assignment, we will need the tidyverse and ggplot libraries. Fortunately, ggplot is part of the tidyverse so all we have to load is the tidyverse package. Step 1: Go ahead and load the tidyverse package. Now we should load the data that was used in Interactive Assignment 4. Step 2: Go ahead and load that file as the data.frame face, just like you did for Interactive Assignment 4. As a reminder from Interactive Assignment 4, these data are a series of face ratings and were used in several studies looking at how older adult (older than 65 years) and younger adult (18-30) raters perceive older and younger adult faces. The variables are as follows: *FaceSex - the sex of each face rated (F for female, M for male) *FaceAgeGroup - the age group of each face rated (OA for older adult faces, YA for younger adult faces) *FaceAgeNumber - the numeric age of the individual rated *ag.old and ag.young - ratings of aggressiveness by older adult and younger adult raters respectively at.old and at.young - ratings of attractiveness by older adult and younger adult raters respectively ba.old and ba.young - ratings of babyfaceness by older adult and younger adult raters respectively co.old and co.young - ratings of competence by older adult and younger adult raters respectively he.old and he.young - ratings of health by older adult and younger adult raters respectively *un.old and un.young - ratings of untrustworthiness by older adult and younger adult raters respectively In this assignment, we are going to look at differences between older faces and younger faces. The first step is to create two new data frames. One data frame will be for older faces and one data frame will be for younger faces. As we discussed in Interactive Assignment 3, you can use the filter() command to create a data frame that contains only rows that fit a certain condition. Step 3: Use the filter() command to create a new data frame called “face.of” which only contains older faces, or those faces where the variable FaceAgeGroup is equal to “OA”. Step 4: Use the filter() command to create a new data frame called “face.ya” which only contains younger faces, or faces where the variable FaceAgeGroup is equal to “YA”. Using ggplot, create a histogram for the aggressiveness rating variables for older faces using the variables ag.old and ag.young for the dataframe face.of. Make your histogram have 10 bins, have bars that have black lines and dark red fill. Your histogram for ag.old should look like this: Question 1: What pattern do you see with the two histograms? Do the ratings appear to be normally distributed? Step 5 Now create two histograms for the aggressiveness variables for the younger faces. The only difference is that instead of using the data frame “face.of”, you should use the data frame “face.yf”. Question 2: What pattern do you see with the two histograms of younger adult faces? Do the ratings appear to be similar to the ratings for older adult faces? Now we are going to do a scatterplot, looking at the correlation between aggressiveness ratings by older raters and aggressiveness ratings by younger raters. We want to see whether the correlation is stronger for older faces or is it stronger for younger faces. That is, do older and younger adults agree more when rating how aggressive older faces are or do they agree more when rating how aggressive younger faces are? Step 6 Create a scatterplot looking at the relationship between ag.old and ag.young for older faces. When making your scatterplot, make sure that you have the following features: your plot title should say “Older Faces” so that we know that this plot is for older faces only your x axis label and y axis label should say “Older Adult Raters” and “Younger Adult Raters” on the correct axis, depending on which one you plot on the x axis and which one you plot on the y axis. You can change the other elements such as the type of points and colors to anything you want. Your plot should look like the plot below: Step 7 Create the same plot you did in Step 6 only instead using younger faces. Make sure you change the title, but everything else can be the same. Question 3: Compare the plot for older faces and younger faces. Does it look like the correlation is stronger for older faces or younger faces? Does it look like it is the same? One concept we will talk about later in this class is the idea of a moderator. A moderator is a third variable that affects or moderates the relationship between two variables. In this case, the age of the face may be a moderator, since the relationship between the two variables ag.old and ag.young is different for different values of the moderator. Step 8 Use the cor.test() function to see whether the correlation between ag.old and ag.young is different for the older faces versus the younger faces. Question 4: What are the correlations? Do the results support your answer for Question 3? Question 5: Repeat steps 5 - 8 for the competence ratings (the variables co.old and co.young), producing a histogram, a scatterplot, and the correlations for the competence ratings. What patterns do you see and what do you conclude? Are the results similar or different from the results for aggressiveness? "],
["interactive-assignment-5-regression-in-r.html", "7 Interactive Assignment 5 - Regression in R 7.1 Simple Regression 7.2 Adding a control variable: Multiple Regression 7.3 Plotting a regression line using ggplot 7.4 Summary", " 7 Interactive Assignment 5 - Regression in R This lab will examine how to do regression in R. In this case, we will look at how to enter a basic linear regresison in R, how to interpret the output, and how to add other control variables. If you need a primer on how to do regression, you can find one here in the regression chapter of Statistics: The Story of Numbers. This lab will cover the following topics: What is regression and what is it used for? Also, you should know what the predictor and outcome variables are. What is the idea of a control variable and why we would use it. How to interpret regressions, by creating a regression line and using that equation to predict a value for the outcome variable, given the equation. The data for this assignment are a set of data examining how various factors predict a person’s wage. These data are from 1985, from the Economics Web Institute surveying people on various attributes including how much they make. Many researchers have found convincing evidence that women make less money than men do. However, it is much more controversial why this is the case. It could be that men are more likely than women to work higher-paying jobs. It could be that men are more likely to have other factors which increase pay, such as higher education or experience. It could be that men are paid more due to discrimination. In this dataset, we are going to explore how regression can help us answer the question. The variables are as follows: ID: person ID WAGE: wage (dollars per hour) OCCUPATION: occupation(1=Management, 2=Sales, 3=Clerical , 4=Service, 5=Professional, 6=Other) SECTOR: sector of employment(0= other, 1=Manufacturing, 2=Construction) UNION: Union membership (1=yes, 0=no) EDUCATION: Years of education (12 = high school diploma, 16= completed college, etc.) EXPERIENCE: Years of work experience AGE: Age in years SEX: Sex (0 – male, 1 – female) MARR: Married (0 – no, 1 – yes) RACE: Race (0 – other, 1 – white, 2 – Hispanic) SOUTH:Southern region (1 – yes, 0 – no) The dataset is saved as the file “IA5WageData.csv” and is available on Canvas. Download this file into a folder on your computer named “IA5” and then open up a new R Script. This will contain the commands you enter in while you analyze these data. Save this script in the same folder as your csv file and then change your working directory to this folder by going to Session -&gt; Set Working Directory. When you are done with this assignment, you need to send me this script as part of the assignment. Step 1: Load this file into your R workspace as the data frame “income”. The first line of your script should look like this: income = read.csv(&quot;IA5WageData.csv&quot;) Also, load the tidyverse package because we will use it later in the lab: library(tidyverse) Make sure that the file read into R correctly. If you type View(income) you should get a data frame with 533 observations of 12 variables. The variables are the same ones listed above. The first thing we want to do is to make a histogram of our dependent variable. Since we are intereseted in money, WAGE is the dependent variable. Step 2: Make a histogram of WAGE, using the ggplot code we leanred about in Interactive Assignment 4. ggplot(data = income) + geom_histogram(aes(x = WAGE), bins = 15, color = &#39;black&#39;, fill = &#39;gray&#39;) Does WAGE look normally distributed? Why do you think that it has the type of shape that it has? (Even if it’s not, we are going to go ahead and continue with the lab as if it is). 7.1 Simple Regression Now let’s look at a simple regression. We want to see how education predicts wages. To summarize, what we are doing here is creating a regression object in R, named x. This object contains all the regression information. Then we want a basic summary of this object, so we type the summary(x) command. This will give us the regression output. Type in the following: x = lm(WAGE~EDUCATION, data = income) summary(x) ## ## Call: ## lm(formula = WAGE ~ EDUCATION, data = income) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.8183 -3.2039 -0.7039 2.3050 16.3139 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.68919 0.99244 -0.694 0.488 ## EDUCATION 0.74109 0.07475 9.914 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.513 on 531 degrees of freedom ## Multiple R-squared: 0.1562, Adjusted R-squared: 0.1546 ## F-statistic: 98.29 on 1 and 531 DF, p-value: &lt; 2.2e-16 Once you type the summary(x) part, you will see the regression output. The regression output has a lot of sections. A. Residuals. This is a summary of the residuals of the regression. Residuals are the error in the regression equation, or how far the line of best fit is from the actual data. This gives you the quartiles for residuals, or the minimum, first quartile, median, third quartile, and maximum. B. Coefficients: these tell us the regression equation for the intercept and the predictor variable. They have four columns. The first column, titled Estimate, gives us the values for the intercept (which would be alpha or a in our equation) and the b coefficient for each predictor. Remember, a linear regression equation is as follows: \\[y = a + bx\\] So in this case, the regression equation would be: \\[y = -.69 + .74x\\] The b coefficient is important because it tells us how much we would expect \\(y\\) to increase if \\(x\\) increases by 1. For each additional year of education, a person would expect to make an additional 74 cents (in 1985 dollars). The last two columns in the Coefficient section tell us whether the predictors are significant by giving a t-value and a p-value. The first row tells us whether the intercept is significantly different from zero. Since the p-value is &gt; .05, this is not significant. However, the second row tells us whether the b coefficient for education is significantly different from zero. If it is, then that indicates that education is a significant predictor of wages. Given that the t-value is very high and the p-value is very, very low, we can conclude that education is a significant predictor of wages. We will talk about the other parts of the output in another lab. Step 3: Now run a regression with SEX as a predictor of WAGE. You should get an output like this: ## ## Call: ## lm(formula = WAGE ~ SEX, data = income) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.995 -3.495 -1.059 2.475 17.251 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.9949 0.2812 35.541 &lt; 2e-16 *** ## SEX -2.2661 0.4156 -5.452 7.63e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.781 on 531 degrees of freedom ## Multiple R-squared: 0.05301, Adjusted R-squared: 0.05123 ## F-statistic: 29.73 on 1 and 531 DF, p-value: 7.633e-08 In this case, SEX is a dichotomous variable, with only two options: 0 for male and 1 for female. Even though this is the case, we can still do a regression and create a regression equation. Write the regression equation you get for this regression. After you write it, solve the equation letting x = 0. This gives us the income that we would predict if a person is a male. Then solve the equation letting x = 1. This gives us the income we would predict if a person is a female. What are the answers you get? Do these results suggest that we have a wage gap in earnings? Now we are going to do a little bit of a detour. If you remember, one way we compare whether two groups are different is by doing a t-test. Step 4 Do a t-test comparing whether there is a significant difference in WAGE comparing women versus men, using SEX as the grouping variable. How does the t-test compare to the results for the regression, comparing the t-value and p-value you get for the SEX variable in the regression to the t-value and the p-value you get for the t-test? Also how does the mean for group 0 and group 1 compare to the answers you have in Question 2, when you computed the expected income for men and for women? 7.2 Adding a control variable: Multiple Regression One of the reasons that regression is more powerful than t-tests is the ability to use a control variable. We can use more than one predictor variable to predict our outcome variable. In this case, we can see whether the effects of the independent variable on the dependent variable are still true, even controlling for another variable. This is an extension of regression called multiple regression. Instead of having one predictor like \\(y = a + bx\\), we can extend our equation with multiple \\(b\\) coefficients: \\[y = a + b_1 x_1 + b_2 x_2 ... b_n x_n\\] Each \\(b\\) coefficient represents a different variable we use as a predictor. Control variables are very important because we often want to make sure a relationship between a predictor and outcome variable is not due to a third variable. For instance, the relationship between violent media such as televsion and video games and how violently a child behaves may be due to other variables, such as parenting styles or socioeconomic status. We may want to control for those variables so we can say that watching violent television predicts violent behavior, even when you control for parental neglect. In the wage study, one reason that men may make more money than women is that men may be more experienced. There are several reasons why men may be more experienced than women. One notable reason is that women are much more likely than men to leave work to raise children. If a woman takes off 3-5 years in order to raise children, then she would be missing out on that level of experience. If this effect is big enough, women on average would have less experience than men have, and since experience is clearly a predictor of income, this could explain the wage gap. If this is the case, then controlling for experience would make the relationship between SEX and WAGE smaller. We can do that by adding this variable to our regression. Adding multiple predictor or independent variables to a regression in R is easy: we list each of the variables and separate them with a plus sign. Step 5: Type the following code to do the multiple regression. x = lm(WAGE~SEX+EXPERIENCE, data = income) summary(x) ## ## Call: ## lm(formula = WAGE ~ SEX + EXPERIENCE, data = income) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3542 -3.5117 -0.9593 2.5514 17.9614 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.12849 0.39737 22.972 &lt; 2e-16 *** ## SEX -2.36524 0.41367 -5.718 1.8e-08 *** ## EXPERIENCE 0.05107 0.01668 3.062 0.00231 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.743 on 530 degrees of freedom ## Multiple R-squared: 0.06948, Adjusted R-squared: 0.06597 ## F-statistic: 19.79 on 2 and 530 DF, p-value: 5.158e-09 When we look at this, we can see that there are now two variables listed below the intercept in Coefficients. The first is SEX and the second is EXPERIENCE. When doing multiple regression, we are creating an equation like this: \\[y = a + b_\\text{sex} x_\\text{sex} + b_\\text{experience} x_\\text{experience}\\] So when we input the values for \\(a\\) and \\(b\\), we get: \\[y = 9.12 -2.36 x_\\text{sex} + .051 x_\\text{experience}\\] Based on this equation, we could predict the expected income given a person’s sex and experience. Using this equation, what would you predict a person to make if he was a male with 9 years experience? What about a female with 12 years experience? The important thing we may be interested in is comparing the equation where sex was the only predictor to the equation where SEX and EXPERIENCE are both predictors. If EXPERIENCE explains some of the wage gap, then SEX should be a weaker predictor when EXPERIENCE is included. When we compare the b coefficients, we find that the b coefficient for SEX is almost the same in both cases, which indicates that EXPERIENCE is probably not a good explanation for the sex difference in wages. In larger studies which have investigated the wage gap, the effect of experience does have a small impact in why women get paid less than men, but the effect does not explain the wage gap, just like in this dataset. Step 6: Try another possible control variable which might explain the relationship between SEX and WAGE instead of EXPERIENCE. Good options may include EDUCATION, AGE, or MARRIED. Write your regression equation below and examine whether the b coefficients for SEX are different when SEX is the only predictor and when SEX is added with the othe predictor. 7.3 Plotting a regression line using ggplot In a previous lab, we discussed creating a scatterplot. We can use ggplot to add a regression line to a scatterplot as well. Here, we are going to create a scatterplot looking at the relationship between EDUCATION and WAGE. Step 7: To create a scatterplot, type: ggplot(data = income) + geom_point(aes(x = EDUCATION, y = WAGE)) Now, we can add a regression line by adding the command geom_smooth. This adds a line that fits the data. By default, ggplot uses a line that has curves in it, but we are interested in using a linear regression so we will have to use a different setting. Step 8: Type the following to add a line of best fit to the plot. Remember, to add things to a plot, we can just add them by using the plus symbol. ggplot(data = income) + geom_point(aes(x = EDUCATION, y = WAGE)) + geom_smooth(aes(x = EDUCATION, y = WAGE), method=&#39;lm&#39;, se=F) This gives us the following plot. Now I’ll break down what is in the geom_smooth command. The first part of it tells us which data are being plotted, using the aes() command. This is just like in the geom_point() command. The second part is the method() part. Since we are using linear regression, we type ‘lm’ in quotes. The last part tells us to turn off standard error. By default, ggplot gives a standard error with the regression line. In this case, I want to turn it off just to give you the simple line. Note that we can change how the line looks by using the same concepts we use to edit histograms and scatterplots. We could add the options color() or size() in order to change how big the line is and what color it is. If you look at the plot for how education predicts wages, you might notice that the data have some outliers. Very few people have very low levels of education, such as below 6 years of education. It might be useful to remove those people from the analysis and see if our results hold. To do this, we would use the filter() command to create a temporary dataframe which only has the data we want. We can do this by nesting the filter command inside the other commands. For instance, to do the regression where we have EDUCATION predict WAGE where we only include those who have an EDUCATION greater than 5 years, we would type the following: x = lm(EDUCATION~WAGE, data = filter(income, EDUCATION &gt; 5)) summary(x) Instead of including the data = income like above, I used the filter command in place of it so that when R went looking for the data it wanted to use for the regression, it chose the data output by the command filter(income, EDUCATION &gt; 5. This idea is called nesting commands because one command is inside of another command. When writing scripts, nesting can save time, but a lot of nesting can create some difficult to read code. Step 9: Change the ggplot code you typed in Step 8 to create the same graph but only including data where EDUCATION is greater than 5. Remember, you would use the same nesting idea that I used in the code snippet above. Make sure to include this code in your script. Step 10: Edit the code below to see how EXPERIENCE predicts WAGE. Make sure you include the code in the script you are writing. 7.4 Summary This lab covered a lot about regression. After completing it, you should be able to understand the basics of regression, which are covered in Statistics I. You should also start getting familiar with the following ideas which will be covered in the next few assignments: How to do a simple linear regression in R as well as a multiple regression. This includes entering the commands and interpreting the output correctly. Know how to compare regressions before and after adding a control variable in order to examine whether a control variable can account for a relationship. Understand how to use ggplot to add a regression line (line of best fit) to a scatterplot "],
["free-assignment-4-regression-and-multiple-regression-in-r.html", "8 Free Assignment 4 - Regression and multiple regression in R", " 8 Free Assignment 4 - Regression and multiple regression in R This lab will examine how various personality variables predict a person’s desire to conform. When completing this assignment, make sure you create a script that has your commands that you type in R, including the ones you paste from the assignment itself. We will use the data we previously used in Interactive Assignment 2. Here is a key to the variables: Participant: a participant number E: extraversion score A: agreeableness score C: conscientiousness score N: Neuroticism score O: Openness score Gender: gender coded F and M Age: participant age Conformity: their score on a survey about how much they would conform RATING: a participant’s rating to the question “how much do I think I conform to others’ behaviors” SelfEsteem: a score on a self-esteem inventory with higher scores indicating more self-esteem These data are available on Google Sheets and you can input it by using the following commands: First we need to load the gsheet package as well as ggplot2 library(gsheet) library(ggplot2) Now to load the data, we have to use this link. You can paste in this link and then type the command below. link = &#39;https://docs.google.com/spreadsheets/d/1zZjEhkT4ROACTM4YsW1LTNn8al_scyu9l4_Qkzqvsew/edit?usp=sharing&#39; d = gsheet2tbl(link) Step 1: Create a regression examining each of the 5 personality variables (extraversion, agreeableness, etc.) as a predictor of conformity. Question 1: Write the regression equations for each of the 5 regressions below. Are any of the personality variables significant predictors of conformity? Step 2: Repeat the same step that you used in Step 1 but instead of having Conformity be your outcome variable, use SelfEsteem. In this case, we want to see whether personality predicts self-esteem. Question 2: Write the regression equations for each of the 5 regressions below. Are any of the personality variables significant predictors of conformity? Step 3: For the three personality variables which either significantly predict self-esteem (or marginally predict self-esteem), create scatterplots using ggplot showing this relationship. Make sure that your scatterplots have a line of best fit. Also make sure that the plots have x axis, y axis, and plot labels that are understandable. An example plot is shown below looking at the relationship between neuroticism and self-esteem. Step 4: Now we want to look at whether the relationship between Extraversion and Self-Esteem is due to Gender. Create a new regression adding Gender as a control variable. Question 3: Is the regression between Extraversion and Self-Esteem with Gender as a control variable different than the one that did not include Gender as a control variable? Question 4: How would you summarize the results from this lab? Do you think there is any evidence that personality affects conformity? How about whether personality affects self-esteem? "],
["free-assignment-5-regression-and-sports.html", "9 Free Assignment 5: Regression and Sports", " 9 Free Assignment 5: Regression and Sports The intersection of statistics and sports has changed how we see sports in the last 40 years. This probably first happened in baseball with statistics articles published by a series of fans in the journal SABR (leading to the term sabrmetrics) and was captured in the book and movie Moneyball. One idea that comes up a lot in statistics research in sports is how to identify the most important skills in a game. In baseball, the objective is to score runs and keep the other team from scoring runs. There are many ways to score runs, however, and some ways may be better than others. For instance, some batters are good at getting hits, others may be good at getting on base and avoiding getting out. Some get out a lot but hit many home runs. Knowing which of these skills is most important can help teams pick the right players. In this assignment, we are going to use multiple regression to determine which skills are most likely to lead to team success in offense. The objective of a team is to score runs. When doing this assignment, make sure you create a script which contains all your R commands. The dataset is FA5_baseball.csv and is listed on Canvas. Step 1: You should import this into R as a dataframe with the name b for baseball. I’m lazy and typing b rather than baseball will save some time. Also, load the tidyverse package. When you load the package, check to make sure it inputted correctly. There should be 90 observations of 31 variables. The observations are for the 2014-2016 seasons and here is a guide to some of the column names. You can find out what all the column names are here. Tm: team name League: League (AL for American League, NL for National League) Year: year data were collected BatAge: The average age of the team’s batters RpG: Runs per game (this will be our main outcome variable) R: total runs in a season H: Total hits in a season 2B: Total number of doubles 3B: total number of triples HR: total number of Home Runs SB: total number of stolen bases BB: total number of walks SO: total number of strikeouts BA: Batting average (number of hits divided by number of at bats) OBP: On-base percentage (percent of time a person reaches base) Step 2: First, we want to visualize the relationship between some of our predictor variables and runs per game. Create scatterplots using ggplot which visualize the following relationships. Also add a regression line to each of the plots. Make sure your axes and plot have ap propriate titles: A. Hits predicting Runs per game B. HR predicting Runs per game C. SB predicting Runs per game D. Batting average predicting Runs per game. Your first plot should look something like this: Question 1: When looking at the plots, do you see any outliers? What do you think the plots tell you? Now we are going to do a concept called stepwise regression. The idea of this is to start out with one variable and add other variables and watch to see how the relationship of each predictor changes as we add more predictors. Step 3 Create three regressions, using the following predictors: A. Hits predicting Runs per game B. Hits and Walks predicting Runs per game C. Hits, walks and HR predicting Runs per game Question 2: When looking at each of these regressions, what do you think the story is? Which do you think is most important in predicting runs per game? Now we want to look at each league separately. Step 4: Using what you learned in the tidyverse lab, create two data frames, one data frame containing only NL teams and one data frame containing only AL teams. Step 5: Repeat the work you did for Step 3 for each of these new data frames and see if there are any differences between the AL and NL in the regressions. Question 3: Are there any differences between the AL and NL in the regressions? What differences might there be? "],
["interactive-assignment-7-moderation-in-multiple-regression.html", "10 Interactive Assignment 7: Moderation in Multiple Regression 10.1 Using moderation in regression", " 10 Interactive Assignment 7: Moderation in Multiple Regression This assignment covers the following skills: How to conduct a multiple regression with a dichotomous interaction term in R How to interpret the output from a moderation analysis in R How to plot a moderation analysis using ggplot, by dividing groups into different colors and plotting separate regression lines for each group How to use facet_wrap() to create figures in ggplot with multiple plots How to break down interactions using simple regression. The idea of moderation is that the relationship between two variables may be affected (or moderated) by a third variable. In regression, this means that the relationship between the independent variable and the dependent variable varies due to a third variable, called a moderating variable. The logic of moderation can allow us to test a lot of psychological hypotheses. In this lab, we will walk through how we examine what moderation is on a conceptual level and how it works to test a psychological theory. The cross-race effect in face memory is the well-established effect that people remember faces of their own race better than other races. People generally report saying faces of other races “all look alike” and do worse in memory tests when trying to remember faces of other races. One possible mechanism for this effect is mental state complexity, which is a rating of how much a person seems to be expressing a complex emotion or mental state mental state. Some faces appear to be blank and have no emotional content, some faces appear to have some emotional content, whereas some faces seem to be having complex mental states, which are uniquely human. The idea of this study was to test three things: Do people think faces of their own race express more complex mental states than faces of other races? Do people remember faces of their own race better? Does face race moderate the relationship between complexity and memory? In this study, White participants rated the mental state complexity of White and Black faces. Then they were given a memory test of those faces. They saw the faces they rated along with other new faces and had to decide whether each face was a face they saw previously or a new face they had never seen before. When completing this lab, make sure you create a script with all your commands as we have done in previousl labs. Step 1: In this lab, we will use the tidyverse package so go ahead and load that package. Step 2: Now load the data from the file ‘IA7data.csv’ into R to the dataframe ‘face’ Face should have the following variables: face: the filename of the face race – the race of the face. mean_acc – the mean of distracter_acc and target_acc complexity_rating – rating of the mental state complexity of the face on a 7 point scale. Step 3: Create a scatterplot of the data to see how memory relates to mental state complexity. Using ggplot, generate a graph with complexity_rating on the X axis and mean_acc on the Y axis. Make sure the plot has a title and labels. Also include a regression line of best fit. What does the plot look like? Do you think there is a relationship between the two variables? 2.How does the regression look? Does it look like a significant relationship? Step 4: Create a regression using complexity_rating as a predictor of mean_acc. Question 3: What is the regression equation from this regression? What is the t-value of complexity as a predictor? What is the p-value? Is it a significant predictor? Step 5: Now let’s see if there is an effect of race on complexity. Conduct a t-test comparing complexity between White and Black individuals. Use race as the grouping variable and complexity_rating as the DV. Question 4: Is there a significant effect of complexity? What are the t and p values? What are the means for each group? Step 6: Do the same as question 4 looking at if there is a difference in mean_acc for different race groups. Question 5: Is accuracy higher for White or Black faces? What are the means for each group, the t-value, and the p-value? 10.1 Using moderation in regression Now we’ve found an answer for the first two hypotheses we wanted to test (see page 1). The third hypothesis is that the relationship between complexity and accuracy would be different for Black versus White faces. Now enter in the following to look at the interaction i = lm(mean_acc~complexity_rating*race, data=face) summary(i) ## ## Call: ## lm(formula = mean_acc ~ complexity_rating * race, data = face) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.24062 -0.06673 0.01484 0.06648 0.18720 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.56340 0.09710 5.802 1.42e-07 *** ## complexity_rating 0.01512 0.02807 0.539 0.5917 ## racewhite -0.19933 0.14269 -1.397 0.1665 ## complexity_rating:racewhite 0.08102 0.04073 1.989 0.0503 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1054 on 76 degrees of freedom ## Multiple R-squared: 0.2402, Adjusted R-squared: 0.2103 ## F-statistic: 8.011 on 3 and 76 DF, p-value: 0.0001045 The output here has three variables in addition to the intercept. The first two variables give us the effect of complexity_rating and race as a predictor. In this case, the first two variables are the main effects or the effects of the independent variable and the moderator variable itself. The reason R calls the ‘race’ variable ‘racewhite’ is that R uses the group “white” as the default group and compares the Black faces to the White faces. In this case, both of these variables are different than the regressions you listed above. This is because we have added the moderator variable. In a regression with a moderator variable, the main effects are not very important because they are superceded by the effect of the interaction variable or the variable which examines whether the relationship between the independent variable and dependent variable varies as a function of the moderator variable. In this case, the interaction variable is listed as ‘complexity_rating:racewhite’. It is marginally significant. When we have an interaction, what this means is that we expect the relationship between complexity and face memory to be different for the different values of the moderator variable. That is, the relationship between complexity and face memory is different for Black faces than for White faces. It could be that for one group of faces, the relationship is a positive correlation and for the other group it is a negative correlation. Or it could be that for one group there is no correlation and for another group, it is a positive correlation. One of the best ways to interpret an interaction is to visualize the interaction using ggplot. We can create a plot with ggplot where we divide the data for White faces and for Black faces and plot different regression lines for each group. ggplot(data = face, aes(x = complexity_rating, y = mean_acc, color = race)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) + labs(x = &quot;Complexity Rating&quot;, y = &quot;Mean Accuracy&quot;, title = &quot;Interaction between Complexity and Race Predicting Mean Accuracy&quot;) Step 7: Try typing the plot above and seeing if you can change some of the appearance options involved with the plot. This ggplot code has a few changes from how we have done it before. Instead of putting the aes() option inside of the geom_point() and geom_smooth(), I put it in the main ggplot() command. The reason I can do this is that if I put the aes() option in the ggplot() command, then it automatically adds that option to every geom command. It saves some typing. The other main change is that I added the color = race option. This tells ggplot that I want to dide my data by the variable race geom_point() and geom_smooth() options, it divides the plot bythe A second way we can do the same plot is to create two plots, one for Black faces and one for White faces. We can do this by using the facet_wrap() option. The following code does this. Notice that the code is the same, except I removed the color = 'race' option. ggplot(data = face, aes(x = complexity_rating, y = mean_acc)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) + facet_wrap(~race) + labs(x = &quot;Complexity Rating&quot;, y = &quot;Mean Accuracy&quot;, title = &quot;Interaction between Complexity and Race Predicting Mean Accuracy&quot;) You may wonder what the facet_wrap(~race) means. What this means is that I am grouping the plot by race. If I want to make the plot have the graphs on top of each other, with two rows with one column, I can add this as an option to facet_wrap() ggplot(data = face, aes(x = complexity_rating, y = mean_acc)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) + facet_wrap(~race, nrow = 2) + labs(x = &quot;Complexity Rating&quot;, y = &quot;Mean Accuracy&quot;, title = &quot;Interaction between Complexity and Race Predicting Mean Accuracy&quot;) Another way we can interpret interactions is to divide our data into groups based on the moderator. Then we can do simple linear regressions for each group and see how the coefficients compare. In this case, this would mean dividing our data and Since we have a signficant interaction, we know that the difference between the two groups is different. Step 8: Using the filter() command, create two data frames, one named ‘face.black’ and one named ‘face.white’ which contain only Black and White faces respectively. Step 9: With each of these dataframes conduct a simple regression with complexity predicting memory. Question 6: What are the results you get for the regression? Please note the b-value, t-value, and p-value for each condition. Do these results relate to the results you see in the graph? Question 7: Based on these results, do you think that the data support the hypotheses? What are some potential limitations or cautions with these data? (examples may include: too small sample size, outliers, data are not normally distributed, ceiling or floor effects, and so forth) "],
["free-assignment-6-moderation-with-r.html", "11 Free Assignment 6: Moderation with R", " 11 Free Assignment 6: Moderation with R This assignment covers moderation and will use the following skills: How to conduct a multiple regression with control variables How to examine a dichotomous interaction in R How to interpret an interaction, using plots and by using simple effects analysis. In this assignment, we are going to investigate the relationship between Facial Width-to-Height Ratio (FWHR) predicts personality traits in children before puberty and adolescents. Research shows that FWHR is correlated with aggressive and leadership traits in adult males, ostensibly because it is correlated with the amount of testosterone a male has in his body. During puberty, testosterone helps to cause additional growth to a boy’s jaw. Higher levels of testosterone lead to wider faces or higher FWHR. Higher levels of testosterone also lead to more aggressive behavior and dominant. Men with higher FWHR are more likely to be more aggressive in sports but also more likely to succeed in some businesses. In this study, we are going to investigate a few hypotheses. We have a dataset that has measurements of the personality of children before puberty (age 9-10) and after puberty (age 17). The researchers have the following theories: Childhood FWHR should not be associated with personality for both boys and girls Adolescent FWHR should be associated with personality, but only for boys. These associations should be strongest for personality traits that are related to dominance and aggression. When completing this assignment, make sure you create a script file which contains the commands you used. Step 1: Load the tidyverse package. Step 2: Load the csv file named FA6_moderation.csv into your R workspace. Give it whatever name you want, though I will call mine d. Check to make sure your data are loaded correctly. You should have 235 observations of 12 variables. The variable names are as follows: ID: Face ID AGE: age group: ADOL for adolescents, CHILD for children CONFIDNT: a measure of how conifdent a person is ASSERTIV: a measure of a person’s assertiveness COGNITIV: a measure of a person’s openeness to learning and new ideas OUTGOING: a measure of a person’s willingness to be outgoing or extraverted DEPENDBL: a measure of how dependendable or conscientious a person is WARMHSTL: a measure of a person’s interpersonal warmth or a tendency to be hostile. SEX: a person’s sex, M for males and F for females ATTRACT: a measure of a person’s attractiveness BABYFACE: a measure of how babyfaced a person is FWHR: a z-score measure of a person’s FWHR relative to those of the same age. Question 1: The researchers think that WARMHSTL is the personality variable most likely to be associated with FWHR. Do you see any other variables that may be associated with FWHR? Pick one other personality variable that you think could be associated with FWHR and write them down. We will use that variable later. Step 3: Since many of our hypotheses involve examining the data separately for children and adolescents, make two separate datasets, one for only children (where AGE equals CHILD) and one for only adolescents (where AGE equals ADOL). Step 4: Using ggplot, create a scatterplot examining the relationship with FWHR predicting WARMHSTL. Use some options to make the plot look nice and make sure to include a line of best fit. Your plot should look like the following: It looks like there might not be a relationship between FWHR and Warmth-Hostiity. This may be because we are looking at all the faces, when we want to see whether there is a difference between the children and the adolescents. Step 5: Use the same plot as you did for Step 4 but use the color = AGE option to divide the points for children versus adolescents. ggplot(data = d, aes(x = FWHR, y = WARMHSTL, color = AGE)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) + labs(x = &quot;FWHR&quot;, y = &quot;Warmth - Hostility&quot;, title = &quot;FWHR predicting Warmth&quot;) Question 2: Does it look like there might be an interaction between FWHR and AGE, based on your plot? Step 6: Use the lm() command to conduct a regression analysis looking at the relationship between FWHR and WARMHSTL with AGE as a moderating variable. Question 3: Is there an interaction between WARMHSTL and AGE? What is the significance of the interaction? Step 7: Repeat Steps 4-6 using the other variable you chose in Question 1 as a dependent variable which you thought may also be related to FWHR. Question 4: What are the results of your analysis in Step 7? Did you find an interaction? Now we want to look at adolescents, separately, because we think that the relationship between personality and FWHR will be strongest for adolescents, specifically males Step 8: Conduct a regression analysis using only the data frame containing adolescents. Use FWHR as a predictor and SEX as a moderator predicting WARMHSTL. Question 5: What are the results of your moderation analysis in Step 8? Does SEX significantly moderate the relationship between WARMHSTL and FWHR? Step 9: Create a plot visualizing your interaction from Step 8. Question 6: Use the plot from Step 9 to explain what is happening to cause the interaction (or lack of a significant interaction). Does this support the researchers’ hypothesis? Step 10 Create separate simple regressions to break down the interaction in Step 8 separately for female and male faces. These interactions should have FWHR predicting WARMHSTL for adolescent female faces and adolescent male faces separately. Question 7: Do the simple regressions help to explain the interaction in the moderation anlysis? Do they support the researcher’s hypothesis? Step 11 Use another variable as a dependent varible and conduct another regression analysis with either AGE or SEX as a moderator. If you choose SEX as your moderator, you can do this either for the children data frame or the adolescent data frame. Create a plot of your interaction as well. Question 8: What did you analyze in Step 11? What were your results? "],
["interactive-assignment-8-between-subject-anova.html", "12 Interactive Assignment 8: Between-subject ANOVA", " 12 Interactive Assignment 8: Between-subject ANOVA This assignment will cover the ideas involved with doing one-way and two-way between-subject ANOVAs in R. It will cover: How to set up data for ANOVAs in R How to use the describeBy() function in the psych package to examine marginal means in ANOVA How to evaluate marginal means for main effects and interactions How to use the aov() function to do a one-way and a two-way ANOVA and interpret the output How to use ggplot() to graph a two-way ANOVA. ANOVA stands for ANanlysis Of VAriance and examines whether the variance due to an independent variable or variables is significantly greater than the natural variance within a series of data. The idea is that if an independent variable can explain a significantly greater proprotion of variance than the natural variance in the data, then that variable is important. In this lab, we will investigate data from Zellner et al. (2010) which explored how symmetry and color affected people’s preferences for food. The data are available in the dataset “IA8food.csv”. Step 1: Load the data into R as a dataframe named “food”. In addition, load the tidyverse and psych packages. The variables are as follows: subject – participant number balance – whether the food was balanced or symmetrical or unbalanced color – whether the food was monochrome or colorful attractiveness – measure of how attractive participants found the food (from -100 to 100) willingness – measure of how willing people were to try the food (-100 to 100) liking – how much they reported liking the food (-100 to 100) The researchers’ main hypothesis is investigating how symmetry (the balance variable) and color affect how people view food. Question 1: What is the design of this study using ANOVA terminology? What are the factors and what are the levels? Step 2: Use the describeBy() function to fill out the following table, to investigate the means and standard deviations of the main effects. For instance, to find the main effects of balance using of the attractiveness as the dependent variable, I would type: describeBy(food$attractiveness, food$balance) ## $asymmetrical ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 34 15.9 42.88 12.5 15.59 47.08 -60.7 92.9 153.6 0.08 -0.91 ## se ## X1 7.35 ## ## $symmetrical ## vars n mean sd median trimmed mad min max range skew ## X1 1 34 32.75 38.37 36.99 34.22 33.14 -66.99 99.83 166.82 -0.47 ## kurtosis se ## X1 -0.2 6.58 ## ## attr(,&quot;call&quot;) ## by.default(data = x, INDICES = group, FUN = describe, type = type) Question 2: Fill out the tables below, the first one with the means and the second one with the standard deviations. Note that I already started to fill out the tables. What main effects do you see? Mean Attractiveness Willing Liking Symmetrical 32.75 Unsymmetrical 15.9 Monochrome Colorful Standard Deviation Attractiveness Willing Liking Symmetrical 42.88 Unsymmetrical 38.37 Monochrome Colorful Now we want to look at the means of each group to see if there are any interactions. In order to do this, we can use the describeBy variable and give it two different variables to break down our data. In order to do this for attractiveness, we would type the following: describeBy(food$attractiveness, list(food$balance, food$color), mat=T) ## item group1 group2 vars n mean sd median trimmed ## X11 1 asymmetrical color 1 17 12.39941 41.29885 7.94 11.72333 ## X12 2 symmetrical color 1 17 48.69941 22.89846 42.00 47.39467 ## X13 3 asymmetrical monochrome 1 17 19.40059 45.39939 13.78 20.06200 ## X14 4 symmetrical monochrome 1 17 16.80059 44.39952 17.68 16.85133 ## mad min max range skew kurtosis se ## X11 45.72338 -57.96 92.90 150.86 0.11536118 -0.9298756 10.016442 ## X12 23.75125 17.60 99.37 81.77 0.55513242 -0.8589993 5.553693 ## X13 51.38692 -60.70 89.58 150.28 0.01654903 -1.1328890 11.010970 ## X14 47.72489 -66.99 99.83 166.82 0.06229426 -1.0049512 10.768465 Notice at the end I had to add the mat = T option. This allows R to display the data better. Step 3: Use the describeBy() function to examine the means for the willing and liking variable. Another way to evaluate whether there are interactions between variables is to plot them. Now we will examine how to plot means data in R with ggplot. We can do this with a bar graph. To do this in ggplot, we have to use the stat_summary() function, which tells ggplot to plot means, rather than the actual values themselves. In the first line, I add the data we will use, including the aes() function in the main ggplot() option. Notice that when we do a two-group ANOVA, we have two different independent variables. In ggplot, I have to assign one of these to be the “x” variable and one to be the “grouping” variable, which is in the “fill” option. It tells ggplot to “fill” the different levels of the color variable (color or monochrome) with different colors. Since each of the independent variables are separate, it is arbitrary which one I say is the “x” variable and which one is the “fill” variable. Here is the code we will use: ggplot(data = food, aes(x = balance, y = attractiveness, fill = color)) + stat_summary(fun.y = mean, geom = &#39;bar&#39;, position = &#39;dodge&#39;) Question 3: Does it look like there is an interaction between balance and color? Question 4: Type the graph below, but switch the x variable and the fill variable. What happens to the plot? Step 4: Change the code for graphing in order to graph the willing variable as the dependent variable. Then change the code for liking as the dependent variable Question 5: Does it look like there is an interaction between balance and color for the willing variable? What about for the liking variable? Now after all this graphing, we will get to actually doing the ANOVA. The code to do the ANOVA with balance and color as factors predicting attractiveness is as follows: x = aov(attractiveness~balance*color, data = food) summary(x) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## balance 1 4827 4827 3.083 0.0839 . ## color 1 2635 2635 1.683 0.1992 ## balance:color 1 6431 6431 4.108 0.0469 * ## Residuals 64 100198 1566 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the output, the first two rows give us the sums of squares, mean squares, F, and p-value for the balance and color main effects. The last row is the interaction between balance and color. Question 6: Based on the output, are there any significant main effects? Interactions? How does this relate to what you predicted after looking at the means and the graphs? Question 7: What do you think explains the interaction (or lack of interaction) that you found in question 6. Try to use your best guess to explain what the data mean. Step 5: Using the code above, repeat the same steps to conduct the ANOVA using willing and liking as dependent variables, instead of attractiveness. Question 8: Based on the output, are there any significant main effects for the willing and liking variable? Interactions? How does this relate to what you predicted after looking at the means and the graphs? Question 9: What do you think explains the results that you found in question 8 Try to use your best guess to explain what the data mean. "],
["free-assignment-7-between-subject-anova.html", "13 Free Assignment 7: Between-subject ANOVA", " 13 Free Assignment 7: Between-subject ANOVA When completing this assignment, be sure to create a script file with all the commands you type into R and turn that in with the assignment. In this assignment, we are going to cover interactions using the wages dataset we used in Interactive Assignment 5. As a reminder, here are the variables: ID: person ID WAGE: wage (dollars per hour) OCCUPATION: occupation(1=Management, 2=Sales, 3=Clerical , 4=Service, 5=Professional, 6=Other) SECTOR: sector of employment(0= other, 1=Manufacturing, 2=Construction) UNION: Union membership (1=yes, 0=no) EDUCATION: Years of education (12 = high school diploma, 16= completed college, etc.) EXPERIENCE: Years of work experience AGE: Age in years SEX: Sex (0 – male, 1 – female) MARR: Married (0 – no, 1 – yes) RACE: Race (0 – other, 1 – white, 2 – Hispanic) SOUTH:Southern region (1 – yes, 0 – no) Step 1: The data are in the file “IA5WageData.csv”. Load the data into R as the dataframe “wages”. Also, load the tidyverse and psych packages into R. In this study, we are going to investigate whether sex and marital status predict wages. If we remember Interactive Assignment 5, we examined the wage gap in earnings, that men make more than women. So we expect to find a main effect of sex in our data. However, we’ll also investigate how marital status plays a role. First, we have to alter our data. Since the factors in our data are given as numbers, we need to change them to factor variables. To do this for sex and race, we do the following code. Step 2: Type the following code into R in order to change SEX and MARR from numeric variables to factor variables. wages$SEX = as.factor(wages$SEX) wages$MARR = as.factor(wages$MARR) Step 3: Use the describeBy package to fill out the following table for wages to examine the main effects and the interactions: Mean Wage SD of Wage Female Male Unmarried Married Means of each cell: Mean Wage Unmarried Married Female Male Step 4 Use ggplot to create a plot looking at the variables. Use SEX as your “x” variable and MARR as your “fill” variable. Feel free to change the labels in order to make the plot look better. Question 1: Based on these means and the plot, what main effects and/or interactions do you expect? Step 5: Use the aov() command to examine whether there is an interaction between SEX and MARR. Question 2: Are there any significant main effects? Interactions? Please report the F and P values below, using APA formatting. B Question 3: Using your answers for questions 1 and 2, what do you think these data mean? What is your interpretation about the effects of sex and marriage on WAGE? For the rest of this lab, I want you to investigate the following question. A researcher has the idea that the wage gap between men and women will be higher in the South than the rest of the country. Can you test this hypothesis using ANOVA? Go through the steps we did for the rest of this lab and test this hypothesis. First, you need to turn SOUTH into a factor variable, and then do the rest of the steps. Question 4: What results did you get? Write what results you had below: Question 5: Do you think there is any evidence for the researcher’s hypothesis? "],
["interactive-assignment-9-within-subject-anova.html", "14 Interactive Assignment 9: Within-subject ANOVA 14.1 Graphing with error bars", " 14 Interactive Assignment 9: Within-subject ANOVA Within-subject and mixed-subject ANOVAs are very useful in a lot of experiments, but present some mathematical problems that are outside of the scope of this chapter. The point of this assignment is to accomplish the following: Understand the difference between within-subject and between-subject factors in ANOVA Use the ez package to do within-subject and mixed-subject ANOVAs using the ezANOVA() command Interpret ANOVA using graphs and learn how to change the scale of the y-axis and add error bars to a bar graph in ggplot. To do within-subject and mixed-subject ANOVAs, we need to use the ez package. Step 1: Install and load the ez package by using the following commands: install.packages(&#39;ez&#39;) library(&#39;ez&#39;) The data for this experiment are from a study investigating how eye gaze affects emotion recognition Step 2 Load the data from the file IA9face.csv into R as a dataframe named “face”. Participants saw a series of faces and had to label whether they were angry or fearful. Participants perceived 240 faces and had to label their emotion as quickly as possible using a button click. The variables are as follows: subject: subject number emotion: face emotion, either angry or fearful gaze: gaze direction: either Direct (looking at the participant) or Averted: (looking away from the participant) RT: average reaction time for the faces (i.e., the row for participant 1 Anger Direct would be the average reaction time for all the angry faces looking at participant 1) acc: average accuracy for the faces (i.e., number of correct faces / number of total faces) The researchers had two possible hypotheses: 1. Direct gaze hypothesis: emotions are faster recognized and more accurately recognized when they are showing direct gaze (i.e., looking at the participant) 2. Shared signal hypothesis: emotions are faster recognized and more accurately recognized when they are showing congruent eye gaze. This means that anger is faster recognized when a person is looking with direct gaze and fear is faster recognized when a person is looking away Question 1. Based on this data, if the direct gaze hypothesis is true, what pattern main effects and/or interactions would you expect. (An example answer to this question which is not correct would be: I would expect a main effect of emotion where anger is faster recognized than fear because of X) Question 2. Based on this data, if the shared signal hypothesis is true, what pattern of main effects and/or interactions would you expect. Question 3. If we are looking at eye gaze and emotion as IVs, which variables are within-subject and which are between-subject? To conduct an ANOVA using the ez package, we have to use the format that ez uses for its variables. Here is the command to do the ANOVA using RT as a dependent variable and emotion and gaze as independent variables. ezANOVA(face, dv = .(RT), wid = .(subject), within = .(gaze,emotion)) ## Warning: Converting &quot;subject&quot; to factor for ANOVA. ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 gaze 1 59 5.543623 0.02189570 * 0.001110086 ## 3 emotion 1 59 6.692092 0.01216943 * 0.007202689 ## 4 gaze:emotion 1 59 6.789935 0.01158632 * 0.001329311 If we look at the output, we have significant effects for gaze, emotion, and the interaction. The significant main effect tells us that reaction time is different for gaze and different for emotion. Since there are only two levels, there are only two possibilities and all we have to do is look at the means. To see the means, we can use describeBy like we did in Interactive Assignment 8. Or we can use another function called aggregate. The aggregate function is a powerful function that takes data, breaks it down by variable, and does another R function. In this case, we want to break the data down by gaze and emotion (direct gaze anger, direct gaze fear, averted gaze anger, averted gaze fear) and then get the mean of each group. To do this using aggregate, we would type the following: aggregate(RT~emotion*gaze, data=face, mean) ## emotion gaze RT ## 1 Anger Averted 963.1116 ## 2 Fear Averted 985.7175 ## 3 Anger Direct 930.6966 ## 4 Fear Direct 987.1787 Question 4: Looking at the mean reaction time, how would you explain the main effects? How would you explain the interactions? You should note which groups are bigger than which other groups and what differences could cause the interaction. Question 5: Do the results support the shared signal hypothesis? Do they support the direct eye gaze hypothesis? Another way to look at the data is by using a graph. Step 2: Use what you’ve learned about ggplot to produce a bar graph to visualize the data. Your output should look something like this: If you have a graph like this, you might think that the scale is hard to see. We can change the scale of the y-axis by using the coord_cartesian(xlim = c(900, 1000)) If you add the above code to your ggplot() output, you should get the following: Step 3: Use the ezANOVA command to examine accuracy (variable acc) as a dependent variable. Use the aggregate() command in order to investigate what might explain the main effects and/or interactions. Question 6: Do the results for accuracy support the shared signal hypothesis? Do they support the direct eye gaze hypothesis? 14.1 Graphing with error bars The last thing I’ll add to this lab is how to add error bars to a graph. These help us understand the interaction. To graph the accuracy data, we would type the following: ggplot(face, aes(x = gaze, y = acc, fill = emotion)) + stat_summary(fun.y = mean, geom = &#39;bar&#39;, position = &#39;dodge&#39;) + coord_cartesian(ylim = c(.7, 1)) If I want to add error bars, I need to add another function to ggplot. These are generally adding geoms, but since my geom is not actually the data itself but some statistic calculated from the data, I will use the stat_summary() function again, with the option mean_se instead of mean to add the error bars. This gives me error bars with standard error. In addition, I don’t want to plot a bar, but an error bar. I would type the following: ggplot(face, aes(x = gaze, y = acc, fill = emotion)) + stat_summary(fun.y = mean, geom = &#39;bar&#39;, position = &#39;dodge&#39;) + stat_summary(fun.data = mean_se, geom = &#39;errorbar&#39;, position = &#39;dodge&#39;) + coord_cartesian(ylim = c(.7, 1)) A lot of times, we want to plot 95% confidence intervals as our error bars. This is useful because if one bar is greater or less than the 95% confidence interval of another bar, it is usually significantly different (with an \\(\\alpha = .05\\)) To do this, we would type the following code: ggplot(face, aes(x = gaze, y = acc, fill = emotion)) + stat_summary(fun.y = mean, geom = &#39;bar&#39;, position = &#39;dodge&#39;) + stat_summary(fun.data = mean_cl_normal, geom = &#39;errorbar&#39;, position = &#39;dodge&#39;) + coord_cartesian(ylim = c(.7, 1)) Notice that the 95% confidence intervals are bigger than the standard error estimates. Either way, the averted anger bar is lower than the rest of the bars, and this difference is statistically significant since none of the means of the other bars are within the averted anger bar. Step 4: Rewrite the code you used to make your graph in Step 2 to add error bars using standard error. Make sure you include this code in the script file you submit when you are done with this lab. "],
["free-assignment-8-within-subject-anova.html", "15 Free Assignment 8: Within-subject ANOVA", " 15 Free Assignment 8: Within-subject ANOVA This assignment will cover how to conduct within-subject and mixed-subject ANOVAs in R. It builds on Interactive Assignment 9 which discusses how to use the ez package to do ANOVA design. Step 1: Before we get started, load the ez package and the tidyverse package. The dataset for this homework investigate how older and younger adults perceive faces and rate them on several characteristics. Older and younger adult participants each rated older and younger faces on the characteristics below. The variables you will need are described below. ID – participant ID number raterage – a between-subject factor for participant age, with either “old” or “young” ratergender – participant gender: either “F” or “M” ag.rate – average ratings of aggressiveness at.rate – average rating of attractiveness ba.rate – average rating of babyfaceness co.rate – average rating of competence he.rate – average rating of health un.rate – average rating of untrustworthiness faceage – a within-subject variable which indicates the age of the face, with values “oldface” and “youngface” In this dataset, I have older and younger adult raters rating older and younger adult faces. To simplify things a bit and save me some typing, I will use the abbreviations OR and YR to refer to Older Raters and Younger Raters and OF and YF to refer to Older Faces and Younger Faces, respectively. They are the same data as we used in Interactive Assignment 4, on graphing, but they are structured a bit differently. In that assignment, I had the ratings for older adults and younger adults in separate columns but in this assignment, the ratings for older adults and for younger adults are in the same column, which is how ez requires us to format the data. In this assignment, the dependent variables will be the face ratings for aggressiveness, attractiveness, babyfaceness, competence, health, and untrustworthiness. Step 2: Load the data named “FA8rate.csv” into R as the data frame “rate”, for ratings. library(ez) library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 3.3.2 ## Loading tidyverse: ggplot2 ## Loading tidyverse: tibble ## Loading tidyverse: tidyr ## Loading tidyverse: readr ## Loading tidyverse: purrr ## Loading tidyverse: dplyr ## Warning: package &#39;ggplot2&#39; was built under R version 3.3.2 ## Warning: package &#39;tibble&#39; was built under R version 3.3.2 ## Warning: package &#39;tidyr&#39; was built under R version 3.3.2 ## Conflicts with tidy packages ---------------------------------------------- ## filter(): dplyr, stats ## lag(): dplyr, stats rate = read.csv(&#39;data/FA8rate.csv&#39;) Step 3: Use the aggregate command in order to fill out tables with the means for each rating, separating the ratings by face age and by rater age. So you will have four means, the mean for older raters (OR) rating older faces (OF), OR rating younger faces (YF), YR rating OF, and YR rating YF. To get the means for aggressiveness, you would type: aggregate(ag.rate~raterage*faceage, data = rate, FUN = mean) ## raterage faceage ag.rate ## 1 old oldface 3.817839 ## 2 young oldface 3.350521 ## 3 old youngface 3.996978 ## 4 young youngface 3.512032 Question 1: Using what you get in Step 3, fill out the following tables with the means. I already filled out the Aggressiveness table. Aggressiveness OR YR OF 3.82 3.35 YF 4.00 3.51 Attractiveness OR YR OF YF Babyfaceness OR YR OF YF Competence OR YR OF YF Health OR YR OF YF Untrustworthiness OR YR OF YF Step 2: Use ggplot to create bar graphs of each of the 6 dependent variables with separate bars for face age and rater age (using faceage and raterage as the independent variables). Make sure to add error bars representing standard error. You can also use the coord_cartesian() function to change the scale of the y axis. For instance, your plot for aggressiveness should look like the following: Question 2: Based on the plots and the tables of the means, do you predict any interactions? Now we are going to conduct the ANOVAs, using face age and rater age as independent variables predicting the dependent variable of aggressiveness ratings (ag.rate). Question 4: Which of these independent variables (if any) are between-subject variables? Which are within-subject variables (since this is a lab on within-subject and mixed ANOVA, one of these has to be a within-subject variable). Step 3:Type the following to look at face age (faceage) and rater age (raterage) as independent variables predicting the dependent variable of aggressiveness ratings (ag.rate). ezANOVA(rate, dv = .(ag.rate), wid = .(ID), within = .(faceage), between = .(raterage)) ## Warning: Collapsing data to cell means. *IF* the requested effects are a ## subset of the full design, you must use the &quot;within_full&quot; argument, else ## results may be inaccurate. ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 raterage 1 30 6.2652118 0.01798802 * 1.525502e-01 ## 3 faceage 1 30 5.0061221 0.03283715 * 2.251707e-02 ## 4 raterage:faceage 1 30 0.0134071 0.90859136 6.168922e-05 Question 5: When you type the following, what results do you get? Step 4: Changing the code above, conduct ANOVAs using face age and rater age predicting the other five face ratings, at.rate, ba.rate, co.rate, he.rate, and un.rate. Question 6: Now I want you to report your results using APA formatting. If you have a significant main effect or interaction, you need to explain why you think there was a main effect or interaction. For main effects, all you have to do is indicate which level was higher. For interactions, you should interpret them by eyeballing the graphs and means and trying to explain why there is an interaction. For instance, this is how I would report the results for aggressiveness rating. We conducted a 2(face age: older vs younger) X 2(participant age: older versus younger) mixed ANOVA with face age as a within-subject variable and participant age as a between-subject variable. We found a significant main effect of rater age, F(1,30) = 6.27, p = .018, indicating that older participants rated faces more aggressively than younger participants. We also found a significant main effect of face age, F(1,30) = 5.01, p = .032, as younger faces were rated more aggressively than older faces. There was no significant interaction, F(1,30) = .013, p = .91. Results for attractiveness: Results for babyfaceness: Results for competence: Results for health: Results for untrustworthiness: Question 7: What do you think the story is with this data? Pick one of the rating variables (other than aggressiveness) and try to explain why you think there were these main effects and interactions. I’m not going to grade this based on your interpretation; instead, I want you to try your best to interpret the data using what you know about the world. For example, I may explain the aggressiveness rating by stating that there is a main effect of face age because overall, people find older faces less aggressive than younger faces. This may be because people have a stereotype that older adults are less aggressive. There is a main effect of rater age because older adults rated faces more aggressively than younger adults. This may be because older adults are more skeptical of people in general. "]
]
